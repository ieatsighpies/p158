{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Example\n",
    "\n",
    "> Train a U-Net for pixelwise segmentation of the prostate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef19056",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate p158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you have a requirements.txt:\n",
    "!pip install --upgrade pip\n",
    "!pip install monai torch pytorch-ignite matplotlib matplotlib-inline pyyaml munch imageio tqdm pandas opencv-python nibabel \"protobuf<=3.20.3\"\n",
    "!pip install tensorboard\n",
    "!pip install scikit-image\n",
    "\n",
    "# # Otherwise install core libs directly:\n",
    "# !pip install monai[\"all\"] ignite matplotlib pyyaml munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import ignite\n",
    "import torch\n",
    "\n",
    "from monai.data.meta_tensor import MetaTensor\n",
    "\n",
    "# Add MetaTensor to the safe globals list\n",
    "torch.serialization.add_safe_globals([MetaTensor])\n",
    "\n",
    "from prostate158.utils import load_config\n",
    "from prostate158.train import SegmentationTrainer\n",
    "from prostate158.report import ReportGenerator\n",
    "from prostate158.viewer import ListViewer\n",
    "import prostate158.utils as utils\n",
    "from prostate158.utils import load_config\n",
    "import psutil\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Helper to print system + GPU memory\n",
    "def print_memory_stats(stage=\"\"):\n",
    "    # System RAM\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(\n",
    "        f\"\\n[MEMORY] {stage} â–¶ System RAM: \"\n",
    "        f\"total {mem.total/1e9:.1f} GB, used {mem.used/1e9:.1f} GB ({mem.percent}%)\"\n",
    "    )\n",
    "    # GPU RAM (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        # call nvidia-smi\n",
    "        print(\"[MEMORY] GPU status via nvidia-smi:\")\n",
    "        try:\n",
    "            gpu_info = subprocess.check_output(\n",
    "                [\n",
    "                    \"nvidia-smi\",\n",
    "                    \"--query-gpu=name,memory.total,memory.used\",\n",
    "                    \"--format=csv\",\n",
    "                ]\n",
    "            ).decode(\"utf-8\")\n",
    "            print(gpu_info.strip())\n",
    "        except Exception as e:\n",
    "            print(\"  (nvidia-smi failed:\", e, \")\")\n",
    "        # PyTorch peak stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d574623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "def check_nifti_sizes(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".nii\") or file.endswith(\".nii.gz\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    img = nib.load(file_path)\n",
    "                    print(f\"File: {file_path}\")\n",
    "                    print(f\"Shape: {img.shape}\")\n",
    "                    print(f\"Header: {img.header}\")\n",
    "                    print(\"-\" * 50)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = os.path.join(os.getcwd(), \"prostate158\", \"train\")\n",
    "    check_nifti_sizes(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(monai.networks.nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2090a9",
   "metadata": {},
   "source": [
    "All parameters needed for training and evaluation are set in `anatomy.yaml` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = load_config(\"tumor.yaml\")  # change to 'tumor.yaml' for tumor segmentation\n",
    "# monai.utils.set_determinism(seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9e5e5",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Print memory before training\n",
    "print_memory_stats(\"Before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857863ad",
   "metadata": {},
   "source": [
    "Create supervised trainer for segmentation task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"tumor.yaml\")\n",
    "\n",
    "trainer = SegmentationTrainer(\n",
    "    progress_bar=True,\n",
    "    early_stopping=True,\n",
    "    metrics=[\"MeanDice\", \"HausdorffDistance\", \"SurfaceDistance\"],\n",
    "    save_latest_metrics=True,\n",
    "    config=cfg,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights from tumor.pt\n",
    "trainer.load_checkpoint(\"models/tumor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"tumor.yaml\")\n",
    "\n",
    "# First create the trainer\n",
    "trainer = SegmentationTrainer(\n",
    "    progress_bar=True,\n",
    "    early_stopping=True,\n",
    "    metrics=[\"MeanDice\", \"HausdorffDistance\", \"SurfaceDistance\"],\n",
    "    save_latest_metrics=True,\n",
    "    config=cfg,\n",
    ")\n",
    "\n",
    "# Then load the pretrained weights into the network\n",
    "# print(f\"Loading pretrained weights from models/tumor.pt\")\n",
    "state_dict = torch.load(\n",
    "    \"./models/tumor.pt\", map_location=trainer.config.device\n",
    ")\n",
    "# Handle both cases: direct state dict or wrapped in 'state_dict' key\n",
    "if isinstance(state_dict, dict) and \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "trainer.network.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "464f4aa3",
   "metadata": {},
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=$config.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d0df8",
   "metadata": {},
   "source": [
    "Adding a learning rate scheduler for one-cylce policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_one_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Print peak GPU memory after fit_one_cycle\n",
    "if torch.cuda.is_available():\n",
    "    peak = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "    print(f\"[MEMORY] Peak GPU memory used during fit_one_cycle: {peak:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157654a",
   "metadata": {},
   "source": [
    "Let's train. This can take several hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac74992",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from prostate158.inference3 import inference_pipeline\n",
    "\n",
    "# Set paths for case ID 20\n",
    "data_dir = \"prostate158_train\"  # Base directory from config\n",
    "case_dir = os.path.join(data_dir, \"train\", \"051\")\n",
    "\n",
    "# Input paths\n",
    "t2_path = os.path.join(case_dir, \"t2.nii.gz\")\n",
    "adc_path = os.path.join(case_dir, \"adc.nii.gz\")\n",
    "dwi_path = os.path.join(case_dir, \"dwi.nii.gz\")\n",
    "\n",
    "# Output path\n",
    "os.makedirs(\"predictions\", exist_ok=True)\n",
    "output_path = os.path.join(\"predictions\", \"case_051_tumor_pred.nii.gz\")\n",
    "\n",
    "# Run inference\n",
    "inference_pipeline(\n",
    "    t2_path=t2_path,\n",
    "    adc_path=adc_path,\n",
    "    dwi_path=dwi_path,\n",
    "    output_path=output_path,\n",
    "    config_path=\"tumor.yaml\",\n",
    "    checkpoint_path=\"models/tumor.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b34c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "# 2) Load each volume and print its shape\n",
    "for name, path in [(\"T2\", t2_path), (\"ADC\", adc_path), (\"DWI\", dwi_path)]:\n",
    "    img = nib.load(path)\n",
    "    data = img.get_fdata()\n",
    "    print(f\"{name}  shape: {data.shape}\")\n",
    "\n",
    "# 3) Load the prediction and print its shape\n",
    "pred_img = nib.load(\"predictions/t2.nii.gz\")\n",
    "pred_data = pred_img.get_fdata()\n",
    "print(f\"Prediction shape: {pred_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Final memory report\n",
    "if torch.cuda.is_available():\n",
    "    peak_total = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "    print(f\"[MEMORY] Peak GPU memory used across all: {peak_total:.2f} GB\")\n",
    "print_memory_stats(\"After trainer.run()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b3048",
   "metadata": {},
   "source": [
    "Finish the training with final evaluation of the best model. To allow visualization of all outputs, add OutputStore handler first. Otherwise only output form the last epoch will be accessible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_handler = ignite.handlers.EpochOutputStore()\n",
    "eos_handler.attach(trainer.evaluator, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(checkpoint=r\"models\\tumor.pt\", map_location=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca14b",
   "metadata": {},
   "source": [
    "Generate a markdown document with segmentation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_generator = ReportGenerator(cfg.run_id, cfg.out_dir, cfg.log_dir)\n",
    "report_generator.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at some outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.evaluator.state.output\n",
    "keys = [\"image\", \"label\", \"pred\"]\n",
    "outputs = {k: [o[0][k].detach().cpu().squeeze() for o in output] for k in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListViewer(\n",
    "    [o.transpose(0, 2).flip(-2) for o in outputs[\"image\"][0:3]]\n",
    "    + [o.argmax(0).transpose(0, 2).flip(-2).float() for o in outputs[\"label\"][0:3]]\n",
    "    + [o.argmax(0).transpose(0, 2).flip(-2).float() for o in outputs[\"pred\"][0:3]]\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
