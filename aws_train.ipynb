{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3802534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --profile asher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f59a2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:  arn:aws:iam::345594598345:role/service-role/AmazonSageMaker-ExecutionRole-20250707T163996\n",
      "sagemaker-ap-southeast-1-345594598345\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.local import LocalSession\n",
    "role = \"arn:aws:iam::345594598345:role/service-role/AmazonSageMaker-ExecutionRole-20250707T163996\" # get_execution_role() #\n",
    "print(\"Role: \", role)\n",
    "# sess = LocalSession()  # for local session\n",
    "# local_sess.config = {'local': {'local_code': True}}\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto3.Session(region_name=\"ap-southeast-1\")) #name of sso session remove profile name in sagemaker studio , profile_name=\"asher\"\n",
    "# #for remote session\n",
    "# settings = dict(\n",
    "#     sagemaker_session=sess,\n",
    "#     role=role,\n",
    "#     instance_type=\"ml.t3.large\",\n",
    "#     dependencies='./requirements.txt',\n",
    "# )\n",
    "\n",
    "bucket_name = sess.default_bucket()\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f7915",
   "metadata": {},
   "source": [
    "Uploaded training, testing and validation datasets to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp ./data s3://{bucket_name}/prostate158 --recursive --profile asher\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072598c1",
   "metadata": {},
   "source": [
    "Upload weights of pre-trained UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp models/tumor.pt s3://{bucket_name}/models/tumor.pt --profile asher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5963bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./tumor.yaml to s3://sagemaker-ap-southeast-1-345594598345/configs/tumor.yaml\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp tumor.yaml s3://{bucket_name}/configs/tumor.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0daea875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-ap-southeast-1-345594598345/prostate158/prostate158_train/train', 'test': 's3://sagemaker-ap-southeast-1-345594598345/prostate158/prostate158_test/test', 'model': 's3://sagemaker-ap-southeast-1-345594598345/models/tumor.pt', 'training_config': 's3://sagemaker-ap-southeast-1-345594598345/configs/tumor.yaml'}\n"
     ]
    }
   ],
   "source": [
    "if bucket_name:\n",
    "    train_channel = f\"s3://{bucket_name}/prostate158/prostate158_train/train\"\n",
    "    test_channel = f\"s3://{bucket_name}/prostate158/prostate158_test/test\"\n",
    "    model_channel = f\"s3://{bucket_name}/models/tumor.pt\"\n",
    "    tumor_config = f\"s3://{bucket_name}/configs/tumor.yaml\"\n",
    "inputs = {\"train\": train_channel, \"test\": test_channel, \"model\": model_channel, \"training_config\": tumor_config}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2060c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting munch\n",
      "  Using cached munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Using cached munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Installing collected packages: munch\n",
      "Successfully installed munch-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50c9f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Munch({'debug': False, 'overwrite': False, 'ndim': 3, 'run_id': 'tumor', 'out_dir': '/opt/ml/model/', 'model_dir': '/opt/ml/input/data/model/', 'log_dir': 'logs', 'seed': 42, 'device': 'cuda', 'num_workers': 4, 'data': Munch({'data_dir': '/opt/ml/input/data/', 'train_csv': './prostate158/train.csv', 'valid_csv': './prostate158/valid.csv', 'test_csv': './prostate158/test.csv', 'image_cols': ['t2', 'adc', 'dwi'], 'label_cols': 'adc_tumor_reader1', 'train': True, 'valid': True, 'test': False, 'dataset_type': 'persistent', 'cache_dir': '/tmp/monai-cache', 'batch_size': 1}), 'transforms': Munch({'prob': 0.175, 'spacing': [0.5, 0.5, 0.5], 'orientation': 'RAS', 'rand_bias_field': Munch({'degree': 2, 'coeff_range': [0.0, 0.01]}), 'rand_gaussian_smooth': Munch({'sigma_x': [0.25, 1.5], 'sigma_y': [0.25, 1.5], 'sigma_z': [0.25, 1.5]}), 'rand_gibbs_nose': Munch({'alpha': [0.5, 1]}), 'rand_affine': Munch({'rotate_range': 5, 'shear_range': 0.5, 'translate_range': 25}), 'rand_rotate90': Munch({'spatial_axes': [0, 1]}), 'rand_rotate': Munch({'range_x': 0.1, 'range_y': 0.1, 'range_z': 0.1}), 'rand_elastic': Munch({'sigma_range': [0.5, 1.5], 'magnitude_range': [0.5, 1.5], 'rotate_range': 5, 'shear_range': 0.5, 'translate_range': 25}), 'rand_zoom': Munch({'min': 0.9, 'max': 1.1}), 'rand_spatial_crop_samples': Munch({'roi_size': [64, 64, 64], 'num_samples': 8}), 'gaussian_noise': Munch({'mean': 0.1, 'std': 0.25}), 'shift_intensity': Munch({'offsets': 0.2}), 'gaussian_sharpen': Munch({'sigma1_x': [0.5, 1.0], 'sigma1_y': [0.5, 1.0], 'sigma1_z': [0.5, 1.0], 'sigma2_x': [0.5, 1.0], 'sigma2_y': [0.5, 1.0], 'sigma2_z': [0.5, 1.0], 'alpha': [10.0, 30.0]}), 'adjust_contrast': Munch({'gamma': 2.0})}), 'model': Munch({'type': 'unet', 'channels': [16, 32, 64, 128, 256, 512], 'strides': [2, 2, 2, 2, 2], 'act': 'PRELU', 'norm': 'BATCH', 'dropout': 0.15, 'num_res_units': 4, 'in_channels': 3, 'out_channels': 2, 'blocks_down': '1,2,2,1', 'blocks_up': '1,2,2', 'num_init_kernels': 16, 'recurrent': True, 'residual': True, 'attention': True, 'se': False}), 'optimizer': Munch({'Novograd': Munch({'lr': 0.001, 'weight_decay': 0.01, 'amsgrad': True})}), 'lr_scheduler': Munch({'OneCycleLR': Munch({'max_lr': 0.001})}), 'loss': Munch({'DiceFocalLoss': Munch({'include_background': False, 'softmax': True, 'to_onehot_y': True})}), 'training': Munch({'max_epochs': 5, 'early_stopping_patience': 1000})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from munch import Munch\n",
    "# config = yaml.safe_load(Path(\"tumor.yaml\").read_text())\n",
    "config = Munch.fromDict(yaml.safe_load(Path(\"tumor.yaml\").read_text()))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a9081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-07-17-08-41-56-628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17 08:42:00 Starting - Starting the training job...\n",
      "2025-07-17 08:42:15 Starting - Preparing the instances for training...\n",
      "2025-07-17 08:42:38 Downloading - Downloading input data...\n",
      "2025-07-17 08:43:08 Downloading - Downloading the training image........................\n",
      "2025-07-17 08:47:05 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "Skipping CUDA compat setup as package not found\n",
      "2025-07-17 08:47:31,879 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2025-07-17 08:47:31,896 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-17 08:47:31,905 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2025-07-17 08:47:31,937 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2025-07-17 08:47:33,682 sagemaker-training-toolkit INFO     Installing module.\n",
      "Processing /opt/ml/code\n",
      "Installing build dependencies: started\n",
      "Installing build dependencies: finished with status 'done'\n",
      "Getting requirements to build wheel: started\n",
      "Getting requirements to build wheel: finished with status 'done'\n",
      "Preparing metadata (pyproject.toml): started\n",
      "Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting monai (from -r requirements.txt (line 1))\n",
      "Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.7.1+cu128)\n",
      "Collecting pytorch-ignite (from -r requirements.txt (line 3))\n",
      "Downloading pytorch_ignite-0.5.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (6.0.2)\n",
      "Collecting munch (from -r requirements.txt (line 6))\n",
      "Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.37.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Collecting nibabel (from -r requirements.txt (line 9))\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (3.20.3)\n",
      "Collecting scikit-image (from -r requirements.txt (line 11))\n",
      "Downloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (3.10.3)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (4.11.0.86)\n",
      "Requirement already satisfied: sagemaker in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.246.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.38.33)\n",
      "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/site-packages (from monai->-r requirements.txt (line 1)) (1.26.4)\n",
      "Collecting torch (from -r requirements.txt (line 2))\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch->-r requirements.txt (line 2))\n",
      "Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (80.9.0)\n",
      "Collecting sympy==1.13.1 (from torch->-r requirements.txt (line 2))\n",
      "Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from pytorch-ignite->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/site-packages (from imageio->-r requirements.txt (line 7)) (11.3.0)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/site-packages (from scikit-image->-r requirements.txt (line 11)) (1.15.3)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->-r requirements.txt (line 11))\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->-r requirements.txt (line 11))\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 12)) (3.2.3)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/site-packages (from matplotlib-inline->-r requirements.txt (line 13)) (5.14.3)\n",
      "Requirement already satisfied: attrs<26,>=24 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (25.3.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (3.1.1)\n",
      "Requirement already satisfied: docker in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.115.12)\n",
      "Requirement already satisfied: google-pasta in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (4.24.0)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (2.3.0)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (4.3.8)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (7.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (2.32.4)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (1.0.37)\n",
      "Requirement already satisfied: schema in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (1.26.20)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/site-packages (from sagemaker->-r requirements.txt (line 15)) (0.34.3)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.33 in /usr/local/lib/python3.12/site-packages (from boto3->-r requirements.txt (line 16)) (1.38.33)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from boto3->-r requirements.txt (line 16)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.12/site-packages (from boto3->-r requirements.txt (line 16)) (0.13.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker->-r requirements.txt (line 15)) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker->-r requirements.txt (line 15)) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 15)) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/site-packages (from omegaconf<3,>=2.2->sagemaker->-r requirements.txt (line 15)) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (2.11.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /usr/local/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (14.0.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /usr/local/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 15)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 15)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 15)) (0.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r requirements.txt (line 15)) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->sagemaker->-r requirements.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->sagemaker->-r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->sagemaker->-r requirements.txt (line 15)) (2025.4.26)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.12/site-packages (from fastapi->sagemaker->-r requirements.txt (line 15)) (0.46.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->sagemaker->-r requirements.txt (line 15)) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (1.7.7)\n",
      "Requirement already satisfied: dill>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.6 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /usr/local/lib/python3.12/site-packages (from pathos->sagemaker->-r requirements.txt (line 15)) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/site-packages (from uvicorn->sagemaker->-r requirements.txt (line 15)) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/site-packages (from uvicorn->sagemaker->-r requirements.txt (line 15)) (0.16.0)\n",
      "Downloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 138.0 MB/s eta 0:00:00\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 766.6/766.6 MB 35.0 MB/s eta 0:00:00\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 105.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 166.0 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 137.8 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 100.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 48.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 124.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 149.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 133.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 114.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.1/150.1 MB 111.4 MB/s eta 0:00:00\n",
      "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 148.8 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 183.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 212.8 MB/s eta 0:00:00\n",
      "Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 253.2/253.2 MB 89.0 MB/s eta 0:00:00\n",
      "Downloading pytorch_ignite-0.5.2-py3-none-any.whl (343 kB)\n",
      "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 159.4 MB/s eta 0:00:00\n",
      "Downloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/15.0 MB 123.7 MB/s eta 0:00:00\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
      "Building wheels for collected packages: prostate158\n",
      "Building wheel for prostate158 (pyproject.toml): started\n",
      "Building wheel for prostate158 (pyproject.toml): finished with status 'done'\n",
      "Created wheel for prostate158: filename=prostate158-0.0.0-py3-none-any.whl size=35479 sha256=d56485c5ac4b39b93badca46e6b49d91b150db76223574b7e76b1217d063f490\n",
      "Stored in directory: /tmp/pip-ephem-wheel-cache-_cssokln/wheels/6e/f2/4a/da972bde801c4b516b25757b9156b5d85fe41e8b907f1e3a0e\n",
      "Successfully built prostate158\n",
      "Installing collected packages: triton, prostate158, nvidia-cusparselt-cu12, tifffile, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nibabel, munch, lazy-loader, scikit-image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, pytorch-ignite, monai\n",
      "Attempting uninstall: triton\n",
      "Found existing installation: triton 3.3.1\n",
      "Uninstalling triton-3.3.1:\n",
      "Successfully uninstalled triton-3.3.1\n",
      "Attempting uninstall: nvidia-cusparselt-cu12\n",
      "Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "Attempting uninstall: sympy\n",
      "Found existing installation: sympy 1.13.3\n",
      "Uninstalling sympy-1.13.3:\n",
      "Successfully uninstalled sympy-1.13.3\n",
      "Attempting uninstall: nvidia-nvtx-cu12\n",
      "Found existing installation: nvidia-nvtx-cu12 12.8.55\n",
      "Uninstalling nvidia-nvtx-cu12-12.8.55:\n",
      "Successfully uninstalled nvidia-nvtx-cu12-12.8.55\n",
      "Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "Found existing installation: nvidia-nvjitlink-cu12 12.8.61\n",
      "Uninstalling nvidia-nvjitlink-cu12-12.8.61:\n",
      "Successfully uninstalled nvidia-nvjitlink-cu12-12.8.61\n",
      "Attempting uninstall: nvidia-nccl-cu12\n",
      "Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "Attempting uninstall: nvidia-curand-cu12\n",
      "Found existing installation: nvidia-curand-cu12 10.3.9.55\n",
      "Uninstalling nvidia-curand-cu12-10.3.9.55:\n",
      "Successfully uninstalled nvidia-curand-cu12-10.3.9.55\n",
      "Attempting uninstall: nvidia-cufft-cu12\n",
      "Found existing installation: nvidia-cufft-cu12 11.3.3.41\n",
      "Uninstalling nvidia-cufft-cu12-11.3.3.41:\n",
      "Successfully uninstalled nvidia-cufft-cu12-11.3.3.41\n",
      "Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "Found existing installation: nvidia-cuda-runtime-cu12 12.8.57\n",
      "Uninstalling nvidia-cuda-runtime-cu12-12.8.57:\n",
      "Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57\n",
      "Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61\n",
      "Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:\n",
      "Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61\n",
      "Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "Found existing installation: nvidia-cuda-cupti-cu12 12.8.57\n",
      "Uninstalling nvidia-cuda-cupti-cu12-12.8.57:\n",
      "Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57\n",
      "Attempting uninstall: nvidia-cublas-cu12\n",
      "Found existing installation: nvidia-cublas-cu12 12.8.3.14\n",
      "Uninstalling nvidia-cublas-cu12-12.8.3.14:\n",
      "Successfully uninstalled nvidia-cublas-cu12-12.8.3.14\n",
      "Attempting uninstall: nvidia-cusparse-cu12\n",
      "Found existing installation: nvidia-cusparse-cu12 12.5.7.53\n",
      "Uninstalling nvidia-cusparse-cu12-12.5.7.53:\n",
      "Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53\n",
      "Attempting uninstall: nvidia-cudnn-cu12\n",
      "Found existing installation: nvidia-cudnn-cu12 9.7.1.26\n",
      "Uninstalling nvidia-cudnn-cu12-9.7.1.26:\n",
      "Successfully uninstalled nvidia-cudnn-cu12-9.7.1.26\n",
      "Attempting uninstall: nvidia-cusolver-cu12\n",
      "Found existing installation: nvidia-cusolver-cu12 11.7.2.55\n",
      "Uninstalling nvidia-cusolver-cu12-11.7.2.55:\n",
      "Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55\n",
      "Attempting uninstall: torch\n",
      "Found existing installation: torch 2.7.1+cu128\n",
      "Uninstalling torch-2.7.1+cu128:\n",
      "Successfully uninstalled torch-2.7.1+cu128\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.1+cu128 requires torch==2.7.1, but you have torch 2.6.0 which is incompatible.\n",
      "torchvision 0.22.1+cu128 requires torch==2.7.1, but you have torch 2.6.0 which is incompatible.\n",
      "Successfully installed lazy-loader-0.4 monai-1.5.0 munch-4.0.0 nibabel-5.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 prostate158-0.0.0 pytorch-ignite-0.5.2 scikit-image-0.25.2 sympy-1.13.1 tifffile-2025.6.11 torch-2.6.0 triton-3.2.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "2025-07-17 08:49:16,993 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-07-17 08:49:16,993 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-07-17 08:49:17,035 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-17 08:49:17,063 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-17 08:49:17,089 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-07-17 08:49:17,098 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"models\": \"/opt/ml/input/data/models\",\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"training_config\": \"/opt/ml/input/data/training_config\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"models\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training_config\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2025-07-17-08-41-56-628\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-17-08-41-56-628/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_gpt\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train_gpt.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=train_gpt.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\n",
      "SM_INPUT_DATA_CONFIG={\"models\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training_config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"models\",\"training\",\"training_config\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train_gpt\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-17-08-41-56-628/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"models\":\"/opt/ml/input/data/models\",\"training\":\"/opt/ml/input/data/training\",\"training_config\":\"/opt/ml/input/data/training_config\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"models\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training_config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2025-07-17-08-41-56-628\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-17-08-41-56-628/source/sourcedir.tar.gz\",\"module_name\":\"train_gpt\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train_gpt.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_MODELS=/opt/ml/input/data/models\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_CHANNEL_TRAINING_CONFIG=/opt/ml/input/data/training_config\n",
      "PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python312.zip:/usr/local/lib/python3.12:/usr/local/lib/python3.12/lib-dynload:/usr/local/lib/python3.12/site-packages\n",
      "Invoking script with the following command:\n",
      "/usr/local/bin/python -m train_gpt\n",
      "2025-07-17 08:49:17,099 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2025-07-17 08:49:17,100 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "torch version: 2.6.0+cu124\n",
      "torch file:  /usr/local/lib/python3.12/site-packages/torch/__init__.py\n",
      "CUDA available? True\n",
      "CUDA device count: 1\n",
      "Device name: Tesla T4\n",
      "2025-07-17 08:49:22,475 - INFO - \n",
      "        Running supervised segmentation training on single GPU\n",
      "        Run ID:     tumor_1\n",
      "        Debug:      False\n",
      "        Out dir:    /opt/ml/output/data\n",
      "        Model dir:  /opt/ml/input/data/model/\n",
      "        Log dir:    tumor_1/logs\n",
      "        Images:     ['t2', 'adc', 'dwi']\n",
      "        Labels:     ['adc_tumor_reader1']\n",
      "        Data dir:   /opt/ml/input/data/training\n",
      "[get_model] model.type = 'unet'\n",
      "No previous checkpoint found. Starting from scratch.\n",
      "/usr/local/lib/python3.12/site-packages/monai/engines/trainer.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if self.amp else None\n",
      "2025-07-17 08:49:23,178 - INFO - Engine run resuming from iteration 0, epoch 0 until 5 epochs\n",
      "2025-07-17 08:49:38,532 - INFO - Epoch: 1/5, Iter: 1/119 -- train_loss: 1.0018\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [1/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [1/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "2025-07-17 08:49:38,537 - INFO - Current learning rate: 4.00751801062372e-05\n",
      "2025-07-17 08:49:38,674 - INFO - Epoch: 1/5, Iter: 2/119 -- train_loss: 1.0029\n",
      "Epoch [1/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [1/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [1/5]: [2/119]   2%|▏         , loss=1 [00:00<00:16]\n",
      "2025-07-17 08:49:38,676 - INFO - Current learning rate: 4.0300696874747094e-05\n",
      "2025-07-17 08:49:41,058 - INFO - Epoch: 1/5, Iter: 3/119 -- train_loss: 1.0016\n",
      "Epoch [1/5]: [2/119]   2%|▏         , loss=1 [00:02<00:16]\n",
      "Epoch [1/5]: [2/119]   2%|▏         , loss=1 [00:02<00:16]\n",
      "Epoch [1/5]: [3/119]   3%|▎         , loss=1 [00:02<02:49]\n",
      "2025-07-17 08:49:41,059 - INFO - Current learning rate: 4.067647966230208e-05\n",
      "2025-07-17 08:49:41,186 - INFO - Epoch: 1/5, Iter: 4/119 -- train_loss: 1.0006\n",
      "Epoch [1/5]: [3/119]   3%|▎         , loss=1 [00:02<02:49]\n",
      "Epoch [1/5]: [3/119]   3%|▎         , loss=1 [00:02<02:49]\n",
      "Epoch [1/5]: [4/119]   3%|▎         , loss=1 [00:02<01:37]\n",
      "2025-07-17 08:49:41,188 - INFO - Current learning rate: 4.120241075477769e-05\n",
      "2025-07-17 08:49:43,194 - INFO - Epoch: 1/5, Iter: 5/119 -- train_loss: 0.8753\n",
      "Epoch [1/5]: [4/119]   3%|▎         , loss=1 [00:04<01:37]\n",
      "Epoch [1/5]: [4/119]   3%|▎         , loss=0.875 [00:04<01:37]\n",
      "Epoch [1/5]: [5/119]   4%|▍         , loss=0.875 [00:04<02:29]\n",
      "2025-07-17 08:49:43,196 - INFO - Current learning rate: 4.18783254040265e-05\n",
      "2025-07-17 08:49:54,032 - INFO - Epoch: 1/5, Iter: 6/119 -- train_loss: 1.0009\n",
      "Epoch [1/5]: [5/119]   4%|▍         , loss=0.875 [00:15<02:29]\n",
      "Epoch [1/5]: [5/119]   4%|▍         , loss=1 [00:15<02:29]\n",
      "Epoch [1/5]: [6/119]   5%|▌         , loss=1 [00:15<08:56]\n",
      "2025-07-17 08:49:54,033 - INFO - Current learning rate: 4.2704011879485205e-05\n",
      "2025-07-17 08:49:54,140 - INFO - Epoch: 1/5, Iter: 7/119 -- train_loss: 0.7197\n",
      "Epoch [1/5]: [6/119]   5%|▌         , loss=1 [00:15<08:56]\n",
      "Epoch [1/5]: [6/119]   5%|▌         , loss=0.72 [00:15<08:56]\n",
      "Epoch [1/5]: [7/119]   6%|▌         , loss=0.72 [00:15<05:54]\n",
      "2025-07-17 08:49:54,141 - INFO - Current learning rate: 4.367921153450023e-05\n",
      "2025-07-17 08:49:54,244 - INFO - Epoch: 1/5, Iter: 8/119 -- train_loss: 1.0013\n",
      "Epoch [1/5]: [7/119]   6%|▌         , loss=0.72 [00:15<05:54]\n",
      "Epoch [1/5]: [7/119]   6%|▌         , loss=1 [00:15<05:54]\n",
      "Epoch [1/5]: [8/119]   7%|▋         , loss=1 [00:15<04:00]\n",
      "2025-07-17 08:49:54,246 - INFO - Current learning rate: 4.4803618887347563e-05\n",
      "2025-07-17 08:49:54,500 - INFO - Epoch: 1/5, Iter: 9/119 -- train_loss: 1.0012\n",
      "Epoch [1/5]: [8/119]   7%|▋         , loss=1 [00:15<04:00]\n",
      "Epoch [1/5]: [8/119]   7%|▋         , loss=1 [00:15<04:00]\n",
      "Epoch [1/5]: [9/119]   8%|▊         , loss=1 [00:15<02:51]\n",
      "2025-07-17 08:49:54,501 - INFO - Current learning rate: 4.607688171692629e-05\n",
      "2025-07-17 08:50:02,666 - INFO - Epoch: 1/5, Iter: 10/119 -- train_loss: 1.0014\n",
      "Epoch [1/5]: [9/119]   8%|▊         , loss=1 [00:24<02:51]\n",
      "Epoch [1/5]: [9/119]   8%|▊         , loss=1 [00:24<02:51]\n",
      "Epoch [1/5]: [10/119]   8%|▊         , loss=1 [00:24<06:35]\n",
      "2025-07-17 08:50:02,668 - INFO - Current learning rate: 4.749860117309176e-05\n",
      "2025-07-17 08:50:05,778 - INFO - Epoch: 1/5, Iter: 11/119 -- train_loss: 1.0025\n",
      "Epoch [1/5]: [10/119]   8%|▊         , loss=1 [00:27<06:35]\n",
      "Epoch [1/5]: [10/119]   8%|▊         , loss=1 [00:27<06:35]\n",
      "Epoch [1/5]: [11/119]   9%|▉         , loss=1 [00:27<06:14]2025-07-17 08:50:05,780 - INFO - Current learning rate: 4.906833190159452e-05\n",
      "2025-07-17 08:50:05,910 - INFO - Epoch: 1/5, Iter: 12/119 -- train_loss: 1.0007\n",
      "Epoch [1/5]: [11/119]   9%|▉         , loss=1 [00:27<06:14]\n",
      "Epoch [1/5]: [11/119]   9%|▉         , loss=1 [00:27<06:14]\n",
      "Epoch [1/5]: [12/119]  10%|█         , loss=1 [00:27<04:21]\n",
      "2025-07-17 08:50:05,911 - INFO - Current learning rate: 5.078558218358934e-05\n",
      "2025-07-17 08:50:07,114 - INFO - Epoch: 1/5, Iter: 13/119 -- train_loss: 1.0007\n",
      "Epoch [1/5]: [12/119]  10%|█         , loss=1 [00:28<04:21]\n",
      "Epoch [1/5]: [12/119]  10%|█         , loss=1 [00:28<04:21]\n",
      "Epoch [1/5]: [13/119]  11%|█         , loss=1 [00:28<03:39]\n",
      "2025-07-17 08:50:07,116 - INFO - Current learning rate: 5.2649814089665244e-05\n",
      "2025-07-17 08:50:13,332 - INFO - Epoch: 1/5, Iter: 14/119 -- train_loss: 0.9462\n",
      "Epoch [1/5]: [13/119]  11%|█         , loss=1 [00:34<03:39]\n",
      "Epoch [1/5]: [13/119]  11%|█         , loss=0.946 [00:34<03:39]\n",
      "Epoch [1/5]: [14/119]  12%|█▏        , loss=0.946 [00:34<05:49]\n",
      "2025-07-17 08:50:13,335 - INFO - Current learning rate: 5.466044364835221e-05\n",
      "2025-07-17 08:50:14,359 - INFO - Epoch: 1/5, Iter: 15/119 -- train_loss: 0.8320\n",
      "Epoch [1/5]: [14/119]  12%|█▏        , loss=0.946 [00:35<05:49]\n",
      "Epoch [1/5]: [14/119]  12%|█▏        , loss=0.832 [00:35<05:49]\n",
      "Epoch [1/5]: [15/119]  13%|█▎        , loss=0.832 [00:35<04:33]\n",
      "2025-07-17 08:50:14,361 - INFO - Current learning rate: 5.6816841029051204e-05\n",
      "2025-07-17 08:50:14,515 - INFO - Epoch: 1/5, Iter: 16/119 -- train_loss: 1.0009\n",
      "Epoch [1/5]: [15/119]  13%|█▎        , loss=0.832 [00:35<04:33]\n",
      "Epoch [1/5]: [15/119]  13%|█▎        , loss=1 [00:35<04:33]\n",
      "Epoch [1/5]: [16/119]  13%|█▎        , loss=1 [00:35<03:14]\n",
      "2025-07-17 08:50:14,518 - INFO - Current learning rate: 5.9118330739328e-05\n",
      "2025-07-17 08:50:14,652 - INFO - Epoch: 1/5, Iter: 17/119 -- train_loss: 1.0014\n",
      "Epoch [1/5]: [16/119]  13%|█▎        , loss=1 [00:36<03:14]\n",
      "Epoch [1/5]: [16/119]  13%|█▎        , loss=1 [00:36<03:14]\n",
      "Epoch [1/5]: [17/119]  14%|█▍        , loss=1 [00:36<02:18]\n",
      "2025-07-17 08:50:14,653 - INFO - Current learning rate: 6.156419183651223e-05\n",
      "2025-07-17 08:50:21,720 - INFO - Epoch: 1/5, Iter: 18/119 -- train_loss: 1.0007\n",
      "Epoch [1/5]: [17/119]  14%|█▍        , loss=1 [00:43<02:18]\n",
      "Epoch [1/5]: [17/119]  14%|█▍        , loss=1 [00:43<02:18]\n",
      "Epoch [1/5]: [18/119]  15%|█▌        , loss=1 [00:43<05:10]\n",
      "2025-07-17 08:50:21,723 - INFO - Current learning rate: 6.415365815353239e-05\n",
      "2025-07-17 08:50:21,846 - INFO - Epoch: 1/5, Iter: 19/119 -- train_loss: 1.0012\n",
      "Epoch [1/5]: [18/119]  15%|█▌        , loss=1 [00:43<05:10]\n",
      "Epoch [1/5]: [18/119]  15%|█▌        , loss=1 [00:43<05:10]\n",
      "Epoch [1/5]: [19/119]  16%|█▌        , loss=1 [00:43<03:38]\n",
      "2025-07-17 08:50:21,847 - INFO - Current learning rate: 6.688591853891868e-05\n",
      "2025-07-17 08:50:21,978 - INFO - Epoch: 1/5, Iter: 20/119 -- train_loss: 1.0005\n",
      "Epoch [1/5]: [19/119]  16%|█▌        , loss=1 [00:43<03:38]\n",
      "Epoch [1/5]: [19/119]  16%|█▌        , loss=1 [00:43<03:38]\n",
      "Epoch [1/5]: [20/119]  17%|█▋        , loss=1 [00:43<02:35]\n",
      "2025-07-17 08:50:21,981 - INFO - Current learning rate: 6.976011711089562e-05\n",
      "2025-07-17 08:50:27,695 - INFO - Epoch: 1/5, Iter: 21/119 -- train_loss: 1.0006\n",
      "Epoch [1/5]: [20/119]  17%|█▋        , loss=1 [00:49<02:35]\n",
      "Epoch [1/5]: [20/119]  17%|█▋        , loss=1 [00:49<02:35]\n",
      "Epoch [1/5]: [21/119]  18%|█▊        , loss=1 [00:49<04:36]\n",
      "2025-07-17 08:50:27,699 - INFO - Current learning rate: 7.277535352548836e-05\n",
      "2025-07-17 08:50:27,989 - INFO - Epoch: 1/5, Iter: 22/119 -- train_loss: 0.8266\n",
      "Epoch [1/5]: [21/119]  18%|█▊        , loss=1 [00:49<04:36]\n",
      "Epoch [1/5]: [21/119]  18%|█▊        , loss=0.827 [00:49<04:36]\n",
      "Epoch [1/5]: [22/119]  18%|█▊        , loss=0.827 [00:49<03:19]\n",
      "2025-07-17 08:50:27,990 - INFO - Current learning rate: 7.593068325855551e-05\n",
      "2025-07-17 08:50:29,315 - INFO - Epoch: 1/5, Iter: 23/119 -- train_loss: 1.0008\n",
      "Epoch [1/5]: [22/119]  18%|█▊        , loss=0.827 [00:50<03:19]\n",
      "Epoch [1/5]: [22/119]  18%|█▊        , loss=1 [00:50<03:19]\n",
      "Epoch [1/5]: [23/119]  19%|█▉        , loss=1 [00:50<02:56]\n",
      "2025-07-17 08:50:29,318 - INFO - Current learning rate: 7.922511790166124e-05\n",
      "2025-07-17 08:50:33,112 - INFO - Epoch: 1/5, Iter: 24/119 -- train_loss: 1.0007\n",
      "Epoch [1/5]: [23/119]  19%|█▉        , loss=1 [00:54<02:56]\n",
      "Epoch [1/5]: [23/119]  19%|█▉        , loss=1 [00:54<02:56]\n",
      "Epoch [1/5]: [24/119]  20%|██        , loss=1 [00:54<03:50]\n",
      "2025-07-17 08:50:33,115 - INFO - Current learning rate: 8.265762547169536e-05\n",
      "2025-07-17 08:50:37,747 - INFO - Epoch: 1/5, Iter: 25/119 -- train_loss: 1.0010\n",
      "Epoch [1/5]: [24/119]  20%|██        , loss=1 [00:59<03:50]\n",
      "Epoch [1/5]: [24/119]  20%|██        , loss=1 [00:59<03:50]\n",
      "Epoch [1/5]: [25/119]  21%|██        , loss=1 [00:59<04:50]\n",
      "2025-07-17 08:50:37,749 - INFO - Current learning rate: 8.622713073414218e-05\n",
      "2025-07-17 08:50:37,875 - INFO - Epoch: 1/5, Iter: 26/119 -- train_loss: 1.0008\n",
      "Epoch [1/5]: [25/119]  21%|██        , loss=1 [00:59<04:50]\n",
      "Epoch [1/5]: [25/119]  21%|██        , loss=1 [00:59<04:50]\n",
      "Epoch [1/5]: [26/119]  22%|██▏       , loss=1 [00:59<03:24]\n",
      "2025-07-17 08:50:37,876 - INFO - Current learning rate: 8.993251553989813e-05\n",
      "2025-07-17 08:50:38,010 - INFO - Epoch: 1/5, Iter: 27/119 -- train_loss: 1.0394\n",
      "Epoch [1/5]: [26/119]  22%|██▏       , loss=1 [00:59<03:24]\n",
      "Epoch [1/5]: [26/119]  22%|██▏       , loss=1.04 [00:59<03:24]\n",
      "Epoch [1/5]: [27/119]  23%|██▎       , loss=1.04 [00:59<02:25]\n",
      "2025-07-17 08:50:38,012 - INFO - Current learning rate: 9.377261917553267e-05\n",
      "2025-07-17 08:50:39,320 - INFO - Epoch: 1/5, Iter: 28/119 -- train_loss: 0.9140\n",
      "Epoch [1/5]: [27/119]  23%|██▎       , loss=1.04 [01:00<02:25]\n",
      "Epoch [1/5]: [27/119]  23%|██▎       , loss=0.914 [01:00<02:25]\n",
      "Epoch [1/5]: [28/119]  24%|██▎       , loss=0.914 [01:00<02:16]\n",
      "2025-07-17 08:50:39,322 - INFO - Current learning rate: 9.774623872688202e-05\n",
      "2025-07-17 08:50:47,401 - INFO - Epoch: 1/5, Iter: 29/119 -- train_loss: 0.9047\n",
      "Epoch [1/5]: [28/119]  24%|██▎       , loss=0.914 [01:08<02:16]\n",
      "Epoch [1/5]: [28/119]  24%|██▎       , loss=0.905 [01:08<02:16]\n",
      "Epoch [1/5]: [29/119]  24%|██▍       , loss=0.905 [01:08<05:12]\n",
      "2025-07-17 08:50:47,403 - INFO - Current learning rate: 0.00010185212945586274\n",
      "2025-07-17 08:50:53,124 - INFO - Epoch: 1/5, Iter: 30/119 -- train_loss: 1.0008\n",
      "Epoch [1/5]: [29/119]  24%|██▍       , loss=0.905 [01:14<05:12]\n",
      "Epoch [1/5]: [29/119]  24%|██▍       , loss=1 [01:14<05:12]\n",
      "Epoch [1/5]: [30/119]  25%|██▌       , loss=1 [01:14<06:09]\n",
      "2025-07-17 08:50:53,127 - INFO - Current learning rate: 0.0001060890051903866\n",
      "2025-07-17 08:50:53,248 - INFO - Epoch: 1/5, Iter: 31/119 -- train_loss: 1.0011\n",
      "Epoch [1/5]: [30/119]  25%|██▌       , loss=1 [01:14<06:09]\n",
      "Epoch [1/5]: [30/119]  25%|██▌       , loss=1 [01:14<06:09]\n",
      "Epoch [1/5]: [31/119]  26%|██▌       , loss=1 [01:14<04:18]\n",
      "2025-07-17 08:50:53,250 - INFO - Current learning rate: 0.00011045553872725429\n",
      "2025-07-17 08:50:53,371 - INFO - Epoch: 1/5, Iter: 32/119 -- train_loss: 1.0018\n",
      "Epoch [1/5]: [31/119]  26%|██▌       , loss=1 [01:14<04:18]\n",
      "Epoch [1/5]: [31/119]  26%|██▌       , loss=1 [01:14<04:18]\n",
      "Epoch [1/5]: [32/119]  27%|██▋       , loss=1 [01:14<03:02]\n",
      "2025-07-17 08:50:53,373 - INFO - Current learning rate: 0.00011495036224790252\n",
      "2025-07-17 08:50:59,613 - INFO - Epoch: 1/5, Iter: 33/119 -- train_loss: 0.9328\n",
      "Epoch [1/5]: [32/119]  27%|██▋       , loss=1 [01:21<03:02]\n",
      "Epoch [1/5]: [32/119]  27%|██▋       , loss=0.933 [01:21<03:02]\n",
      "Epoch [1/5]: [33/119]  28%|██▊       , loss=0.933 [01:21<04:47]\n",
      "2025-07-17 08:50:59,615 - INFO - Current learning rate: 0.00011957206774687373\n",
      "2025-07-17 08:51:00,856 - INFO - Epoch: 1/5, Iter: 34/119 -- train_loss: 0.8513\n",
      "Epoch [1/5]: [33/119]  28%|██▊       , loss=0.933 [01:22<04:47]\n",
      "Epoch [1/5]: [33/119]  28%|██▊       , loss=0.851 [01:22<04:47]\n",
      "Epoch [1/5]: [34/119]  29%|██▊       , loss=0.851 [01:22<03:50]\n",
      "2025-07-17 08:51:00,857 - INFO - Current learning rate: 0.0001243192074728745\n",
      "2025-07-17 08:51:01,008 - INFO - Epoch: 1/5, Iter: 35/119 -- train_loss: 1.0010\n",
      "Epoch [1/5]: [34/119]  29%|██▊       , loss=0.851 [01:22<03:50]\n",
      "Epoch [1/5]: [34/119]  29%|██▊       , loss=1 [01:22<03:50]\n",
      "Epoch [1/5]: [35/119]  29%|██▉       , loss=1 [01:22<02:43]\n",
      "2025-07-17 08:51:01,009 - INFO - Current learning rate: 0.00012919029438228423\n",
      "2025-07-17 08:51:01,186 - INFO - Epoch: 1/5, Iter: 36/119 -- train_loss: 1.0132\n",
      "Epoch [1/5]: [35/119]  29%|██▉       , loss=1 [01:22<02:43]\n",
      "Epoch [1/5]: [35/119]  29%|██▉       , loss=1.01 [01:22<02:43]\n",
      "Epoch [1/5]: [36/119]  30%|███       , loss=1.01 [01:22<01:57]\n",
      "2025-07-17 08:51:01,189 - INFO - Current learning rate: 0.0001341838026049729\n",
      "2025-07-17 08:51:11,294 - INFO - Epoch: 1/5, Iter: 37/119 -- train_loss: 1.0014\n",
      "Epoch [1/5]: [36/119]  30%|███       , loss=1.01 [01:32<01:57]\n",
      "Epoch [1/5]: [36/119]  30%|███       , loss=1 [01:32<01:57]\n",
      "Epoch [1/5]: [37/119]  31%|███       , loss=1 [01:32<05:29]\n",
      "2025-07-17 08:51:11,296 - INFO - Current learning rate: 0.00013929816792227867\n",
      "2025-07-17 08:51:11,414 - INFO - Epoch: 1/5, Iter: 38/119 -- train_loss: 0.9715\n",
      "Epoch [1/5]: [37/119]  31%|███       , loss=1 [01:32<05:29]\n",
      "Epoch [1/5]: [37/119]  31%|███       , loss=0.971 [01:32<05:29]\n",
      "Epoch [1/5]: [38/119]  32%|███▏      , loss=0.971 [01:32<03:50]\n",
      "2025-07-17 08:51:11,415 - INFO - Current learning rate: 0.00014453178825700225\n",
      "2025-07-17 08:51:11,545 - INFO - Epoch: 1/5, Iter: 39/119 -- train_loss: 0.9020\n",
      "Epoch [1/5]: [38/119]  32%|███▏      , loss=0.971 [01:33<03:50]\n",
      "Epoch [1/5]: [38/119]  32%|███▏      , loss=0.902 [01:33<03:50]\n",
      "Epoch [1/5]: [39/119]  33%|███▎      , loss=0.902 [01:33<02:42]\n",
      "2025-07-17 08:51:11,547 - INFO - Current learning rate: 0.00014988302417525713\n",
      "2025-07-17 08:51:11,685 - INFO - Epoch: 1/5, Iter: 40/119 -- train_loss: 1.0016\n",
      "Epoch [1/5]: [39/119]  33%|███▎      , loss=0.902 [01:33<02:42]\n",
      "Epoch [1/5]: [39/119]  33%|███▎      , loss=1 [01:33<02:42]\n",
      "Epoch [1/5]: [40/119]  34%|███▎      , loss=1 [01:33<01:55]\n",
      "2025-07-17 08:51:11,688 - INFO - Current learning rate: 0.00015535019940002325\n",
      "2025-07-17 08:51:19,595 - INFO - Epoch: 1/5, Iter: 41/119 -- train_loss: 0.9827\n",
      "Epoch [1/5]: [40/119]  34%|███▎      , loss=1 [01:41<01:55]\n",
      "Epoch [1/5]: [40/119]  34%|███▎      , loss=0.983 [01:41<01:55]\n",
      "Epoch [1/5]: [41/119]  34%|███▍      , loss=0.983 [01:41<04:25]\n",
      "2025-07-17 08:51:19,597 - INFO - Current learning rate: 0.00016093160133624112\n",
      "2025-07-17 08:51:19,733 - INFO - Epoch: 1/5, Iter: 42/119 -- train_loss: 0.8439\n",
      "Epoch [1/5]: [41/119]  34%|███▍      , loss=0.983 [01:41<04:25]\n",
      "Epoch [1/5]: [41/119]  34%|███▍      , loss=0.844 [01:41<04:25]\n",
      "Epoch [1/5]: [42/119]  35%|███▌      , loss=0.844 [01:41<03:06]\n",
      "2025-07-17 08:51:19,734 - INFO - Current learning rate: 0.0001666254816072839\n",
      "2025-07-17 08:51:19,836 - INFO - Epoch: 1/5, Iter: 43/119 -- train_loss: 0.8998\n",
      "Epoch [1/5]: [42/119]  35%|███▌      , loss=0.844 [01:41<03:06]\n",
      "Epoch [1/5]: [42/119]  35%|███▌      , loss=0.9 [01:41<03:06]\n",
      "Epoch [1/5]: [43/119]  36%|███▌      , loss=0.9 [01:41<02:11]\n",
      "2025-07-17 08:51:19,838 - INFO - Current learning rate: 0.0001724300566026361\n",
      "2025-07-17 08:51:19,970 - INFO - Epoch: 1/5, Iter: 44/119 -- train_loss: 1.0012\n",
      "Epoch [1/5]: [43/119]  36%|███▌      , loss=0.9 [01:41<02:11]\n",
      "Epoch [1/5]: [43/119]  36%|███▌      , loss=1 [01:41<02:11]\n",
      "Epoch [1/5]: [44/119]  37%|███▋      , loss=1 [01:41<01:33]\n",
      "2025-07-17 08:51:19,971 - INFO - Current learning rate: 0.0001783435080366109\n",
      "2025-07-17 08:51:27,834 - INFO - Epoch: 1/5, Iter: 45/119 -- train_loss: 0.9646\n",
      "Epoch [1/5]: [44/119]  37%|███▋      , loss=1 [01:49<01:33]\n",
      "Epoch [1/5]: [44/119]  37%|███▋      , loss=0.965 [01:49<01:33]\n",
      "Epoch [1/5]: [45/119]  38%|███▊      , loss=0.965 [01:49<03:59]\n",
      "2025-07-17 08:51:27,836 - INFO - Current learning rate: 0.0001843639835179292\n",
      "2025-07-17 08:51:27,967 - INFO - Epoch: 1/5, Iter: 46/119 -- train_loss: 1.0014\n",
      "Epoch [1/5]: [45/119]  38%|███▊      , loss=0.965 [01:49<03:59]\n",
      "Epoch [1/5]: [45/119]  38%|███▊      , loss=1 [01:49<03:59]\n",
      "Epoch [1/5]: [46/119]  39%|███▊      , loss=1 [01:49<02:48]\n",
      "2025-07-17 08:51:27,969 - INFO - Current learning rate: 0.00019048959712998138\n",
      "2025-07-17 08:51:28,080 - INFO - Epoch: 1/5, Iter: 47/119 -- train_loss: 0.8524\n",
      "Epoch [1/5]: [46/119]  39%|███▊      , loss=1 [01:49<02:48]\n",
      "Epoch [1/5]: [46/119]  39%|███▊      , loss=0.852 [01:49<02:48]\n",
      "Epoch [1/5]: [47/119]  39%|███▉      , loss=0.852 [01:49<01:58]\n",
      "2025-07-17 08:51:28,081 - INFO - Current learning rate: 0.00019671843002159193\n",
      "2025-07-17 08:51:28,224 - INFO - Epoch: 1/5, Iter: 48/119 -- train_loss: 0.9773\n",
      "Epoch [1/5]: [47/119]  39%|███▉      , loss=0.852 [01:49<01:58]\n",
      "Epoch [1/5]: [47/119]  39%|███▉      , loss=0.977 [01:49<01:58]\n",
      "Epoch [1/5]: [48/119]  40%|████      , loss=0.977 [01:49<01:24]\n",
      "2025-07-17 08:51:28,226 - INFO - Current learning rate: 0.0002030485310081004\n",
      "2025-07-17 08:51:37,116 - INFO - Epoch: 1/5, Iter: 49/119 -- train_loss: 0.9019\n",
      "Epoch [1/5]: [48/119]  40%|████      , loss=0.977 [01:58<01:24]\n",
      "Epoch [1/5]: [48/119]  40%|████      , loss=0.902 [01:58<01:24]\n",
      "Epoch [1/5]: [49/119]  41%|████      , loss=0.902 [01:58<04:05]\n",
      "2025-07-17 08:51:37,118 - INFO - Current learning rate: 0.00020947791718257057\n",
      "2025-07-17 08:51:37,245 - INFO - Epoch: 1/5, Iter: 50/119 -- train_loss: 0.9384\n",
      "Epoch [1/5]: [49/119]  41%|████      , loss=0.902 [01:58<04:05]\n",
      "Epoch [1/5]: [49/119]  41%|████      , loss=0.938 [01:58<04:05]\n",
      "Epoch [1/5]: [50/119]  42%|████▏     , loss=0.938 [01:58<02:51]\n",
      "2025-07-17 08:51:37,246 - INFO - Current learning rate: 0.00021600457453693745\n",
      "2025-07-17 08:51:37,415 - INFO - Epoch: 1/5, Iter: 51/119 -- train_loss: 1.0063\n",
      "Epoch [1/5]: [50/119]  42%|████▏     , loss=0.938 [01:58<02:51]\n",
      "Epoch [1/5]: [50/119]  42%|████▏     , loss=1.01 [01:58<02:51]\n",
      "Epoch [1/5]: [51/119]  43%|████▎     , loss=1.01 [01:58<02:02]\n",
      "2025-07-17 08:51:37,417 - INFO - Current learning rate: 0.00022262645859289572\n",
      "2025-07-17 08:51:37,569 - INFO - Epoch: 1/5, Iter: 52/119 -- train_loss: 1.0017\n",
      "Epoch [1/5]: [51/119]  43%|████▎     , loss=1.01 [01:59<02:02]\n",
      "Epoch [1/5]: [51/119]  43%|████▎     , loss=1 [01:59<02:02]\n",
      "Epoch [1/5]: [52/119]  44%|████▎     , loss=1 [01:59<01:27]\n",
      "2025-07-17 08:51:37,571 - INFO - Current learning rate: 0.0002293414950423334\n",
      "2025-07-17 08:51:47,642 - INFO - Epoch: 1/5, Iter: 53/119 -- train_loss: 1.0021\n",
      "Epoch [1/5]: [52/119]  44%|████▎     , loss=1 [02:09<01:27]\n",
      "Epoch [1/5]: [52/119]  44%|████▎     , loss=1 [02:09<01:27]\n",
      "Epoch [1/5]: [53/119]  45%|████▍     , loss=1 [02:09<04:19]\n",
      "2025-07-17 08:51:47,644 - INFO - Current learning rate: 0.0002361475803971106\n",
      "2025-07-17 08:51:48,429 - INFO - Epoch: 1/5, Iter: 54/119 -- train_loss: 0.9311\n",
      "Epoch [1/5]: [53/119]  45%|████▍     , loss=1 [02:09<04:19]\n",
      "Epoch [1/5]: [53/119]  45%|████▍     , loss=0.931 [02:09<04:19]\n",
      "Epoch [1/5]: [54/119]  45%|████▌     , loss=0.931 [02:09<03:14]\n",
      "2025-07-17 08:51:48,432 - INFO - Current learning rate: 0.0002430425826479769\n",
      "2025-07-17 08:51:48,558 - INFO - Epoch: 1/5, Iter: 55/119 -- train_loss: 0.8968\n",
      "Epoch [1/5]: [54/119]  45%|████▌     , loss=0.931 [02:10<03:14]\n",
      "Epoch [1/5]: [54/119]  45%|████▌     , loss=0.897 [02:10<03:14]\n",
      "Epoch [1/5]: [55/119]  46%|████▌     , loss=0.897 [02:10<02:16]\n",
      "2025-07-17 08:51:48,559 - INFO - Current learning rate: 0.00025002434193242525\n",
      "2025-07-17 08:51:48,662 - INFO - Epoch: 1/5, Iter: 56/119 -- train_loss: 0.9621\n",
      "Epoch [1/5]: [55/119]  46%|████▌     , loss=0.897 [02:10<02:16]\n",
      "Epoch [1/5]: [55/119]  46%|████▌     , loss=0.962 [02:10<02:16]\n",
      "Epoch [1/5]: [56/119]  47%|████▋     , loss=0.962 [02:10<01:35]\n",
      "2025-07-17 08:51:48,663 - INFO - Current learning rate: 0.0002570906712112695\n",
      "2025-07-17 08:51:57,893 - INFO - Epoch: 1/5, Iter: 57/119 -- train_loss: 1.0024\n",
      "Epoch [1/5]: [56/119]  47%|████▋     , loss=0.962 [02:19<01:35]\n",
      "Epoch [1/5]: [56/119]  47%|████▋     , loss=1 [02:19<01:35]\n",
      "Epoch [1/5]: [57/119]  48%|████▊     , loss=1 [02:19<03:57]\n",
      "2025-07-17 08:51:57,895 - INFO - Current learning rate: 0.00026423935695373446\n",
      "2025-07-17 08:51:57,998 - INFO - Epoch: 1/5, Iter: 58/119 -- train_loss: 1.0019\n",
      "Epoch [1/5]: [57/119]  48%|████▊     , loss=1 [02:19<03:57]\n",
      "Epoch [1/5]: [57/119]  48%|████▊     , loss=1 [02:19<03:57]\n",
      "Epoch [1/5]: [58/119]  49%|████▊     , loss=1 [02:19<02:45]2025-07-17 08:51:57,999 - INFO - Current learning rate: 0.00027146815983084775\n",
      "2025-07-17 08:51:58,136 - INFO - Epoch: 1/5, Iter: 59/119 -- train_loss: 0.8636\n",
      "Epoch [1/5]: [58/119]  49%|████▊     , loss=1 [02:19<02:45]\n",
      "Epoch [1/5]: [58/119]  49%|████▊     , loss=0.864 [02:19<02:45]\n",
      "Epoch [1/5]: [59/119]  50%|████▉     , loss=0.864 [02:19<01:56]\n",
      "2025-07-17 08:51:58,137 - INFO - Current learning rate: 0.0002787748154169076\n",
      "2025-07-17 08:51:58,243 - INFO - Epoch: 1/5, Iter: 60/119 -- train_loss: 0.9077\n",
      "Epoch [1/5]: [59/119]  50%|████▉     , loss=0.864 [02:19<01:56]\n",
      "Epoch [1/5]: [59/119]  50%|████▉     , loss=0.908 [02:19<01:56]\n",
      "Epoch [1/5]: [60/119]  50%|█████     , loss=0.908 [02:19<01:22]\n",
      "2025-07-17 08:51:58,244 - INFO - Current learning rate: 0.00028615703489881766\n",
      "2025-07-17 08:52:04,854 - INFO - Epoch: 1/5, Iter: 61/119 -- train_loss: 0.7594\n",
      "Epoch [1/5]: [60/119]  50%|█████     , loss=0.908 [02:26<01:22]\n",
      "Epoch [1/5]: [60/119]  50%|█████     , loss=0.759 [02:26<01:22]\n",
      "Epoch [1/5]: [61/119]  51%|█████▏    , loss=0.759 [02:26<02:51]\n",
      "2025-07-17 08:52:04,858 - INFO - Current learning rate: 0.00029361250579305735\n",
      "2025-07-17 08:52:09,940 - INFO - Epoch: 1/5, Iter: 62/119 -- train_loss: 0.8817\n",
      "Epoch [1/5]: [61/119]  51%|█████▏    , loss=0.759 [02:31<02:51]\n",
      "Epoch [1/5]: [61/119]  51%|█████▏    , loss=0.882 [02:31<02:51]\n",
      "Epoch [1/5]: [62/119]  52%|█████▏    , loss=0.882 [02:31<03:24]2025-07-17 08:52:09,942 - INFO - Current learning rate: 0.0003011388926700688\n",
      "2025-07-17 08:52:10,068 - INFO - Epoch: 1/5, Iter: 63/119 -- train_loss: 1.0017\n",
      "Epoch [1/5]: [62/119]  52%|█████▏    , loss=0.882 [02:31<03:24]\n",
      "Epoch [1/5]: [62/119]  52%|█████▏    , loss=1 [02:31<03:24]\n",
      "Epoch [1/5]: [63/119]  53%|█████▎    , loss=1 [02:31<02:23]\n",
      "2025-07-17 08:52:10,069 - INFO - Current learning rate: 0.0003087338378858315\n",
      "2025-07-17 08:52:10,200 - INFO - Epoch: 1/5, Iter: 64/119 -- train_loss: 1.0027\n",
      "Epoch [1/5]: [63/119]  53%|█████▎    , loss=1 [02:31<02:23]\n",
      "Epoch [1/5]: [63/119]  53%|█████▎    , loss=1 [02:31<02:23]\n",
      "Epoch [1/5]: [64/119]  54%|█████▍    , loss=1 [02:31<01:40]\n",
      "2025-07-17 08:52:10,202 - INFO - Current learning rate: 0.00031639496232039417\n",
      "2025-07-17 08:52:10,587 - INFO - Epoch: 1/5, Iter: 65/119 -- train_loss: 1.0014\n",
      "Epoch [1/5]: [64/119]  54%|█████▍    , loss=1 [02:32<01:40]\n",
      "Epoch [1/5]: [64/119]  54%|█████▍    , loss=1 [02:32<01:40]\n",
      "Epoch [1/5]: [65/119]  55%|█████▍    , loss=1 [02:32<01:15]\n",
      "2025-07-17 08:52:10,590 - INFO - Current learning rate: 0.00032411986612313685\n",
      "2025-07-17 08:52:19,880 - INFO - Epoch: 1/5, Iter: 66/119 -- train_loss: 0.9308\n",
      "Epoch [1/5]: [65/119]  55%|█████▍    , loss=1 [02:41<01:15]\n",
      "Epoch [1/5]: [65/119]  55%|█████▍    , loss=0.931 [02:41<01:15]\n",
      "Epoch [1/5]: [66/119]  55%|█████▌    , loss=0.931 [02:41<03:19]\n",
      "2025-07-17 08:52:19,881 - INFO - Current learning rate: 0.0003319061294645237\n",
      "2025-07-17 08:52:19,985 - INFO - Epoch: 1/5, Iter: 67/119 -- train_loss: 1.0013\n",
      "Epoch [1/5]: [66/119]  55%|█████▌    , loss=0.931 [02:41<03:19]\n",
      "Epoch [1/5]: [66/119]  55%|█████▌    , loss=1 [02:41<03:19]\n",
      "Epoch [1/5]: [67/119]  56%|█████▋    , loss=1 [02:41<02:18]\n",
      "2025-07-17 08:52:19,986 - INFO - Current learning rate: 0.00033975131329411646\n",
      "2025-07-17 08:52:20,090 - INFO - Epoch: 1/5, Iter: 68/119 -- train_loss: 1.0165\n",
      "Epoch [1/5]: [67/119]  56%|█████▋    , loss=1 [02:41<02:18]\n",
      "Epoch [1/5]: [67/119]  56%|█████▋    , loss=1.02 [02:41<02:18]\n",
      "Epoch [1/5]: [68/119]  57%|█████▋    , loss=1.02 [02:41<01:36]\n",
      "2025-07-17 08:52:20,091 - INFO - Current learning rate: 0.00034765296010460774\n",
      "2025-07-17 08:52:20,195 - INFO - Epoch: 1/5, Iter: 69/119 -- train_loss: 1.0010\n",
      "Epoch [1/5]: [68/119]  57%|█████▋    , loss=1.02 [02:41<01:36]\n",
      "Epoch [1/5]: [68/119]  57%|█████▋    , loss=1 [02:41<01:36]\n",
      "Epoch [1/5]: [69/119]  58%|█████▊    , loss=1 [02:41<01:08]\n",
      "2025-07-17 08:52:20,196 - INFO - Current learning rate: 0.00035560859470163634\n",
      "2025-07-17 08:52:31,470 - INFO - Epoch: 1/5, Iter: 70/119 -- train_loss: 1.0014\n",
      "Epoch [1/5]: [69/119]  58%|█████▊    , loss=1 [02:52<01:08]\n",
      "Epoch [1/5]: [69/119]  58%|█████▊    , loss=1 [02:52<01:08]#015Epoch [1/5]: [70/119]  59%|█████▉    , loss=1 [02:52<03:32]\n",
      "2025-07-17 08:52:31,472 - INFO - Current learning rate: 0.0003636157249791435\n",
      "2025-07-17 08:52:31,604 - INFO - Epoch: 1/5, Iter: 71/119 -- train_loss: 1.0029\n",
      "Epoch [1/5]: [70/119]  59%|█████▉    , loss=1 [02:53<03:32]\n",
      "Epoch [1/5]: [70/119]  59%|█████▉    , loss=1 [02:53<03:32]\n",
      "Epoch [1/5]: [71/119]  60%|█████▉    , loss=1 [02:53<02:27]\n",
      "2025-07-17 08:52:31,606 - INFO - Current learning rate: 0.0003716718427000253\n",
      "2025-07-17 08:52:31,711 - INFO - Epoch: 1/5, Iter: 72/119 -- train_loss: 1.0009\n",
      "Epoch [1/5]: [71/119]  60%|█████▉    , loss=1 [02:53<02:27]\n",
      "Epoch [1/5]: [71/119]  60%|█████▉    , loss=1 [02:53<02:27]\n",
      "Epoch [1/5]: [72/119]  61%|██████    , loss=1 [02:53<01:42]\n",
      "2025-07-17 08:52:31,713 - INFO - Current learning rate: 0.0003797744242818392\n",
      "2025-07-17 08:52:31,841 - INFO - Epoch: 1/5, Iter: 73/119 -- train_loss: 1.0010\n",
      "Epoch [1/5]: [72/119]  61%|██████    , loss=1 [02:53<01:42]\n",
      "Epoch [1/5]: [72/119]  61%|██████    , loss=1 [02:53<01:42]\n",
      "Epoch [1/5]: [73/119]  61%|██████▏   , loss=1 [02:53<01:12]\n",
      "2025-07-17 08:52:31,843 - INFO - Current learning rate: 0.0003879209315873186\n",
      "2025-07-17 08:52:40,798 - INFO - Epoch: 1/5, Iter: 74/119 -- train_loss: 0.9365\n",
      "Epoch [1/5]: [73/119]  61%|██████▏   , loss=1 [03:02<01:12]\n",
      "Epoch [1/5]: [73/119]  61%|██████▏   , loss=0.936 [03:02<01:12]#015Epoch [1/5]: [74/119]  62%|██████▏   , loss=0.936 [03:02<02:50]\n",
      "2025-07-17 08:52:40,800 - INFO - Current learning rate: 0.00039610881271944355\n",
      "2025-07-17 08:52:40,952 - INFO - Epoch: 1/5, Iter: 75/119 -- train_loss: 1.0012\n",
      "Epoch [1/5]: [74/119]  62%|██████▏   , loss=0.936 [03:02<02:50]\n",
      "Epoch [1/5]: [74/119]  62%|██████▏   , loss=1 [03:02<02:50]\n",
      "Epoch [1/5]: [75/119]  63%|██████▎   , loss=1 [03:02<01:58]\n",
      "2025-07-17 08:52:40,953 - INFO - Current learning rate: 0.00040433550282082515\n",
      "2025-07-17 08:52:41,061 - INFO - Epoch: 1/5, Iter: 76/119 -- train_loss: 0.9836\n",
      "Epoch [1/5]: [75/119]  63%|██████▎   , loss=1 [03:02<01:58]\n",
      "Epoch [1/5]: [75/119]  63%|██████▎   , loss=0.984 [03:02<01:58]\n",
      "Epoch [1/5]: [76/119]  64%|██████▍   , loss=0.984 [03:02<01:22]\n",
      "2025-07-17 08:52:41,062 - INFO - Current learning rate: 0.0004125984248771475\n",
      "2025-07-17 08:52:41,166 - INFO - Epoch: 1/5, Iter: 77/119 -- train_loss: 0.7313\n",
      "Epoch [1/5]: [76/119]  64%|██████▍   , loss=0.984 [03:02<01:22]\n",
      "Epoch [1/5]: [76/119]  64%|██████▍   , loss=0.731 [03:02<01:22]\n",
      "Epoch [1/5]: [77/119]  65%|██████▍   , loss=0.731 [03:02<00:57]\n",
      "2025-07-17 08:52:41,167 - INFO - Current learning rate: 0.000420894990524419\n",
      "2025-07-17 08:52:47,546 - INFO - Epoch: 1/5, Iter: 78/119 -- train_loss: 0.9304\n",
      "Epoch [1/5]: [77/119]  65%|██████▍   , loss=0.731 [03:09<00:57]\n",
      "Epoch [1/5]: [77/119]  65%|██████▍   , loss=0.93 [03:09<00:57]\n",
      "Epoch [1/5]: [78/119]  66%|██████▌   , loss=0.93 [03:09<01:57]\n",
      "2025-07-17 08:52:47,548 - INFO - Current learning rate: 0.000429222600859778\n",
      "2025-07-17 08:52:47,684 - INFO - Epoch: 1/5, Iter: 79/119 -- train_loss: 1.0015\n",
      "Epoch [1/5]: [78/119]  66%|██████▌   , loss=0.93 [03:09<01:57]\n",
      "Epoch [1/5]: [78/119]  66%|██████▌   , loss=1 [03:09<01:57]\n",
      "Epoch [1/5]: [79/119]  66%|██████▋   , loss=1 [03:09<01:22]\n",
      "2025-07-17 08:52:47,685 - INFO - Current learning rate: 0.0004375786472556008\n",
      "2025-07-17 08:52:47,830 - INFO - Epoch: 1/5, Iter: 80/119 -- train_loss: 0.8792\n",
      "Epoch [1/5]: [79/119]  66%|██████▋   , loss=1 [03:09<01:22]\n",
      "Epoch [1/5]: [79/119]  66%|██████▋   , loss=0.879 [03:09<01:22]#015Epoch [1/5]: [80/119]  67%|██████▋   , loss=0.879 [03:09<00:57]\n",
      "2025-07-17 08:52:47,832 - INFO - Current learning rate: 0.00044596051217665495\n",
      "2025-07-17 08:52:47,986 - INFO - Epoch: 1/5, Iter: 81/119 -- train_loss: 0.9325\n",
      "Epoch [1/5]: [80/119]  67%|██████▋   , loss=0.879 [03:09<00:57]\n",
      "Epoch [1/5]: [80/119]  67%|██████▋   , loss=0.932 [03:09<00:57]\n",
      "Epoch [1/5]: [81/119]  68%|██████▊   , loss=0.932 [03:09<00:41]\n",
      "2025-07-17 08:52:47,987 - INFO - Current learning rate: 0.00045436557000004356\n",
      "2025-07-17 08:52:58,587 - INFO - Epoch: 1/5, Iter: 82/119 -- train_loss: 0.8136\n",
      "Epoch [1/5]: [81/119]  68%|██████▊   , loss=0.932 [03:20<00:41]\n",
      "Epoch [1/5]: [81/119]  68%|██████▊   , loss=0.814 [03:20<00:41]\n",
      "Epoch [1/5]: [82/119]  69%|██████▉   , loss=0.814 [03:20<02:25]\n",
      "2025-07-17 08:52:58,589 - INFO - Current learning rate: 0.0004627911878376832\n",
      "2025-07-17 08:52:58,697 - INFO - Epoch: 1/5, Iter: 83/119 -- train_loss: 1.0020\n",
      "Epoch [1/5]: [82/119]  69%|██████▉   , loss=0.814 [03:20<02:25]\n",
      "Epoch [1/5]: [82/119]  69%|██████▉   , loss=1 [03:20<02:25]\n",
      "Epoch [1/5]: [83/119]  70%|██████▉   , loss=1 [03:20<01:40]\n",
      "2025-07-17 08:52:58,698 - INFO - Current learning rate: 0.00047123472636105643\n",
      "2025-07-17 08:52:58,801 - INFO - Epoch: 1/5, Iter: 84/119 -- train_loss: 1.0020\n",
      "Epoch [1/5]: [83/119]  70%|██████▉   , loss=1 [03:20<01:40]\n",
      "Epoch [1/5]: [83/119]  70%|██████▉   , loss=1 [03:20<01:40]\n",
      "Epoch [1/5]: [84/119]  71%|███████   , loss=1 [03:20<01:09]\n",
      "2025-07-17 08:52:58,803 - INFO - Current learning rate: 0.00047969354062798325\n",
      "2025-07-17 08:52:58,939 - INFO - Epoch: 1/5, Iter: 85/119 -- train_loss: 1.0010\n",
      "Epoch [1/5]: [84/119]  71%|███████   , loss=1 [03:20<01:09]\n",
      "Epoch [1/5]: [84/119]  71%|███████   , loss=1 [03:20<01:09]\n",
      "Epoch [1/5]: [85/119]  71%|███████▏  , loss=1 [03:20<00:48]\n",
      "2025-07-17 08:52:58,941 - INFO - Current learning rate: 0.0004881649809111501\n",
      "2025-07-17 08:53:06,752 - INFO - Epoch: 1/5, Iter: 86/119 -- train_loss: 0.9131\n",
      "Epoch [1/5]: [85/119]  71%|███████▏  , loss=1 [03:28<00:48]\n",
      "Epoch [1/5]: [85/119]  71%|███████▏  , loss=0.913 [03:28<00:48]#015Epoch [1/5]: [86/119]  72%|███████▏  , loss=0.913 [03:28<01:50]2025-07-17 08:53:06,754 - INFO - Current learning rate: 0.0004966463935281385\n",
      "2025-07-17 08:53:06,904 - INFO - Epoch: 1/5, Iter: 87/119 -- train_loss: 1.0019\n",
      "Epoch [1/5]: [86/119]  72%|███████▏  , loss=0.913 [03:28<01:50]\n",
      "Epoch [1/5]: [86/119]  72%|███████▏  , loss=1 [03:28<01:50]\n",
      "Epoch [1/5]: [87/119]  73%|███████▎  , loss=1 [03:28<01:16]\n",
      "2025-07-17 08:53:06,906 - INFO - Current learning rate: 0.0005051351216726919\n",
      "2025-07-17 08:53:07,016 - INFO - Epoch: 1/5, Iter: 88/119 -- train_loss: 0.9606\n",
      "Epoch [1/5]: [87/119]  73%|███████▎  , loss=1 [03:28<01:16]\n",
      "Epoch [1/5]: [87/119]  73%|███████▎  , loss=0.961 [03:28<01:16]\n",
      "Epoch [1/5]: [88/119]  74%|███████▍  , loss=0.961 [03:28<00:52]\n",
      "2025-07-17 08:53:07,017 - INFO - Current learning rate: 0.0005136285062469611\n",
      "2025-07-17 08:53:12,466 - INFO - Epoch: 1/5, Iter: 89/119 -- train_loss: 1.0012\n",
      "Epoch [1/5]: [88/119]  74%|███████▍  , loss=0.961 [03:33<00:52]\n",
      "Epoch [1/5]: [88/119]  74%|███████▍  , loss=1 [03:33<00:52]\n",
      "Epoch [1/5]: [89/119]  75%|███████▍  , loss=1 [03:33<01:24]\n",
      "2025-07-17 08:53:12,468 - INFO - Current learning rate: 0.000522123886694469\n",
      "2025-07-17 08:53:13,056 - INFO - Epoch: 1/5, Iter: 90/119 -- train_loss: 1.0031\n",
      "Epoch [1/5]: [89/119]  75%|███████▍  , loss=1 [03:34<01:24]\n",
      "Epoch [1/5]: [89/119]  75%|███████▍  , loss=1 [03:34<01:24]\n",
      "Epoch [1/5]: [90/119]  76%|███████▌  , loss=1 [03:34<01:02]\n",
      "2025-07-17 08:53:13,060 - INFO - Current learning rate: 0.0005306186018335297\n",
      "2025-07-17 08:53:13,233 - INFO - Epoch: 1/5, Iter: 91/119 -- train_loss: 1.0018\n",
      "Epoch [1/5]: [90/119]  76%|███████▌  , loss=1 [03:34<01:02]\n",
      "Epoch [1/5]: [90/119]  76%|███████▌  , loss=1 [03:34<01:02]\n",
      "Epoch [1/5]: [91/119]  76%|███████▋  , loss=1 [03:34<00:43]\n",
      "2025-07-17 08:53:13,236 - INFO - Current learning rate: 0.0005391099906908656\n",
      "2025-07-17 08:53:13,400 - INFO - Epoch: 1/5, Iter: 92/119 -- train_loss: 1.0026\n",
      "Epoch [1/5]: [91/119]  76%|███████▋  , loss=1 [03:34<00:43]\n",
      "Epoch [1/5]: [91/119]  76%|███████▋  , loss=1 [03:34<00:43]\n",
      "Epoch [1/5]: [92/119]  77%|███████▋  , loss=1 [03:34<00:30]\n",
      "2025-07-17 08:53:13,402 - INFO - Current learning rate: 0.0005475953933351584\n",
      "2025-07-17 08:53:20,764 - INFO - Epoch: 1/5, Iter: 93/119 -- train_loss: 0.9055\n",
      "Epoch [1/5]: [92/119]  77%|███████▋  , loss=1 [03:42<00:30]\n",
      "Epoch [1/5]: [92/119]  77%|███████▋  , loss=0.905 [03:42<00:30]\n",
      "Epoch [1/5]: [93/119]  78%|███████▊  , loss=0.905 [03:42<01:18]\n",
      "2025-07-17 08:53:20,767 - INFO - Current learning rate: 0.000556072151710274\n",
      "2025-07-17 08:53:20,936 - INFO - Epoch: 1/5, Iter: 94/119 -- train_loss: 0.7948\n",
      "Epoch [1/5]: [93/119]  78%|███████▊  , loss=0.905 [03:42<01:18]\n",
      "Epoch [1/5]: [93/119]  78%|███████▊  , loss=0.795 [03:42<01:18]\n",
      "Epoch [1/5]: [94/119]  79%|███████▉  , loss=0.795 [03:42<00:53]\n",
      "2025-07-17 08:53:20,939 - INFO - Current learning rate: 0.0005645376104678986\n",
      "2025-07-17 08:53:21,096 - INFO - Epoch: 1/5, Iter: 95/119 -- train_loss: 1.0022\n",
      "Epoch [1/5]: [94/119]  79%|███████▉  , loss=0.795 [03:42<00:53]\n",
      "Epoch [1/5]: [94/119]  79%|███████▉  , loss=1 [03:42<00:53]\n",
      "Epoch [1/5]: [95/119]  80%|███████▉  , loss=1 [03:42<00:37]\n",
      "2025-07-17 08:53:21,098 - INFO - Current learning rate: 0.0005729891177993294\n",
      "2025-07-17 08:53:21,228 - INFO - Epoch: 1/5, Iter: 96/119 -- train_loss: 1.0025\n",
      "Epoch [1/5]: [95/119]  80%|███████▉  , loss=1 [03:42<00:37]\n",
      "Epoch [1/5]: [95/119]  80%|███████▉  , loss=1 [03:42<00:37]\n",
      "Epoch [1/5]: [96/119]  81%|████████  , loss=1 [03:42<00:26]\n",
      "2025-07-17 08:53:21,230 - INFO - Current learning rate: 0.0005814240262661536\n",
      "2025-07-17 08:53:29,137 - INFO - Epoch: 1/5, Iter: 97/119 -- train_loss: 1.0013\n",
      "Epoch [1/5]: [96/119]  81%|████████  , loss=1 [03:50<00:26]\n",
      "Epoch [1/5]: [96/119]  81%|████████  , loss=1 [03:50<00:26]\n",
      "Epoch [1/5]: [97/119]  82%|████████▏ , loss=1 [03:50<01:09]\n",
      "2025-07-17 08:53:29,139 - INFO - Current learning rate: 0.0005898396936295607\n",
      "2025-07-17 08:53:32,363 - INFO - Epoch: 1/5, Iter: 98/119 -- train_loss: 0.9146\n",
      "Epoch [1/5]: [97/119]  82%|████████▏ , loss=1 [03:53<01:09]\n",
      "Epoch [1/5]: [97/119]  82%|████████▏ , loss=0.915 [03:53<01:09]\n",
      "Epoch [1/5]: [98/119]  82%|████████▏ , loss=0.915 [03:53<01:06]\n",
      "2025-07-17 08:53:32,364 - INFO - Current learning rate: 0.0005982334836780231\n",
      "2025-07-17 08:53:32,487 - INFO - Epoch: 1/5, Iter: 99/119 -- train_loss: 0.8938\n",
      "Epoch [1/5]: [98/119]  82%|████████▏ , loss=0.915 [03:53<01:06]\n",
      "Epoch [1/5]: [98/119]  82%|████████▏ , loss=0.894 [03:53<01:06]\n",
      "Epoch [1/5]: [99/119]  83%|████████▎ , loss=0.894 [03:53<00:45]\n",
      "2025-07-17 08:53:32,489 - INFO - Current learning rate: 0.000606602767053093\n",
      "2025-07-17 08:53:32,599 - INFO - Epoch: 1/5, Iter: 100/119 -- train_loss: 1.0043\n",
      "Epoch [1/5]: [99/119]  83%|████████▎ , loss=0.894 [03:54<00:45]\n",
      "Epoch [1/5]: [99/119]  83%|████████▎ , loss=1 [03:54<00:45]\n",
      "Epoch [1/5]: [100/119]  84%|████████▍ , loss=1 [03:54<00:30]\n",
      "2025-07-17 08:53:32,600 - INFO - Current learning rate: 0.0006149449220730483\n",
      "2025-07-17 08:53:38,509 - INFO - Epoch: 1/5, Iter: 101/119 -- train_loss: 1.0016\n",
      "Epoch [1/5]: [100/119]  84%|████████▍ , loss=1 [03:59<00:30]\n",
      "Epoch [1/5]: [100/119]  84%|████████▍ , loss=1 [03:59<00:30]\n",
      "Epoch [1/5]: [101/119]  85%|████████▍ , loss=1 [03:59<00:52]\n",
      "2025-07-17 08:53:38,511 - INFO - Current learning rate: 0.0006232573355541365\n",
      "2025-07-17 08:53:38,643 - INFO - Epoch: 1/5, Iter: 102/119 -- train_loss: 0.9702\n",
      "Epoch [1/5]: [101/119]  85%|████████▍ , loss=1 [04:00<00:52]\n",
      "Epoch [1/5]: [101/119]  85%|████████▍ , loss=0.97 [04:00<00:52]\n",
      "Epoch [1/5]: [102/119]  86%|████████▌ , loss=0.97 [04:00<00:35]\n",
      "2025-07-17 08:53:38,644 - INFO - Current learning rate: 0.000631537403629155\n",
      "2025-07-17 08:53:38,769 - INFO - Epoch: 1/5, Iter: 103/119 -- train_loss: 1.0037\n",
      "Epoch [1/5]: [102/119]  86%|████████▌ , loss=0.97 [04:00<00:35]\n",
      "Epoch [1/5]: [102/119]  86%|████████▌ , loss=1 [04:00<00:35]\n",
      "Epoch [1/5]: [103/119]  87%|████████▋ , loss=1 [04:00<00:23]\n",
      "2025-07-17 08:53:38,770 - INFO - Current learning rate: 0.000639782532563114\n",
      "2025-07-17 08:53:42,431 - INFO - Epoch: 1/5, Iter: 104/119 -- train_loss: 1.0019\n",
      "Epoch [1/5]: [103/119]  87%|████████▋ , loss=1 [04:03<00:23]\n",
      "Epoch [1/5]: [103/119]  87%|████████▋ , loss=1 [04:03<00:23]\n",
      "Epoch [1/5]: [104/119]  87%|████████▋ , loss=1 [04:03<00:32]\n",
      "2025-07-17 08:53:42,434 - INFO - Current learning rate: 0.0006479901395657253\n",
      "2025-07-17 08:53:46,417 - INFO - Epoch: 1/5, Iter: 105/119 -- train_loss: 0.9664\n",
      "Epoch [1/5]: [104/119]  87%|████████▋ , loss=1 [04:07<00:32]\n",
      "Epoch [1/5]: [104/119]  87%|████████▋ , loss=0.966 [04:07<00:32]\n",
      "Epoch [1/5]: [105/119]  88%|████████▊ , loss=0.966 [04:07<00:37]\n",
      "2025-07-17 08:53:46,419 - INFO - Current learning rate: 0.0006561576536004625\n",
      "2025-07-17 08:53:46,576 - INFO - Epoch: 1/5, Iter: 106/119 -- train_loss: 0.9238\n",
      "Epoch [1/5]: [105/119]  88%|████████▊ , loss=0.966 [04:08<00:37]\n",
      "Epoch [1/5]: [105/119]  88%|████████▊ , loss=0.924 [04:08<00:37]\n",
      "Epoch [1/5]: [106/119]  89%|████████▉ , loss=0.924 [04:08<00:25]\n",
      "2025-07-17 08:53:46,578 - INFO - Current learning rate: 0.0006642825161899382\n",
      "2025-07-17 08:53:46,729 - INFO - Epoch: 1/5, Iter: 107/119 -- train_loss: 0.9228\n",
      "Epoch [1/5]: [106/119]  89%|████████▉ , loss=0.924 [04:08<00:25]\n",
      "Epoch [1/5]: [106/119]  89%|████████▉ , loss=0.923 [04:08<00:25]\n",
      "Epoch [1/5]: [107/119]  90%|████████▉ , loss=0.923 [04:08<00:16]\n",
      "2025-07-17 08:53:46,731 - INFO - Current learning rate: 0.0006723621822173502\n",
      "2025-07-17 08:53:50,667 - INFO - Epoch: 1/5, Iter: 108/119 -- train_loss: 1.0031\n",
      "Epoch [1/5]: [107/119]  90%|████████▉ , loss=0.923 [04:12<00:16]\n",
      "Epoch [1/5]: [107/119]  90%|████████▉ , loss=1 [04:12<00:16]\n",
      "Epoch [1/5]: [108/119]  91%|█████████ , loss=1 [04:12<00:23]\n",
      "2025-07-17 08:53:50,670 - INFO - Current learning rate: 0.0006803941207237387\n",
      "2025-07-17 08:53:53,335 - INFO - Epoch: 1/5, Iter: 109/119 -- train_loss: 1.0011\n",
      "Epoch [1/5]: [108/119]  91%|█████████ , loss=1 [04:14<00:23]\n",
      "Epoch [1/5]: [108/119]  91%|█████████ , loss=1 [04:14<00:23]\n",
      "Epoch [1/5]: [109/119]  92%|█████████▏, loss=1 [04:14<00:23]\n",
      "2025-07-17 08:53:53,337 - INFO - Current learning rate: 0.0006883758157008112\n",
      "2025-07-17 08:53:56,855 - INFO - Epoch: 1/5, Iter: 110/119 -- train_loss: 0.8054\n",
      "Epoch [1/5]: [109/119]  92%|█████████▏, loss=1 [04:18<00:23]\n",
      "Epoch [1/5]: [109/119]  92%|█████████▏, loss=0.805 [04:18<00:23]#015Epoch [1/5]: [110/119]  92%|█████████▏, loss=0.805 [04:18<00:24]\n",
      "2025-07-17 08:53:56,856 - INFO - Current learning rate: 0.0006963047668790832\n",
      "2025-07-17 08:53:56,982 - INFO - Epoch: 1/5, Iter: 111/119 -- train_loss: 0.9319\n",
      "Epoch [1/5]: [110/119]  92%|█████████▏, loss=0.805 [04:18<00:24]\n",
      "Epoch [1/5]: [110/119]  92%|█████████▏, loss=0.932 [04:18<00:24]\n",
      "Epoch [1/5]: [111/119]  93%|█████████▎, loss=0.932 [04:18<00:15]\n",
      "2025-07-17 08:53:56,984 - INFO - Current learning rate: 0.0007041784905110883\n",
      "2025-07-17 08:53:59,719 - INFO - Epoch: 1/5, Iter: 112/119 -- train_loss: 1.0004\n",
      "Epoch [1/5]: [111/119]  93%|█████████▎, loss=0.932 [04:21<00:15]\n",
      "Epoch [1/5]: [111/119]  93%|█████████▎, loss=1 [04:21<00:15]\n",
      "Epoch [1/5]: [112/119]  94%|█████████▍, loss=1 [04:21<00:15]\n",
      "2025-07-17 08:53:59,722 - INFO - Current learning rate: 0.0007119945201494132\n",
      "2025-07-17 08:54:06,933 - INFO - Epoch: 1/5, Iter: 113/119 -- train_loss: 1.0193\n",
      "Epoch [1/5]: [112/119]  94%|█████████▍, loss=1 [04:28<00:15]\n",
      "Epoch [1/5]: [112/119]  94%|█████████▍, loss=1.02 [04:28<00:15]\n",
      "Epoch [1/5]: [113/119]  95%|█████████▍, loss=1.02 [04:28<00:22]\n",
      "2025-07-17 08:54:06,936 - INFO - Current learning rate: 0.0007197504074193139\n",
      "2025-07-17 08:54:07,113 - INFO - Epoch: 1/5, Iter: 114/119 -- train_loss: 0.9423\n",
      "Epoch [1/5]: [113/119]  95%|█████████▍, loss=1.02 [04:28<00:22]\n",
      "Epoch [1/5]: [113/119]  95%|█████████▍, loss=0.942 [04:28<00:22]\n",
      "Epoch [1/5]: [114/119]  96%|█████████▌, loss=0.942 [04:28<00:13]\n",
      "2025-07-17 08:54:07,115 - INFO - Current learning rate: 0.00072744372278567\n",
      "2025-07-17 08:54:07,273 - INFO - Epoch: 1/5, Iter: 115/119 -- train_loss: 1.0016\n",
      "Epoch [1/5]: [114/119]  96%|█████████▌, loss=0.942 [04:28<00:13]\n",
      "Epoch [1/5]: [114/119]  96%|█████████▌, loss=1 [04:28<00:13]\n",
      "Epoch [1/5]: [115/119]  97%|█████████▋, loss=1 [04:28<00:07]\n",
      "2025-07-17 08:54:07,275 - INFO - Current learning rate: 0.0007350720563140377\n",
      "2025-07-17 08:54:10,055 - INFO - Epoch: 1/5, Iter: 116/119 -- train_loss: 1.0049\n",
      "Epoch [1/5]: [115/119]  97%|█████████▋, loss=1 [04:31<00:07]\n",
      "Epoch [1/5]: [115/119]  97%|█████████▋, loss=1 [04:31<00:07]\n",
      "Epoch [1/5]: [116/119]  97%|█████████▋, loss=1 [04:31<00:06]\n",
      "2025-07-17 08:54:10,056 - INFO - Current learning rate: 0.0007426330184255622\n",
      "2025-07-17 08:54:12,588 - INFO - Epoch: 1/5, Iter: 117/119 -- train_loss: 0.9503\n",
      "Epoch [1/5]: [116/119]  97%|█████████▋, loss=1 [04:34<00:06]\n",
      "Epoch [1/5]: [116/119]  97%|█████████▋, loss=0.95 [04:34<00:06]\n",
      "Epoch [1/5]: [117/119]  98%|█████████▊, loss=0.95 [04:34<00:04]\n",
      "2025-07-17 08:54:12,589 - INFO - Current learning rate: 0.0007501242406455155\n",
      "2025-07-17 08:54:12,689 - INFO - Epoch: 1/5, Iter: 118/119 -- train_loss: 0.7597\n",
      "Epoch [1/5]: [117/119]  98%|█████████▊, loss=0.95 [04:34<00:04]\n",
      "Epoch [1/5]: [117/119]  98%|█████████▊, loss=0.76 [04:34<00:04]\n",
      "Epoch [1/5]: [118/119]  99%|█████████▉, loss=0.76 [04:34<00:01]\n",
      "2025-07-17 08:54:12,690 - INFO - Current learning rate: 0.0007575433763452208\n",
      "2025-07-17 08:54:12,790 - INFO - Epoch: 1/5, Iter: 119/119 -- train_loss: 1.0009\n",
      "Epoch [1/5]: [118/119]  99%|█████████▉, loss=0.76 [04:34<00:01]\n",
      "Epoch [1/5]: [118/119]  99%|█████████▉, loss=1 [04:34<00:01]\n",
      "Epoch [1/5]: [119/119] 100%|██████████, loss=1 [04:34<00:00]\n",
      "2025-07-17 08:54:12,791 - INFO - Current learning rate: 0.0007648881014771364\n",
      "2025-07-17 08:54:12,791 - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Iteration: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "/usr/local/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/usr/local/lib/python3.12/site-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 0 is all 0, this may result in nan/inf distance.\n",
      "  warnings.warn(\n",
      "Iteration: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Iteration: [2/20]  10%|#033[32m█         #033[0m [00:00<00:11]#033[A\n",
      "Iteration: [2/20]  10%|#033[32m█         #033[0m [00:01<00:11]\n",
      "#033[A\n",
      "Iteration: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:10]\n",
      "#033[A\n",
      "/usr/local/lib/python3.12/site-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 0 is all 0, this may result in nan/inf distance.\n",
      "  warnings.warn(\n",
      "Iteration: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:10]#033[A\n",
      "Iteration: [4/20]  20%|#033[32m██        #033[0m [00:01<00:08]#033[A\n",
      "Iteration: [4/20]  20%|#033[32m██        #033[0m [00:02<00:08]#033[A\n",
      "#015Iteration: [5/20]  25%|#033[32m██▌       #033[0m [00:02<00:12]#033[A\n",
      "Iteration: [5/20]  25%|#033[32m██▌       #033[0m [00:03<00:12]#033[A\n",
      "#015Iteration: [6/20]  30%|#033[32m███       #033[0m [00:03<00:10]#033[A\n",
      "Iteration: [6/20]  30%|#033[32m███       #033[0m [00:04<00:10]\n",
      "#033[A\n",
      "Iteration: [7/20]  35%|#033[32m███▌      #033[0m [00:04<00:10]\n",
      "#033[A\n",
      "Iteration: [7/20]  35%|#033[32m███▌      #033[0m [00:04<00:10]\n",
      "#033[A\n",
      "Iteration: [8/20]  40%|#033[32m████      #033[0m [00:04<00:08]\n",
      "#033[A\n",
      "Iteration: [8/20]  40%|#033[32m████      #033[0m [00:06<00:08]#033[A\n",
      "Iteration: [9/20]  45%|#033[32m████▌     #033[0m [00:06<00:10]#033[A\n",
      "Iteration: [9/20]  45%|#033[32m████▌     #033[0m [00:08<00:10]#033[A\n",
      "#015Iteration: [10/20]  50%|#033[32m█████     #033[0m [00:08<00:14]#033[A\n",
      "Iteration: [10/20]  50%|#033[32m█████     #033[0m [00:10<00:14]#033[A\n",
      "#015Iteration: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:13]#033[A\n",
      "Iteration: [11/20]  55%|#033[32m█████▌    #033[0m [00:11<00:13]#033[A\n",
      "Iteration: [12/20]  60%|#033[32m██████    #033[0m [00:11<00:09]#033[A\n",
      "Iteration: [12/20]  60%|#033[32m██████    #033[0m [00:12<00:09]#033[A\n",
      "#015Iteration: [13/20]  65%|#033[32m██████▌   #033[0m [00:12<00:08]#033[A\n",
      "Iteration: [13/20]  65%|#033[32m██████▌   #033[0m [00:12<00:08]\n",
      "#033[A\n",
      "Iteration: [14/20]  70%|#033[32m███████   #033[0m [00:12<00:06]\n",
      "#033[A\n",
      "Iteration: [14/20]  70%|#033[32m███████   #033[0m [00:14<00:06]#033[A\n",
      "Iteration: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:05]#033[A\n",
      "Iteration: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:05]#033[A\n",
      "Iteration: [16/20]  80%|#033[32m████████  #033[0m [00:14<00:03]#033[A\n",
      "Iteration: [16/20]  80%|#033[32m████████  #033[0m [00:15<00:03]#033[A\n",
      "Iteration: [17/20]  85%|#033[32m████████▌ #033[0m [00:15<00:02]#033[A\n",
      "Iteration: [17/20]  85%|#033[32m████████▌ #033[0m [00:15<00:02]#033[A\n",
      "Iteration: [18/20]  90%|#033[32m█████████ #033[0m [00:15<00:01]#033[A\n",
      "Iteration: [18/20]  90%|#033[32m█████████ #033[0m [00:16<00:01]#033[A\n",
      "Iteration: [19/20]  95%|#033[32m█████████▌#033[0m [00:16<00:00]#033[A\n",
      "Iteration: [19/20]  95%|#033[32m█████████▌#033[0m [00:17<00:00]#033[A\n",
      "Iteration: [20/20] 100%|#033[32m██████████#033[0m [00:17<00:00]#033[A\n",
      "2025-07-17 08:54:35,003 - INFO - Got new best metric of val_mean_dice: 0.0\n",
      "#033[A\n",
      "/usr/local/lib/python3.12/site-packages/monai/handlers/utils.py:130: RuntimeWarning: Mean of empty slice\n",
      "  v = np.concatenate([v, np.nanmean(v, axis=1, keepdims=True)], axis=1)\n",
      "2025-07-17 08:54:37,546 - INFO - Epoch[1] Complete. Time taken: 00:00:24.687\n",
      "2025-07-17 08:54:37,546 - INFO - Engine run finished. Time taken: 00:00:24.755\n",
      "2025-07-17 08:54:37,593 - INFO - Epoch[1] Complete. Time taken: 00:05:14.374\n",
      "2025-07-17 08:54:48,813 - INFO - Epoch: 2/5, Iter: 1/119 -- train_loss: 1.0011\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [2/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [2/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "2025-07-17 08:54:48,817 - INFO - Current learning rate: 0.0007721561153028634\n",
      "2025-07-17 08:54:48,966 - INFO - Epoch: 2/5, Iter: 2/119 -- train_loss: 1.0020\n",
      "Epoch [2/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [2/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [2/5]: [2/119]   2%|▏         , loss=1 [00:00<00:18]\n",
      "2025-07-17 08:54:48,970 - INFO - Current learning rate: 0.0007793451411138535\n",
      "2025-07-17 08:54:49,098 - INFO - Epoch: 2/5, Iter: 3/119 -- train_loss: 1.0009\n",
      "Epoch [2/5]: [2/119]   2%|▏         , loss=1 [00:00<00:18]\n",
      "Epoch [2/5]: [2/119]   2%|▏         , loss=1 [00:00<00:18]\n",
      "Epoch [2/5]: [3/119]   3%|▎         , loss=1 [00:00<00:16]\n",
      "2025-07-17 08:54:49,100 - INFO - Current learning rate: 0.0007864529269445896\n",
      "2025-07-17 08:54:49,508 - INFO - Epoch: 2/5, Iter: 4/119 -- train_loss: 1.0018\n",
      "Epoch [2/5]: [3/119]   3%|▎         , loss=1 [00:00<00:16]\n",
      "Epoch [2/5]: [3/119]   3%|▎         , loss=1 [00:00<00:16]\n",
      "Epoch [2/5]: [4/119]   3%|▎         , loss=1 [00:00<00:30]\n",
      "2025-07-17 08:54:49,510 - INFO - Current learning rate: 0.0007934772462780135\n",
      "2025-07-17 08:54:58,368 - INFO - Epoch: 2/5, Iter: 5/119 -- train_loss: 0.9834\n",
      "Epoch [2/5]: [4/119]   3%|▎         , loss=1 [00:09<00:30]\n",
      "Epoch [2/5]: [4/119]   3%|▎         , loss=0.983 [00:09<00:30]\n",
      "Epoch [2/5]: [5/119]   4%|▍         , loss=0.983 [00:09<06:56]\n",
      "2025-07-17 08:54:58,372 - INFO - Current learning rate: 0.0008004158987429848\n",
      "2025-07-17 08:54:58,905 - INFO - Epoch: 2/5, Iter: 6/119 -- train_loss: 0.8439\n",
      "Epoch [2/5]: [5/119]   4%|▍         , loss=0.983 [00:10<06:56]\n",
      "Epoch [2/5]: [5/119]   4%|▍         , loss=0.844 [00:10<06:56]\n",
      "Epoch [2/5]: [6/119]   5%|▌         , loss=0.844 [00:10<04:46]\n",
      "2025-07-17 08:54:58,907 - INFO - Current learning rate: 0.0008072667108035476\n",
      "2025-07-17 08:54:59,035 - INFO - Epoch: 2/5, Iter: 7/119 -- train_loss: 0.8313\n",
      "Epoch [2/5]: [6/119]   5%|▌         , loss=0.844 [00:10<04:46]\n",
      "Epoch [2/5]: [6/119]   5%|▌         , loss=0.831 [00:10<04:46]\n",
      "Epoch [2/5]: [7/119]   6%|▌         , loss=0.831 [00:10<03:12]\n",
      "2025-07-17 08:54:59,037 - INFO - Current learning rate: 0.0008140275364397912\n",
      "2025-07-17 08:55:07,139 - INFO - Epoch: 2/5, Iter: 8/119 -- train_loss: 1.0016\n",
      "Epoch [2/5]: [7/119]   6%|▌         , loss=0.831 [00:18<03:12]\n",
      "Epoch [2/5]: [7/119]   6%|▌         , loss=1 [00:18<03:12]\n",
      "Epoch [2/5]: [8/119]   7%|▋         , loss=1 [00:18<07:02]\n",
      "2025-07-17 08:55:07,140 - INFO - Current learning rate: 0.0008206962578200921\n",
      "2025-07-17 08:55:11,494 - INFO - Epoch: 2/5, Iter: 9/119 -- train_loss: 1.0013\n",
      "Epoch [2/5]: [8/119]   7%|▋         , loss=1 [00:22<07:02]\n",
      "Epoch [2/5]: [8/119]   7%|▋         , loss=1 [00:22<07:02]\n",
      "Epoch [2/5]: [9/119]   8%|▊         , loss=1 [00:22<07:17]\n",
      "2025-07-17 08:55:11,496 - INFO - Current learning rate: 0.0008272707859645263\n",
      "2025-07-17 08:55:11,622 - INFO - Epoch: 2/5, Iter: 10/119 -- train_loss: 1.0024\n",
      "Epoch [2/5]: [9/119]   8%|▊         , loss=1 [00:22<07:17]\n",
      "Epoch [2/5]: [9/119]   8%|▊         , loss=1 [00:22<07:17]\n",
      "Epoch [2/5]: [10/119]   8%|▊         , loss=1 [00:22<05:02]\n",
      "2025-07-17 08:55:11,623 - INFO - Current learning rate: 0.0008337490613992423\n",
      "2025-07-17 08:55:11,753 - INFO - Epoch: 2/5, Iter: 11/119 -- train_loss: 0.7625\n",
      "Epoch [2/5]: [10/119]   8%|▊         , loss=1 [00:22<05:02]\n",
      "Epoch [2/5]: [10/119]   8%|▊         , loss=0.762 [00:22<05:02]\n",
      "Epoch [2/5]: [11/119]   9%|▉         , loss=0.762 [00:22<03:31]\n",
      "2025-07-17 08:55:11,754 - INFO - Current learning rate: 0.0008401290548015924\n",
      "2025-07-17 08:55:14,728 - INFO - Epoch: 2/5, Iter: 12/119 -- train_loss: 0.8910\n",
      "Epoch [2/5]: [11/119]   9%|▉         , loss=0.762 [00:25<03:31]\n",
      "Epoch [2/5]: [11/119]   9%|▉         , loss=0.891 [00:25<03:31]\n",
      "Epoch [2/5]: [12/119]  10%|█         , loss=0.891 [00:25<04:02]\n",
      "2025-07-17 08:55:14,731 - INFO - Current learning rate: 0.0008464087676358193\n",
      "2025-07-17 08:55:16,765 - INFO - Epoch: 2/5, Iter: 13/119 -- train_loss: 0.5426\n",
      "Epoch [2/5]: [12/119]  10%|█         , loss=0.891 [00:27<04:02]\n",
      "Epoch [2/5]: [12/119]  10%|█         , loss=0.543 [00:27<04:02]\n",
      "Epoch [2/5]: [13/119]  11%|█         , loss=0.543 [00:27<03:53]\n",
      "2025-07-17 08:55:16,767 - INFO - Current learning rate: 0.0008525862327790979\n",
      "2025-07-17 08:55:22,548 - INFO - Epoch: 2/5, Iter: 14/119 -- train_loss: 0.9397\n",
      "Epoch [2/5]: [13/119]  11%|█         , loss=0.543 [00:33<03:53]\n",
      "Epoch [2/5]: [13/119]  11%|█         , loss=0.94 [00:33<03:53]\n",
      "Epoch [2/5]: [14/119]  12%|█▏        , loss=0.94 [00:33<05:44]\n",
      "2025-07-17 08:55:22,551 - INFO - Current learning rate: 0.0008586595151377375\n",
      "2025-07-17 08:55:22,678 - INFO - Epoch: 2/5, Iter: 15/119 -- train_loss: 1.0049\n",
      "Epoch [2/5]: [14/119]  12%|█▏        , loss=0.94 [00:33<05:44]\n",
      "Epoch [2/5]: [14/119]  12%|█▏        , loss=1 [00:33<05:44]\n",
      "Epoch [2/5]: [15/119]  13%|█▎        , loss=1 [00:33<04:02]\n",
      "2025-07-17 08:55:22,680 - INFO - Current learning rate: 0.000864626712253351\n",
      "2025-07-17 08:55:26,615 - INFO - Epoch: 2/5, Iter: 16/119 -- train_loss: 1.0077\n",
      "Epoch [2/5]: [15/119]  13%|█▎        , loss=1 [00:37<04:02]\n",
      "Epoch [2/5]: [15/119]  13%|█▎        , loss=1.01 [00:37<04:02]\n",
      "Epoch [2/5]: [16/119]  13%|█▎        , loss=1.01 [00:37<04:50]\n",
      "2025-07-17 08:55:26,617 - INFO - Current learning rate: 0.0008704859548988004\n",
      "2025-07-17 08:55:26,741 - INFO - Epoch: 2/5, Iter: 17/119 -- train_loss: 0.9317\n",
      "Epoch [2/5]: [16/119]  13%|█▎        , loss=1.01 [00:37<04:50]\n",
      "Epoch [2/5]: [16/119]  13%|█▎        , loss=0.932 [00:37<04:50]\n",
      "Epoch [2/5]: [17/119]  14%|█▍        , loss=0.932 [00:37<03:24]\n",
      "2025-07-17 08:55:26,743 - INFO - Current learning rate: 0.0008762354076637336\n",
      "2025-07-17 08:55:28,778 - INFO - Epoch: 2/5, Iter: 18/119 -- train_loss: 0.7703\n",
      "Epoch [2/5]: [17/119]  14%|█▍        , loss=0.932 [00:39<03:24]\n",
      "Epoch [2/5]: [17/119]  14%|█▍        , loss=0.77 [00:39<03:24]\n",
      "Epoch [2/5]: [18/119]  15%|█▌        , loss=0.77 [00:39<03:23]\n",
      "2025-07-17 08:55:28,780 - INFO - Current learning rate: 0.0008818732695295267\n",
      "2025-07-17 08:55:28,914 - INFO - Epoch: 2/5, Iter: 19/119 -- train_loss: 1.0021\n",
      "Epoch [2/5]: [18/119]  15%|█▌        , loss=0.77 [00:40<03:23]\n",
      "Epoch [2/5]: [18/119]  15%|█▌        , loss=1 [00:40<03:23]\n",
      "Epoch [2/5]: [19/119]  16%|█▌        , loss=1 [00:40<02:25]\n",
      "2025-07-17 08:55:28,915 - INFO - Current learning rate: 0.0008873977744334548\n",
      "2025-07-17 08:55:33,574 - INFO - Epoch: 2/5, Iter: 20/119 -- train_loss: 1.0019\n",
      "Epoch [2/5]: [19/119]  16%|█▌        , loss=1 [00:44<02:25]\n",
      "Epoch [2/5]: [19/119]  16%|█▌        , loss=1 [00:44<02:25]\n",
      "Epoch [2/5]: [20/119]  17%|█▋        , loss=1 [00:44<03:59]\n",
      "2025-07-17 08:55:33,576 - INFO - Current learning rate: 0.0008928071918219112\n",
      "2025-07-17 08:55:33,717 - INFO - Epoch: 2/5, Iter: 21/119 -- train_loss: 1.0016\n",
      "Epoch [2/5]: [20/119]  17%|█▋        , loss=1 [00:44<03:59]\n",
      "Epoch [2/5]: [20/119]  17%|█▋        , loss=1 [00:44<03:59]\n",
      "Epoch [2/5]: [21/119]  18%|█▊        , loss=1 [00:44<02:49]\n",
      "2025-07-17 08:55:33,719 - INFO - Current learning rate: 0.0008980998271925047\n",
      "2025-07-17 08:55:42,354 - INFO - Epoch: 2/5, Iter: 22/119 -- train_loss: 0.8551\n",
      "Epoch [2/5]: [21/119]  18%|█▊        , loss=1 [00:53<02:49]\n",
      "Epoch [2/5]: [21/119]  18%|█▊        , loss=0.855 [00:53<02:49]\n",
      "Epoch [2/5]: [22/119]  18%|█▊        , loss=0.855 [00:53<06:09]\n",
      "2025-07-17 08:55:42,355 - INFO - Current learning rate: 0.0009032740226248624\n",
      "2025-07-17 08:55:42,485 - INFO - Epoch: 2/5, Iter: 23/119 -- train_loss: 1.0021\n",
      "Epoch [2/5]: [22/119]  18%|█▊        , loss=0.855 [00:53<06:09]\n",
      "Epoch [2/5]: [22/119]  18%|█▊        , loss=1 [00:53<06:09]\n",
      "Epoch [2/5]: [23/119]  19%|█▉        , loss=1 [00:53<04:19]\n",
      "2025-07-17 08:55:42,486 - INFO - Current learning rate: 0.0009083281572999747\n",
      "2025-07-17 08:55:42,630 - INFO - Epoch: 2/5, Iter: 24/119 -- train_loss: 1.0019\n",
      "Epoch [2/5]: [23/119]  19%|█▉        , loss=1 [00:53<04:19]\n",
      "Epoch [2/5]: [23/119]  19%|█▉        , loss=1 [00:53<04:19]#015Epoch [2/5]: [24/119]  20%|██        , loss=1 [00:53<03:03]\n",
      "2025-07-17 08:55:42,632 - INFO - Current learning rate: 0.0009132606480079176\n",
      "2025-07-17 08:55:42,804 - INFO - Epoch: 2/5, Iter: 25/119 -- train_loss: 0.9219\n",
      "Epoch [2/5]: [24/119]  20%|██        , loss=1 [00:53<03:03]\n",
      "Epoch [2/5]: [24/119]  20%|██        , loss=0.922 [00:53<03:03]\n",
      "Epoch [2/5]: [25/119]  21%|██        , loss=0.922 [00:53<02:12]\n",
      "2025-07-17 08:55:42,807 - INFO - Current learning rate: 0.0009180699496437932\n",
      "2025-07-17 08:55:55,847 - INFO - Epoch: 2/5, Iter: 26/119 -- train_loss: 1.0034\n",
      "Epoch [2/5]: [25/119]  21%|██        , loss=0.922 [01:07<02:12]\n",
      "Epoch [2/5]: [25/119]  21%|██        , loss=1 [01:07<02:12]\n",
      "Epoch [2/5]: [26/119]  22%|██▏       , loss=1 [01:07<07:35]2025-07-17 08:55:55,848 - INFO - Current learning rate: 0.0009227545556917353\n",
      "2025-07-17 08:55:56,003 - INFO - Epoch: 2/5, Iter: 27/119 -- train_loss: 0.9508\n",
      "Epoch [2/5]: [26/119]  22%|██▏       , loss=1 [01:07<07:35]\n",
      "Epoch [2/5]: [26/119]  22%|██▏       , loss=0.951 [01:07<07:35]\n",
      "Epoch [2/5]: [27/119]  23%|██▎       , loss=0.951 [01:07<05:19]\n",
      "2025-07-17 08:55:56,004 - INFO - Current learning rate: 0.0009273129986968259\n",
      "2025-07-17 08:55:56,156 - INFO - Epoch: 2/5, Iter: 28/119 -- train_loss: 0.8067\n",
      "Epoch [2/5]: [27/119]  23%|██▎       , loss=0.951 [01:07<05:19]\n",
      "Epoch [2/5]: [27/119]  23%|██▎       , loss=0.807 [01:07<05:19]\n",
      "Epoch [2/5]: [28/119]  24%|██▎       , loss=0.807 [01:07<03:45]\n",
      "2025-07-17 08:55:56,159 - INFO - Current learning rate: 0.0009317438507247756\n",
      "2025-07-17 08:55:56,300 - INFO - Epoch: 2/5, Iter: 29/119 -- train_loss: 1.0015\n",
      "Epoch [2/5]: [28/119]  24%|██▎       , loss=0.807 [01:07<03:45]\n",
      "Epoch [2/5]: [28/119]  24%|██▎       , loss=1 [01:07<03:45]\n",
      "Epoch [2/5]: [29/119]  24%|██▍       , loss=1 [01:07<02:40]\n",
      "2025-07-17 08:55:56,302 - INFO - Current learning rate: 0.0009360457238092254\n",
      "2025-07-17 08:56:01,922 - INFO - Epoch: 2/5, Iter: 30/119 -- train_loss: 1.0013\n",
      "Epoch [2/5]: [29/119]  24%|██▍       , loss=1 [01:13<02:40]\n",
      "Epoch [2/5]: [29/119]  24%|██▍       , loss=1 [01:13<02:40]\n",
      "Epoch [2/5]: [30/119]  25%|██▌       , loss=1 [01:13<04:20]2025-07-17 08:56:01,923 - INFO - Current learning rate: 0.0009402172703865272\n",
      "2025-07-17 08:56:02,078 - INFO - Epoch: 2/5, Iter: 31/119 -- train_loss: 1.0024\n",
      "Epoch [2/5]: [30/119]  25%|██▌       , loss=1 [01:13<04:20]\n",
      "Epoch [2/5]: [30/119]  25%|██▌       , loss=1 [01:13<04:20]\n",
      "Epoch [2/5]: [31/119]  26%|██▌       , loss=1 [01:13<03:04]\n",
      "2025-07-17 08:56:02,081 - INFO - Current learning rate: 0.0009442571837178694\n",
      "2025-07-17 08:56:02,234 - INFO - Epoch: 2/5, Iter: 32/119 -- train_loss: 1.0018\n",
      "Epoch [2/5]: [31/119]  26%|██▌       , loss=1 [01:13<03:04]\n",
      "Epoch [2/5]: [31/119]  26%|██▌       , loss=1 [01:13<03:04]\n",
      "Epoch [2/5]: [32/119]  27%|██▋       , loss=1 [01:13<02:11]\n",
      "2025-07-17 08:56:02,236 - INFO - Current learning rate: 0.0009481641982986124\n",
      "2025-07-17 08:56:02,361 - INFO - Epoch: 2/5, Iter: 33/119 -- train_loss: 1.0105\n",
      "Epoch [2/5]: [32/119]  27%|██▋       , loss=1 [01:13<02:11]\n",
      "Epoch [2/5]: [32/119]  27%|██▋       , loss=1.01 [01:13<02:11]\n",
      "Epoch [2/5]: [33/119]  28%|██▊       , loss=1.01 [01:13<01:34]\n",
      "2025-07-17 08:56:02,363 - INFO - Current learning rate: 0.0009519370902547094\n",
      "2025-07-17 08:56:12,457 - INFO - Epoch: 2/5, Iter: 34/119 -- train_loss: 1.0033\n",
      "Epoch [2/5]: [33/119]  28%|██▊       , loss=1.01 [01:23<01:34]\n",
      "Epoch [2/5]: [33/119]  28%|██▊       , loss=1 [01:23<01:34]\n",
      "Epoch [2/5]: [34/119]  29%|██▊       , loss=1 [01:23<05:22]\n",
      "2025-07-17 08:56:12,458 - INFO - Current learning rate: 0.0009555746777260853\n",
      "2025-07-17 08:56:12,567 - INFO - Epoch: 2/5, Iter: 35/119 -- train_loss: 0.9259\n",
      "Epoch [2/5]: [34/119]  29%|██▊       , loss=1 [01:23<05:22]\n",
      "Epoch [2/5]: [34/119]  29%|██▊       , loss=0.926 [01:23<05:22]\n",
      "Epoch [2/5]: [35/119]  29%|██▉       , loss=0.926 [01:23<03:46]\n",
      "2025-07-17 08:56:12,568 - INFO - Current learning rate: 0.0009590758212368541\n",
      "2025-07-17 08:56:12,673 - INFO - Epoch: 2/5, Iter: 36/119 -- train_loss: 1.0017\n",
      "Epoch [2/5]: [35/119]  29%|██▉       , loss=0.926 [01:23<03:46]\n",
      "Epoch [2/5]: [35/119]  29%|██▉       , loss=1 [01:23<03:46]\n",
      "Epoch [2/5]: [36/119]  30%|███       , loss=1 [01:23<02:39]\n",
      "2025-07-17 08:56:12,674 - INFO - Current learning rate: 0.0009624394240522616\n",
      "2025-07-17 08:56:12,809 - INFO - Epoch: 2/5, Iter: 37/119 -- train_loss: 0.6891\n",
      "Epoch [2/5]: [36/119]  30%|███       , loss=1 [01:23<02:39]\n",
      "Epoch [2/5]: [36/119]  30%|███       , loss=0.689 [01:23<02:39]\n",
      "Epoch [2/5]: [37/119]  31%|███       , loss=0.689 [01:23<01:53]\n",
      "2025-07-17 08:56:12,811 - INFO - Current learning rate: 0.0009656644325222369\n",
      "2025-07-17 08:56:18,314 - INFO - Epoch: 2/5, Iter: 38/119 -- train_loss: 1.0015\n",
      "Epoch [2/5]: [37/119]  31%|███       , loss=0.689 [01:29<01:53]\n",
      "Epoch [2/5]: [37/119]  31%|███       , loss=1 [01:29<01:53]\n",
      "Epoch [2/5]: [38/119]  32%|███▏      , loss=1 [01:29<03:32]\n",
      "2025-07-17 08:56:18,315 - INFO - Current learning rate: 0.0009687498364114489\n",
      "2025-07-17 08:56:18,420 - INFO - Epoch: 2/5, Iter: 39/119 -- train_loss: 0.9834\n",
      "Epoch [2/5]: [38/119]  32%|███▏      , loss=1 [01:29<03:32]\n",
      "Epoch [2/5]: [38/119]  32%|███▏      , loss=0.983 [01:29<03:32]\n",
      "Epoch [2/5]: [39/119]  33%|███▎      , loss=0.983 [01:29<02:29]\n",
      "2025-07-17 08:56:18,421 - INFO - Current learning rate: 0.0009716946692157638\n",
      "2025-07-17 08:56:23,939 - INFO - Epoch: 2/5, Iter: 40/119 -- train_loss: 0.9572\n",
      "Epoch [2/5]: [39/119]  33%|███▎      , loss=0.983 [01:35<02:29]\n",
      "Epoch [2/5]: [39/119]  33%|███▎      , loss=0.957 [01:35<02:29]#015Epoch [2/5]: [40/119]  34%|███▎      , loss=0.957 [01:35<03:53]2025-07-17 08:56:23,942 - INFO - Current learning rate: 0.0009744980084650021\n",
      "2025-07-17 08:56:24,063 - INFO - Epoch: 2/5, Iter: 41/119 -- train_loss: 1.0028\n",
      "Epoch [2/5]: [40/119]  34%|███▎      , loss=0.957 [01:35<03:53]\n",
      "Epoch [2/5]: [40/119]  34%|███▎      , loss=1 [01:35<03:53]\n",
      "Epoch [2/5]: [41/119]  34%|███▍      , loss=1 [01:35<02:44]\n",
      "2025-07-17 08:56:24,064 - INFO - Current learning rate: 0.0009771589760119031\n",
      "2025-07-17 08:56:34,175 - INFO - Epoch: 2/5, Iter: 42/119 -- train_loss: 1.0020\n",
      "Epoch [2/5]: [41/119]  34%|███▍      , loss=1 [01:45<02:44]\n",
      "Epoch [2/5]: [41/119]  34%|███▍      , loss=1 [01:45<02:44]\n",
      "Epoch [2/5]: [42/119]  35%|███▌      , loss=1 [01:45<05:47]\n",
      "2025-07-17 08:56:34,177 - INFO - Current learning rate: 0.0009796767383072047\n",
      "2025-07-17 08:56:34,311 - INFO - Epoch: 2/5, Iter: 43/119 -- train_loss: 1.0056\n",
      "Epoch [2/5]: [42/119]  35%|███▌      , loss=1 [01:45<05:47]\n",
      "Epoch [2/5]: [42/119]  35%|███▌      , loss=1.01 [01:45<05:47]\n",
      "Epoch [2/5]: [43/119]  36%|███▌      , loss=1.01 [01:45<04:03]\n",
      "2025-07-17 08:56:34,313 - INFO - Current learning rate: 0.0009820505066607534\n",
      "2025-07-17 08:56:34,432 - INFO - Epoch: 2/5, Iter: 44/119 -- train_loss: 0.9488\n",
      "Epoch [2/5]: [43/119]  36%|███▌      , loss=1.01 [01:45<04:03]\n",
      "Epoch [2/5]: [43/119]  36%|███▌      , loss=0.949 [01:45<04:03]#015Epoch [2/5]: [44/119]  37%|███▋      , loss=0.949 [01:45<02:50]\n",
      "2025-07-17 08:56:34,434 - INFO - Current learning rate: 0.000984279537488562\n",
      "2025-07-17 08:56:34,598 - INFO - Epoch: 2/5, Iter: 45/119 -- train_loss: 1.0041\n",
      "Epoch [2/5]: [44/119]  37%|███▋      , loss=0.949 [01:45<02:50]\n",
      "Epoch [2/5]: [44/119]  37%|███▋      , loss=1 [01:45<02:50]\n",
      "Epoch [2/5]: [45/119]  38%|███▊      , loss=1 [01:45<02:01]\n",
      "2025-07-17 08:56:34,600 - INFO - Current learning rate: 0.0009863631325457364\n",
      "2025-07-17 08:56:40,308 - INFO - Epoch: 2/5, Iter: 46/119 -- train_loss: 0.8523\n",
      "Epoch [2/5]: [45/119]  38%|███▊      , loss=1 [01:51<02:01]\n",
      "Epoch [2/5]: [45/119]  38%|███▊      , loss=0.852 [01:51<02:01]\n",
      "Epoch [2/5]: [46/119]  39%|███▊      , loss=0.852 [01:51<03:28]\n",
      "2025-07-17 08:56:40,310 - INFO - Current learning rate: 0.0009883006391452032\n",
      "2025-07-17 08:56:40,430 - INFO - Epoch: 2/5, Iter: 47/119 -- train_loss: 1.0010\n",
      "Epoch [2/5]: [46/119]  39%|███▊      , loss=0.852 [01:51<03:28]\n",
      "Epoch [2/5]: [46/119]  39%|███▊      , loss=1 [01:51<03:28]    #015Epoch [2/5]: [47/119]  39%|███▉      , loss=1 [01:51<02:26]\n",
      "2025-07-17 08:56:40,432 - INFO - Current learning rate: 0.0009900914503621623\n",
      "2025-07-17 08:56:40,539 - INFO - Epoch: 2/5, Iter: 48/119 -- train_loss: 0.8570\n",
      "Epoch [2/5]: [47/119]  39%|███▉      , loss=1 [01:51<02:26]\n",
      "Epoch [2/5]: [47/119]  39%|███▉      , loss=0.857 [01:51<02:26]\n",
      "Epoch [2/5]: [48/119]  40%|████      , loss=0.857 [01:51<01:43]\n",
      "2025-07-17 08:56:40,540 - INFO - Current learning rate: 0.0009917350052242072\n",
      "2025-07-17 08:56:40,645 - INFO - Epoch: 2/5, Iter: 49/119 -- train_loss: 0.7536\n",
      "Epoch [2/5]: [48/119]  40%|████      , loss=0.857 [01:51<01:43]\n",
      "Epoch [2/5]: [48/119]  40%|████      , loss=0.754 [01:51<01:43]\n",
      "Epoch [2/5]: [49/119]  41%|████      , loss=0.754 [01:51<01:13]\n",
      "2025-07-17 08:56:40,647 - INFO - Current learning rate: 0.0009932307888870498\n",
      "2025-07-17 08:56:45,920 - INFO - Epoch: 2/5, Iter: 50/119 -- train_loss: 0.8799\n",
      "Epoch [2/5]: [49/119]  41%|████      , loss=0.754 [01:57<01:13]\n",
      "Epoch [2/5]: [49/119]  41%|████      , loss=0.88 [01:57<01:13]\n",
      "Epoch [2/5]: [50/119]  42%|████▏     , loss=0.88 [01:57<02:40]\n",
      "2025-07-17 08:56:45,922 - INFO - Current learning rate: 0.0009945783327957958\n",
      "2025-07-17 08:56:46,026 - INFO - Epoch: 2/5, Iter: 51/119 -- train_loss: 0.9659\n",
      "Epoch [2/5]: [50/119]  42%|████▏     , loss=0.88 [01:57<02:40]\n",
      "Epoch [2/5]: [50/119]  42%|████▏     , loss=0.966 [01:57<02:40]\n",
      "Epoch [2/5]: [51/119]  43%|████▎     , loss=0.966 [01:57<01:52]\n",
      "2025-07-17 08:56:46,028 - INFO - Current learning rate: 0.000995777214831719\n",
      "2025-07-17 08:56:49,183 - INFO - Epoch: 2/5, Iter: 52/119 -- train_loss: 0.9886\n",
      "Epoch [2/5]: [51/119]  43%|████▎     , loss=0.966 [02:00<01:52]\n",
      "Epoch [2/5]: [51/119]  43%|████▎     , loss=0.989 [02:00<01:52]#015Epoch [2/5]: [52/119]  44%|████▎     , loss=0.989 [02:00<02:21]\n",
      "2025-07-17 08:56:49,185 - INFO - Current learning rate: 0.0009968270594444913\n",
      "2025-07-17 08:56:49,315 - INFO - Epoch: 2/5, Iter: 53/119 -- train_loss: 0.8089\n",
      "Epoch [2/5]: [52/119]  44%|████▎     , loss=0.989 [02:00<02:21]\n",
      "Epoch [2/5]: [52/119]  44%|████▎     , loss=0.809 [02:00<02:21]\n",
      "Epoch [2/5]: [53/119]  45%|████▍     , loss=0.809 [02:00<01:39]\n",
      "2025-07-17 08:56:49,317 - INFO - Current learning rate: 0.0009977275377698227\n",
      "2025-07-17 08:56:56,298 - INFO - Epoch: 2/5, Iter: 54/119 -- train_loss: 1.0034\n",
      "Epoch [2/5]: [53/119]  45%|████▍     , loss=0.809 [02:07<01:39]\n",
      "Epoch [2/5]: [53/119]  45%|████▍     , loss=1 [02:07<01:39]\n",
      "Epoch [2/5]: [54/119]  45%|████▌     , loss=1 [02:07<03:25]\n",
      "2025-07-17 08:56:56,300 - INFO - Current learning rate: 0.0009984783677324794\n",
      "2025-07-17 08:56:56,442 - INFO - Epoch: 2/5, Iter: 55/119 -- train_loss: 1.0001\n",
      "Epoch [2/5]: [54/119]  45%|████▌     , loss=1 [02:07<03:25]\n",
      "Epoch [2/5]: [54/119]  45%|████▌     , loss=1 [02:07<03:25]\n",
      "Epoch [2/5]: [55/119]  46%|████▌     , loss=1 [02:07<02:24]\n",
      "2025-07-17 08:56:56,444 - INFO - Current learning rate: 0.000999079314134643\n",
      "2025-07-17 08:56:59,054 - INFO - Epoch: 2/5, Iter: 56/119 -- train_loss: 1.0017\n",
      "Epoch [2/5]: [55/119]  46%|████▌     , loss=1 [02:10<02:24]\n",
      "Epoch [2/5]: [55/119]  46%|████▌     , loss=1 [02:10<02:24]\n",
      "Epoch [2/5]: [56/119]  47%|████▋     , loss=1 [02:10<02:28]\n",
      "2025-07-17 08:56:59,055 - INFO - Current learning rate: 0.0009995301887295873\n",
      "2025-07-17 08:56:59,159 - INFO - Epoch: 2/5, Iter: 57/119 -- train_loss: 0.9691\n",
      "Epoch [2/5]: [56/119]  47%|████▋     , loss=1 [02:10<02:28]\n",
      "Epoch [2/5]: [56/119]  47%|████▋     , loss=0.969 [02:10<02:28]\n",
      "Epoch [2/5]: [57/119]  48%|████▊     , loss=0.969 [02:10<01:44]\n",
      "2025-07-17 08:56:59,160 - INFO - Current learning rate: 0.0009998308502806458\n",
      "2025-07-17 08:57:03,150 - INFO - Epoch: 2/5, Iter: 58/119 -- train_loss: 1.0013\n",
      "Epoch [2/5]: [57/119]  48%|████▊     , loss=0.969 [02:14<01:44]\n",
      "Epoch [2/5]: [57/119]  48%|████▊     , loss=1 [02:14<01:44]\n",
      "Epoch [2/5]: [58/119]  49%|████▊     , loss=1 [02:14<02:24]\n",
      "2025-07-17 08:57:03,153 - INFO - Current learning rate: 0.0009999812046054544\n",
      "2025-07-17 08:57:03,837 - INFO - Epoch: 2/5, Iter: 59/119 -- train_loss: 1.0015\n",
      "Epoch [2/5]: [58/119]  49%|████▊     , loss=1 [02:15<02:24]\n",
      "Epoch [2/5]: [58/119]  49%|████▊     , loss=1 [02:15<02:24]#015Epoch [2/5]: [59/119]  50%|████▉     , loss=1 [02:15<01:52]2025-07-17 08:57:03,840 - INFO - Current learning rate: 0.0009999964441167017\n",
      "2025-07-17 08:57:13,421 - INFO - Epoch: 2/5, Iter: 60/119 -- train_loss: 0.9373\n",
      "Epoch [2/5]: [59/119]  50%|████▉     , loss=1 [02:24<01:52]\n",
      "Epoch [2/5]: [59/119]  50%|████▉     , loss=0.937 [02:24<01:52]#015Epoch [2/5]: [60/119]  50%|█████     , loss=0.937 [02:24<04:06]2025-07-17 08:57:13,423 - INFO - Current learning rate: 0.000999967997353778\n",
      "2025-07-17 08:57:13,595 - INFO - Epoch: 2/5, Iter: 61/119 -- train_loss: 0.8300\n",
      "Epoch [2/5]: [60/119]  50%|█████     , loss=0.937 [02:24<04:06]\n",
      "Epoch [2/5]: [60/119]  50%|█████     , loss=0.83 [02:24<04:06]\n",
      "Epoch [2/5]: [61/119]  51%|█████▏    , loss=0.83 [02:24<02:52]\n",
      "2025-07-17 08:57:13,598 - INFO - Current learning rate: 0.0009999111054463852\n",
      "2025-07-17 08:57:13,763 - INFO - Epoch: 2/5, Iter: 62/119 -- train_loss: 1.0007\n",
      "Epoch [2/5]: [61/119]  51%|█████▏    , loss=0.83 [02:24<02:52]\n",
      "Epoch [2/5]: [61/119]  51%|█████▏    , loss=1 [02:24<02:52]\n",
      "Epoch [2/5]: [62/119]  52%|█████▏    , loss=1 [02:24<02:01]\n",
      "2025-07-17 08:57:13,764 - INFO - Current learning rate: 0.0009998257716313408\n",
      "2025-07-17 08:57:16,592 - INFO - Epoch: 2/5, Iter: 63/119 -- train_loss: 1.0010\n",
      "Epoch [2/5]: [62/119]  52%|█████▏    , loss=1 [02:27<02:01]\n",
      "Epoch [2/5]: [62/119]  52%|█████▏    , loss=1 [02:27<02:01]\n",
      "Epoch [2/5]: [63/119]  53%|█████▎    , loss=1 [02:27<02:11]\n",
      "2025-07-17 08:57:16,595 - INFO - Current learning rate: 0.0009997120007636398\n",
      "2025-07-17 08:57:24,578 - INFO - Epoch: 2/5, Iter: 64/119 -- train_loss: 0.8728\n",
      "Epoch [2/5]: [63/119]  53%|█████▎    , loss=1 [02:35<02:11]\n",
      "Epoch [2/5]: [63/119]  53%|█████▎    , loss=0.873 [02:35<02:11]\n",
      "Epoch [2/5]: [64/119]  54%|█████▍    , loss=0.873 [02:35<03:42]\n",
      "2025-07-17 08:57:24,580 - INFO - Current learning rate: 0.0009995697993161804\n",
      "2025-07-17 08:57:24,706 - INFO - Epoch: 2/5, Iter: 65/119 -- train_loss: 0.9386\n",
      "Epoch [2/5]: [64/119]  54%|█████▍    , loss=0.873 [02:35<03:42]\n",
      "Epoch [2/5]: [64/119]  54%|█████▍    , loss=0.939 [02:35<03:42]\n",
      "Epoch [2/5]: [65/119]  55%|█████▍    , loss=0.939 [02:35<02:34]\n",
      "2025-07-17 08:57:24,708 - INFO - Current learning rate: 0.0009993991753793942\n",
      "2025-07-17 08:57:24,812 - INFO - Epoch: 2/5, Iter: 66/119 -- train_loss: 1.0010\n",
      "Epoch [2/5]: [65/119]  55%|█████▍    , loss=0.939 [02:35<02:34]\n",
      "Epoch [2/5]: [65/119]  55%|█████▍    , loss=1 [02:35<02:34]\n",
      "Epoch [2/5]: [66/119]  55%|█████▌    , loss=1 [02:35<01:47]\n",
      "2025-07-17 08:57:24,813 - INFO - Current learning rate: 0.000999200138660786\n",
      "2025-07-17 08:57:24,936 - INFO - Epoch: 2/5, Iter: 67/119 -- train_loss: 1.0019\n",
      "Epoch [2/5]: [66/119]  55%|█████▌    , loss=1 [02:36<01:47]\n",
      "Epoch [2/5]: [66/119]  55%|█████▌    , loss=1 [02:36<01:47]\n",
      "Epoch [2/5]: [67/119]  56%|█████▋    , loss=1 [02:36<01:16]\n",
      "2025-07-17 08:57:24,939 - INFO - Current learning rate: 0.0009989727004843826\n",
      "2025-07-17 08:57:33,486 - INFO - Epoch: 2/5, Iter: 68/119 -- train_loss: 0.9749\n",
      "Epoch [2/5]: [67/119]  56%|█████▋    , loss=1 [02:44<01:16]\n",
      "Epoch [2/5]: [67/119]  56%|█████▋    , loss=0.975 [02:44<01:16]\n",
      "Epoch [2/5]: [68/119]  57%|█████▋    , loss=0.975 [02:44<03:03]\n",
      "2025-07-17 08:57:33,488 - INFO - Current learning rate: 0.0009987168737900878\n",
      "2025-07-17 08:57:33,615 - INFO - Epoch: 2/5, Iter: 69/119 -- train_loss: 0.9459\n",
      "Epoch [2/5]: [68/119]  57%|█████▋    , loss=0.975 [02:44<03:03]\n",
      "Epoch [2/5]: [68/119]  57%|█████▋    , loss=0.946 [02:44<03:03]\n",
      "Epoch [2/5]: [69/119]  58%|█████▊    , loss=0.946 [02:44<02:07]\n",
      "2025-07-17 08:57:33,617 - INFO - Current learning rate: 0.0009984326731329455\n",
      "2025-07-17 08:57:33,742 - INFO - Epoch: 2/5, Iter: 70/119 -- train_loss: 1.0026\n",
      "Epoch [2/5]: [69/119]  58%|█████▊    , loss=0.946 [02:44<02:07]\n",
      "Epoch [2/5]: [69/119]  58%|█████▊    , loss=1 [02:44<02:07]\n",
      "Epoch [2/5]: [70/119]  59%|█████▉    , loss=1 [02:44<01:29]\n",
      "2025-07-17 08:57:33,743 - INFO - Current learning rate: 0.0009981201146823133\n",
      "2025-07-17 08:57:36,071 - INFO - Epoch: 2/5, Iter: 71/119 -- train_loss: 1.0020\n",
      "Epoch [2/5]: [70/119]  59%|█████▉    , loss=1 [02:47<01:29]\n",
      "Epoch [2/5]: [70/119]  59%|█████▉    , loss=1 [02:47<01:29]\n",
      "Epoch [2/5]: [71/119]  60%|█████▉    , loss=1 [02:47<01:34]\n",
      "2025-07-17 08:57:36,073 - INFO - Current learning rate: 0.0009977792162209405\n",
      "2025-07-17 08:57:43,964 - INFO - Epoch: 2/5, Iter: 72/119 -- train_loss: 1.0029\n",
      "Epoch [2/5]: [71/119]  60%|█████▉    , loss=1 [02:55<01:34]\n",
      "Epoch [2/5]: [71/119]  60%|█████▉    , loss=1 [02:55<01:34]#015Epoch [2/5]: [72/119]  61%|██████    , loss=1 [02:55<02:56]\n",
      "2025-07-17 08:57:43,965 - INFO - Current learning rate: 0.0009974099971439586\n",
      "2025-07-17 08:57:44,093 - INFO - Epoch: 2/5, Iter: 73/119 -- train_loss: 1.0016\n",
      "Epoch [2/5]: [72/119]  61%|██████    , loss=1 [02:55<02:56]\n",
      "Epoch [2/5]: [72/119]  61%|██████    , loss=1 [02:55<02:56]\n",
      "Epoch [2/5]: [73/119]  61%|██████▏   , loss=1 [02:55<02:02]\n",
      "2025-07-17 08:57:44,094 - INFO - Current learning rate: 0.0009970124784577758\n",
      "2025-07-17 08:57:44,224 - INFO - Epoch: 2/5, Iter: 74/119 -- train_loss: 1.0014\n",
      "Epoch [2/5]: [73/119]  61%|██████▏   , loss=1 [02:55<02:02]\n",
      "Epoch [2/5]: [73/119]  61%|██████▏   , loss=1 [02:55<02:02]\n",
      "Epoch [2/5]: [74/119]  62%|██████▏   , loss=1 [02:55<01:25]\n",
      "2025-07-17 08:57:44,227 - INFO - Current learning rate: 0.0009965866827788832\n",
      "2025-07-17 08:57:47,416 - INFO - Epoch: 2/5, Iter: 75/119 -- train_loss: 0.9449\n",
      "Epoch [2/5]: [74/119]  62%|██████▏   , loss=1 [02:58<01:25]\n",
      "Epoch [2/5]: [74/119]  62%|██████▏   , loss=0.945 [02:58<01:25]\n",
      "Epoch [2/5]: [75/119]  63%|██████▎   , loss=0.945 [02:58<01:40]\n",
      "2025-07-17 08:57:47,419 - INFO - Current learning rate: 0.0009961326343325672\n",
      "2025-07-17 08:57:50,845 - INFO - Epoch: 2/5, Iter: 76/119 -- train_loss: 0.7324\n",
      "Epoch [2/5]: [75/119]  63%|██████▎   , loss=0.945 [03:02<01:40]\n",
      "Epoch [2/5]: [75/119]  63%|██████▎   , loss=0.732 [03:02<01:40]\n",
      "Epoch [2/5]: [76/119]  64%|██████▍   , loss=0.732 [03:02<01:53]\n",
      "2025-07-17 08:57:50,847 - INFO - Current learning rate: 0.0009956503589515322\n",
      "2025-07-17 08:57:50,980 - INFO - Epoch: 2/5, Iter: 77/119 -- train_loss: 1.0091\n",
      "Epoch [2/5]: [76/119]  64%|██████▍   , loss=0.732 [03:02<01:53]\n",
      "Epoch [2/5]: [76/119]  64%|██████▍   , loss=1.01 [03:02<01:53]\n",
      "Epoch [2/5]: [77/119]  65%|██████▍   , loss=1.01 [03:02<01:19]\n",
      "2025-07-17 08:57:50,982 - INFO - Current learning rate: 0.0009951398840744297\n",
      "2025-07-17 08:57:51,117 - INFO - Epoch: 2/5, Iter: 78/119 -- train_loss: 0.8706\n",
      "Epoch [2/5]: [77/119]  65%|██████▍   , loss=1.01 [03:02<01:19]\n",
      "Epoch [2/5]: [77/119]  65%|██████▍   , loss=0.871 [03:02<01:19]\n",
      "Epoch [2/5]: [78/119]  66%|██████▌   , loss=0.871 [03:02<00:55]\n",
      "2025-07-17 08:57:51,118 - INFO - Current learning rate: 0.0009946012387442983\n",
      "2025-07-17 08:57:58,029 - INFO - Epoch: 2/5, Iter: 79/119 -- train_loss: 0.6582\n",
      "Epoch [2/5]: [78/119]  66%|██████▌   , loss=0.871 [03:09<00:55]\n",
      "Epoch [2/5]: [78/119]  66%|██████▌   , loss=0.658 [03:09<00:55]\n",
      "Epoch [2/5]: [79/119]  66%|██████▋   , loss=0.658 [03:09<02:01]\n",
      "2025-07-17 08:57:58,031 - INFO - Current learning rate: 0.0009940344536069103\n",
      "2025-07-17 08:58:02,705 - INFO - Epoch: 2/5, Iter: 80/119 -- train_loss: 1.0018\n",
      "Epoch [2/5]: [79/119]  66%|██████▋   , loss=0.658 [03:13<02:01]\n",
      "Epoch [2/5]: [79/119]  66%|██████▋   , loss=1 [03:13<02:01]\n",
      "Epoch [2/5]: [80/119]  67%|██████▋   , loss=1 [03:13<02:17]\n",
      "2025-07-17 08:58:02,707 - INFO - Current learning rate: 0.0009934395609090287\n",
      "2025-07-17 08:58:02,864 - INFO - Epoch: 2/5, Iter: 81/119 -- train_loss: 0.8062\n",
      "Epoch [2/5]: [80/119]  67%|██████▋   , loss=1 [03:14<02:17]\n",
      "Epoch [2/5]: [80/119]  67%|██████▋   , loss=0.806 [03:14<02:17]\n",
      "Epoch [2/5]: [81/119]  68%|██████▊   , loss=0.806 [03:14<01:35]\n",
      "2025-07-17 08:58:02,866 - INFO - Current learning rate: 0.0009928165944965732\n",
      "2025-07-17 08:58:03,011 - INFO - Epoch: 2/5, Iter: 82/119 -- train_loss: 1.0028\n",
      "Epoch [2/5]: [81/119]  68%|██████▊   , loss=0.806 [03:14<01:35]\n",
      "Epoch [2/5]: [81/119]  68%|██████▊   , loss=1 [03:14<01:35]\n",
      "Epoch [2/5]: [82/119]  69%|██████▉   , loss=1 [03:14<01:06]\n",
      "2025-07-17 08:58:03,013 - INFO - Current learning rate: 0.000992165589812693\n",
      "2025-07-17 08:58:06,109 - INFO - Epoch: 2/5, Iter: 83/119 -- train_loss: 0.8101\n",
      "Epoch [2/5]: [82/119]  69%|██████▉   , loss=1 [03:17<01:06]\n",
      "Epoch [2/5]: [82/119]  69%|██████▉   , loss=0.81 [03:17<01:06]\n",
      "Epoch [2/5]: [83/119]  70%|██████▉   , loss=0.81 [03:17<01:18]\n",
      "2025-07-17 08:58:06,111 - INFO - Current learning rate: 0.0009914865838957516\n",
      "2025-07-17 08:58:14,604 - INFO - Epoch: 2/5, Iter: 84/119 -- train_loss: 0.9379\n",
      "Epoch [2/5]: [83/119]  70%|██████▉   , loss=0.81 [03:25<01:18]\n",
      "Epoch [2/5]: [83/119]  70%|██████▉   , loss=0.938 [03:25<01:18]\n",
      "Epoch [2/5]: [84/119]  71%|███████   , loss=0.938 [03:25<02:22]\n",
      "2025-07-17 08:58:14,607 - INFO - Current learning rate: 0.0009907796153772188\n",
      "2025-07-17 08:58:14,714 - INFO - Epoch: 2/5, Iter: 85/119 -- train_loss: 1.0032\n",
      "Epoch [2/5]: [84/119]  71%|███████   , loss=0.938 [03:25<02:22]\n",
      "Epoch [2/5]: [84/119]  71%|███████   , loss=1 [03:25<02:22]\n",
      "Epoch [2/5]: [85/119]  71%|███████▏  , loss=1 [03:25<01:38]\n",
      "2025-07-17 08:58:14,716 - INFO - Current learning rate: 0.0009900447244794732\n",
      "2025-07-17 08:58:14,820 - INFO - Epoch: 2/5, Iter: 86/119 -- train_loss: 0.8367\n",
      "Epoch [2/5]: [85/119]  71%|███████▏  , loss=1 [03:26<01:38]\n",
      "Epoch [2/5]: [85/119]  71%|███████▏  , loss=0.837 [03:26<01:38]#015Epoch [2/5]: [86/119]  72%|███████▏  , loss=0.837 [03:26<01:07]\n",
      "2025-07-17 08:58:14,821 - INFO - Current learning rate: 0.0009892819530135136\n",
      "2025-07-17 08:58:14,924 - INFO - Epoch: 2/5, Iter: 87/119 -- train_loss: 1.0021\n",
      "Epoch [2/5]: [86/119]  72%|███████▏  , loss=0.837 [03:26<01:07]\n",
      "Epoch [2/5]: [86/119]  72%|███████▏  , loss=1 [03:26<01:07]\n",
      "Epoch [2/5]: [87/119]  73%|███████▎  , loss=1 [03:26<00:47]\n",
      "2025-07-17 08:58:14,926 - INFO - Current learning rate: 0.000988491344376581\n",
      "2025-07-17 08:58:24,563 - INFO - Epoch: 2/5, Iter: 88/119 -- train_loss: 0.9167\n",
      "Epoch [2/5]: [87/119]  73%|███████▎  , loss=1 [03:35<00:47]\n",
      "Epoch [2/5]: [87/119]  73%|███████▎  , loss=0.917 [03:35<00:47]\n",
      "Epoch [2/5]: [88/119]  74%|███████▍  , loss=0.917 [03:35<02:01]\n",
      "2025-07-17 08:58:24,565 - INFO - Current learning rate: 0.0009876729435496874\n",
      "2025-07-17 08:58:24,700 - INFO - Epoch: 2/5, Iter: 89/119 -- train_loss: 0.9361\n",
      "Epoch [2/5]: [88/119]  74%|███████▍  , loss=0.917 [03:35<02:01]\n",
      "Epoch [2/5]: [88/119]  74%|███████▍  , loss=0.936 [03:35<02:01]\n",
      "Epoch [2/5]: [89/119]  75%|███████▍  , loss=0.936 [03:35<01:23]\n",
      "2025-07-17 08:58:24,702 - INFO - Current learning rate: 0.0009868267970950591\n",
      "2025-07-17 08:58:24,847 - INFO - Epoch: 2/5, Iter: 90/119 -- train_loss: 1.0016\n",
      "Epoch [2/5]: [89/119]  75%|███████▍  , loss=0.936 [03:36<01:23]\n",
      "Epoch [2/5]: [89/119]  75%|███████▍  , loss=1 [03:36<01:23]\n",
      "Epoch [2/5]: [90/119]  76%|███████▌  , loss=1 [03:36<00:57]\n",
      "2025-07-17 08:58:24,850 - INFO - Current learning rate: 0.000985952953153486\n",
      "2025-07-17 08:58:24,993 - INFO - Epoch: 2/5, Iter: 91/119 -- train_loss: 0.9169\n",
      "Epoch [2/5]: [90/119]  76%|███████▌  , loss=1 [03:36<00:57]\n",
      "Epoch [2/5]: [90/119]  76%|███████▌  , loss=0.917 [03:36<00:57]#015Epoch [2/5]: [91/119]  76%|███████▋  , loss=0.917 [03:36<00:40]\n",
      "2025-07-17 08:58:24,994 - INFO - Current learning rate: 0.0009850514614415835\n",
      "2025-07-17 08:58:32,450 - INFO - Epoch: 2/5, Iter: 92/119 -- train_loss: 1.0191\n",
      "Epoch [2/5]: [91/119]  76%|███████▋  , loss=0.917 [03:43<00:40]\n",
      "Epoch [2/5]: [91/119]  76%|███████▋  , loss=1.02 [03:43<00:40]\n",
      "Epoch [2/5]: [92/119]  77%|███████▋  , loss=1.02 [03:43<01:27]\n",
      "2025-07-17 08:58:32,453 - INFO - Current learning rate: 0.0009841223732489639\n",
      "2025-07-17 08:58:32,629 - INFO - Epoch: 2/5, Iter: 93/119 -- train_loss: 0.9639\n",
      "Epoch [2/5]: [92/119]  77%|███████▋  , loss=1.02 [03:43<01:27]\n",
      "Epoch [2/5]: [92/119]  77%|███████▋  , loss=0.964 [03:43<01:27]\n",
      "Epoch [2/5]: [93/119]  78%|███████▊  , loss=0.964 [03:43<01:00]\n",
      "2025-07-17 08:58:32,632 - INFO - Current learning rate: 0.000983165741435317\n",
      "2025-07-17 08:58:32,794 - INFO - Epoch: 2/5, Iter: 94/119 -- train_loss: 1.0013\n",
      "Epoch [2/5]: [93/119]  78%|███████▊  , loss=0.964 [03:43<01:00]\n",
      "Epoch [2/5]: [93/119]  78%|███████▊  , loss=1 [03:43<01:00]\n",
      "Epoch [2/5]: [94/119]  79%|███████▉  , loss=1 [03:43<00:41]\n",
      "2025-07-17 08:58:32,795 - INFO - Current learning rate: 0.0009821816204274051\n",
      "2025-07-17 08:58:32,920 - INFO - Epoch: 2/5, Iter: 95/119 -- train_loss: 1.0019\n",
      "Epoch [2/5]: [94/119]  79%|███████▉  , loss=1 [03:44<00:41]\n",
      "Epoch [2/5]: [94/119]  79%|███████▉  , loss=1 [03:44<00:41]\n",
      "Epoch [2/5]: [95/119]  80%|███████▉  , loss=1 [03:44<00:29]\n",
      "2025-07-17 08:58:32,921 - INFO - Current learning rate: 0.0009811700662159639\n",
      "2025-07-17 08:58:39,058 - INFO - Epoch: 2/5, Iter: 96/119 -- train_loss: 0.8994\n",
      "Epoch [2/5]: [95/119]  80%|███████▉  , loss=1 [03:50<00:29]\n",
      "Epoch [2/5]: [95/119]  80%|███████▉  , loss=0.899 [03:50<00:29]\n",
      "Epoch [2/5]: [96/119]  81%|████████  , loss=0.899 [03:50<01:01]\n",
      "2025-07-17 08:58:39,060 - INFO - Current learning rate: 0.0009801311363525187\n",
      "2025-07-17 08:58:39,234 - INFO - Epoch: 2/5, Iter: 97/119 -- train_loss: 0.9069\n",
      "Epoch [2/5]: [96/119]  81%|████████  , loss=0.899 [03:50<01:01]\n",
      "Epoch [2/5]: [96/119]  81%|████████  , loss=0.907 [03:50<01:01]\n",
      "Epoch [2/5]: [97/119]  82%|████████▏ , loss=0.907 [03:50<00:42]\n",
      "2025-07-17 08:58:39,236 - INFO - Current learning rate: 0.0009790648899461092\n",
      "2025-07-17 08:58:43,114 - INFO - Epoch: 2/5, Iter: 98/119 -- train_loss: 1.0009\n",
      "Epoch [2/5]: [97/119]  82%|████████▏ , loss=0.907 [03:54<00:42]\n",
      "Epoch [2/5]: [97/119]  82%|████████▏ , loss=1 [03:54<00:42]\n",
      "Epoch [2/5]: [98/119]  82%|████████▏ , loss=1 [03:54<00:52]\n",
      "2025-07-17 08:58:43,116 - INFO - Current learning rate: 0.0009779713876599272\n",
      "2025-07-17 08:58:43,268 - INFO - Epoch: 2/5, Iter: 99/119 -- train_loss: 1.0007\n",
      "Epoch [2/5]: [98/119]  82%|████████▏ , loss=1 [03:54<00:52]\n",
      "Epoch [2/5]: [98/119]  82%|████████▏ , loss=1 [03:54<00:52]\n",
      "Epoch [2/5]: [99/119]  83%|████████▎ , loss=1 [03:54<00:36]\n",
      "2025-07-17 08:58:43,270 - INFO - Current learning rate: 0.0009768506917078646\n",
      "2025-07-17 08:58:51,716 - INFO - Epoch: 2/5, Iter: 100/119 -- train_loss: 0.8384\n",
      "Epoch [2/5]: [99/119]  83%|████████▎ , loss=1 [04:02<00:36]\n",
      "Epoch [2/5]: [99/119]  83%|████████▎ , loss=0.838 [04:02<00:36]#015Epoch [2/5]: [100/119]  84%|████████▍ , loss=0.838 [04:02<01:12]\n",
      "2025-07-17 08:58:51,717 - INFO - Current learning rate: 0.0009757028658509734\n",
      "2025-07-17 08:58:51,849 - INFO - Epoch: 2/5, Iter: 101/119 -- train_loss: 1.0012\n",
      "Epoch [2/5]: [100/119]  84%|████████▍ , loss=0.838 [04:03<01:12]\n",
      "Epoch [2/5]: [100/119]  84%|████████▍ , loss=1 [04:03<01:12]\n",
      "Epoch [2/5]: [101/119]  85%|████████▍ , loss=1 [04:03<00:48]\n",
      "2025-07-17 08:58:51,852 - INFO - Current learning rate: 0.0009745279753938402\n",
      "2025-07-17 08:58:51,995 - INFO - Epoch: 2/5, Iter: 102/119 -- train_loss: 0.8265\n",
      "Epoch [2/5]: [101/119]  85%|████████▍ , loss=1 [04:03<00:48]\n",
      "Epoch [2/5]: [101/119]  85%|████████▍ , loss=0.826 [04:03<00:48]\n",
      "Epoch [2/5]: [102/119]  86%|████████▌ , loss=0.826 [04:03<00:32]\n",
      "2025-07-17 08:58:51,996 - INFO - Current learning rate: 0.000973326087180868\n",
      "2025-07-17 08:58:52,138 - INFO - Epoch: 2/5, Iter: 103/119 -- train_loss: 1.0017\n",
      "Epoch [2/5]: [102/119]  86%|████████▌ , loss=0.826 [04:03<00:32]\n",
      "Epoch [2/5]: [102/119]  86%|████████▌ , loss=1 [04:03<00:32]\n",
      "Epoch [2/5]: [103/119]  87%|████████▋ , loss=1 [04:03<00:22]\n",
      "2025-07-17 08:58:52,140 - INFO - Current learning rate: 0.0009720972695924745\n",
      "2025-07-17 08:58:58,217 - INFO - Epoch: 2/5, Iter: 104/119 -- train_loss: 0.7586\n",
      "Epoch [2/5]: [103/119]  87%|████████▋ , loss=1 [04:09<00:22]\n",
      "Epoch [2/5]: [103/119]  87%|████████▋ , loss=0.759 [04:09<00:22]\n",
      "Epoch [2/5]: [104/119]  87%|████████▋ , loss=0.759 [04:09<00:42]\n",
      "2025-07-17 08:58:58,218 - INFO - Current learning rate: 0.000970841592541202\n",
      "2025-07-17 08:58:58,369 - INFO - Epoch: 2/5, Iter: 105/119 -- train_loss: 1.0028\n",
      "Epoch [2/5]: [104/119]  87%|████████▋ , loss=0.759 [04:09<00:42]\n",
      "Epoch [2/5]: [104/119]  87%|████████▋ , loss=1 [04:09<00:42]\n",
      "Epoch [2/5]: [105/119]  88%|████████▊ , loss=1 [04:09<00:28]\n",
      "2025-07-17 08:58:58,370 - INFO - Current learning rate: 0.0009695591274677391\n",
      "2025-07-17 08:58:58,528 - INFO - Epoch: 2/5, Iter: 106/119 -- train_loss: 0.9007\n",
      "Epoch [2/5]: [105/119]  88%|████████▊ , loss=1 [04:09<00:28]\n",
      "Epoch [2/5]: [105/119]  88%|████████▊ , loss=0.901 [04:09<00:28]\n",
      "Epoch [2/5]: [106/119]  89%|████████▉ , loss=0.901 [04:09<00:18]\n",
      "2025-07-17 08:58:58,530 - INFO - Current learning rate: 0.0009682499473368564\n",
      "2025-07-17 08:59:01,239 - INFO - Epoch: 2/5, Iter: 107/119 -- train_loss: 0.9266\n",
      "Epoch [2/5]: [106/119]  89%|████████▉ , loss=0.901 [04:12<00:18]\n",
      "Epoch [2/5]: [106/119]  89%|████████▉ , loss=0.927 [04:12<00:18]\n",
      "Epoch [2/5]: [107/119]  90%|████████▉ , loss=0.927 [04:12<00:21]\n",
      "2025-07-17 08:59:01,241 - INFO - Current learning rate: 0.000966914126633255\n",
      "2025-07-17 08:59:12,482 - INFO - Epoch: 2/5, Iter: 108/119 -- train_loss: 1.0018\n",
      "Epoch [2/5]: [107/119]  90%|████████▉ , loss=0.927 [04:23<00:21]\n",
      "Epoch [2/5]: [107/119]  90%|████████▉ , loss=1 [04:23<00:21]\n",
      "Epoch [2/5]: [108/119]  91%|█████████ , loss=1 [04:23<00:51]2025-07-17 08:59:12,484 - INFO - Current learning rate: 0.0009655517413573294\n",
      "2025-07-17 08:59:12,616 - INFO - Epoch: 2/5, Iter: 109/119 -- train_loss: 0.8938\n",
      "Epoch [2/5]: [108/119]  91%|█████████ , loss=1 [04:23<00:51]\n",
      "Epoch [2/5]: [108/119]  91%|█████████ , loss=0.894 [04:23<00:51]\n",
      "Epoch [2/5]: [109/119]  92%|█████████▏, loss=0.894 [04:23<00:32]\n",
      "2025-07-17 08:59:12,617 - INFO - Current learning rate: 0.0009641628690208426\n",
      "2025-07-17 08:59:12,739 - INFO - Epoch: 2/5, Iter: 110/119 -- train_loss: 0.9449\n",
      "Epoch [2/5]: [109/119]  92%|█████████▏, loss=0.894 [04:23<00:32]\n",
      "Epoch [2/5]: [109/119]  92%|█████████▏, loss=0.945 [04:23<00:32]\n",
      "Epoch [2/5]: [110/119]  92%|█████████▏, loss=0.945 [04:23<00:21]\n",
      "2025-07-17 08:59:12,742 - INFO - Current learning rate: 0.0009627475886425167\n",
      "2025-07-17 08:59:12,898 - INFO - Epoch: 2/5, Iter: 111/119 -- train_loss: 1.0013\n",
      "Epoch [2/5]: [110/119]  92%|█████████▏, loss=0.945 [04:24<00:21]\n",
      "Epoch [2/5]: [110/119]  92%|█████████▏, loss=1 [04:24<00:21]\n",
      "Epoch [2/5]: [111/119]  93%|█████████▎, loss=1 [04:24<00:13]\n",
      "2025-07-17 08:59:12,899 - INFO - Current learning rate: 0.0009613059807435375\n",
      "2025-07-17 08:59:24,981 - INFO - Epoch: 2/5, Iter: 112/119 -- train_loss: 0.9908\n",
      "Epoch [2/5]: [111/119]  93%|█████████▎, loss=1 [04:36<00:13]\n",
      "Epoch [2/5]: [111/119]  93%|█████████▎, loss=0.991 [04:36<00:13]\n",
      "Epoch [2/5]: [112/119]  94%|█████████▍, loss=0.991 [04:36<00:33]\n",
      "2025-07-17 08:59:24,982 - INFO - Current learning rate: 0.0009598381273429725\n",
      "2025-07-17 08:59:25,094 - INFO - Epoch: 2/5, Iter: 113/119 -- train_loss: 0.6660\n",
      "Epoch [2/5]: [112/119]  94%|█████████▍, loss=0.991 [04:36<00:33]\n",
      "Epoch [2/5]: [112/119]  94%|█████████▍, loss=0.666 [04:36<00:33]\n",
      "Epoch [2/5]: [113/119]  95%|█████████▍, loss=0.666 [04:36<00:20]\n",
      "2025-07-17 08:59:25,096 - INFO - Current learning rate: 0.0009583441119531051\n",
      "2025-07-17 08:59:25,200 - INFO - Epoch: 2/5, Iter: 114/119 -- train_loss: 1.0010\n",
      "Epoch [2/5]: [113/119]  95%|█████████▍, loss=0.666 [04:36<00:20]\n",
      "Epoch [2/5]: [113/119]  95%|█████████▍, loss=1 [04:36<00:20]\n",
      "Epoch [2/5]: [114/119]  96%|█████████▌, loss=1 [04:36<00:12]\n",
      "2025-07-17 08:59:25,201 - INFO - Current learning rate: 0.0009568240195746829\n",
      "2025-07-17 08:59:25,307 - INFO - Epoch: 2/5, Iter: 115/119 -- train_loss: 1.0079\n",
      "Epoch [2/5]: [114/119]  96%|█████████▌, loss=1 [04:36<00:12]\n",
      "Epoch [2/5]: [114/119]  96%|█████████▌, loss=1.01 [04:36<00:12]\n",
      "Epoch [2/5]: [115/119]  97%|█████████▋, loss=1.01 [04:36<00:06]\n",
      "2025-07-17 08:59:25,308 - INFO - Current learning rate: 0.000955277936692082\n",
      "2025-07-17 08:59:38,301 - INFO - Epoch: 2/5, Iter: 116/119 -- train_loss: 1.0009\n",
      "Epoch [2/5]: [115/119]  97%|█████████▋, loss=1.01 [04:49<00:06]\n",
      "Epoch [2/5]: [115/119]  97%|█████████▋, loss=1 [04:49<00:06]\n",
      "Epoch [2/5]: [116/119]  97%|█████████▋, loss=1 [04:49<00:15]\n",
      "2025-07-17 08:59:38,302 - INFO - Current learning rate: 0.0009537059512683863\n",
      "2025-07-17 08:59:38,402 - INFO - Epoch: 2/5, Iter: 117/119 -- train_loss: 1.0018\n",
      "Epoch [2/5]: [116/119]  97%|█████████▋, loss=1 [04:49<00:15]\n",
      "Epoch [2/5]: [116/119]  97%|█████████▋, loss=1 [04:49<00:15]\n",
      "Epoch [2/5]: [117/119]  98%|█████████▊, loss=1 [04:49<00:07]\n",
      "2025-07-17 08:59:38,403 - INFO - Current learning rate: 0.0009521081527403829\n",
      "2025-07-17 08:59:38,504 - INFO - Epoch: 2/5, Iter: 118/119 -- train_loss: 0.4547\n",
      "Epoch [2/5]: [117/119]  98%|█████████▊, loss=1 [04:49<00:07]\n",
      "Epoch [2/5]: [117/119]  98%|█████████▊, loss=0.455 [04:49<00:07]\n",
      "Epoch [2/5]: [118/119]  99%|█████████▉, loss=0.455 [04:49<00:02]\n",
      "2025-07-17 08:59:38,505 - INFO - Current learning rate: 0.0009504846320134736\n",
      "2025-07-17 08:59:38,605 - INFO - Epoch: 2/5, Iter: 119/119 -- train_loss: 0.9856\n",
      "Epoch [2/5]: [118/119]  99%|█████████▉, loss=0.455 [04:49<00:02]\n",
      "Epoch [2/5]: [118/119]  99%|█████████▉, loss=0.986 [04:49<00:02]\n",
      "Epoch [2/5]: [119/119] 100%|██████████, loss=0.986 [04:49<00:00]\n",
      "2025-07-17 08:59:38,606 - INFO - Current learning rate: 0.0009488354814565037\n",
      "2025-07-17 08:59:38,606 - INFO - Engine run resuming from iteration 0, epoch 1 until 2 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [2/2]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [2/2]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [2/2]: [2/20]  10%|#033[32m█         #033[0m [00:00<00:09]#033[A\n",
      "Epoch [2/2]: [2/20]  10%|#033[32m█         #033[0m [00:01<00:09]#033[A\n",
      "Epoch [2/2]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:10]#033[A\n",
      "Epoch [2/2]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:10]\n",
      "#033[A\n",
      "Epoch [2/2]: [4/20]  20%|#033[32m██        #033[0m [00:01<00:08]\n",
      "#033[A\n",
      "Epoch [2/2]: [4/20]  20%|#033[32m██        #033[0m [00:03<00:08]#033[A\n",
      "Epoch [2/2]: [5/20]  25%|#033[32m██▌       #033[0m [00:03<00:16]#033[A\n",
      "Epoch [2/2]: [5/20]  25%|#033[32m██▌       #033[0m [00:04<00:16]#033[A\n",
      "Epoch [2/2]: [6/20]  30%|#033[32m███       #033[0m [00:04<00:12]#033[A\n",
      "Epoch [2/2]: [6/20]  30%|#033[32m███       #033[0m [00:04<00:12]#033[A\n",
      "Epoch [2/2]: [7/20]  35%|#033[32m███▌      #033[0m [00:04<00:10]#033[A\n",
      "Epoch [2/2]: [7/20]  35%|#033[32m███▌      #033[0m [00:05<00:10]\n",
      "#033[A\n",
      "Epoch [2/2]: [8/20]  40%|#033[32m████      #033[0m [00:05<00:08]\n",
      "#033[A\n",
      "Epoch [2/2]: [8/20]  40%|#033[32m████      #033[0m [00:06<00:08]\n",
      "#033[A\n",
      "Epoch [2/2]: [9/20]  45%|#033[32m████▌     #033[0m [00:06<00:09]\n",
      "#033[A\n",
      "Epoch [2/2]: [9/20]  45%|#033[32m████▌     #033[0m [00:08<00:09]#033[A\n",
      "Epoch [2/2]: [10/20]  50%|#033[32m█████     #033[0m [00:08<00:12]#033[A\n",
      "Epoch [2/2]: [10/20]  50%|#033[32m█████     #033[0m [00:10<00:12]#033[A\n",
      "Epoch [2/2]: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:13]#033[A\n",
      "Epoch [2/2]: [11/20]  55%|#033[32m█████▌    #033[0m [00:11<00:13]#033[A\n",
      "Epoch [2/2]: [12/20]  60%|#033[32m██████    #033[0m [00:11<00:09]#033[A\n",
      "Epoch [2/2]: [12/20]  60%|#033[32m██████    #033[0m [00:12<00:09]\n",
      "#033[A\n",
      "Epoch [2/2]: [13/20]  65%|#033[32m██████▌   #033[0m [00:12<00:07]\n",
      "#033[A\n",
      "Epoch [2/2]: [13/20]  65%|#033[32m██████▌   #033[0m [00:13<00:07]#033[A\n",
      "Epoch [2/2]: [14/20]  70%|#033[32m███████   #033[0m [00:13<00:06]#033[A\n",
      "Epoch [2/2]: [14/20]  70%|#033[32m███████   #033[0m [00:13<00:06]\n",
      "#033[A\n",
      "Epoch [2/2]: [15/20]  75%|#033[32m███████▌  #033[0m [00:13<00:04]\n",
      "#033[A\n",
      "Epoch [2/2]: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:04]#033[A\n",
      "Epoch [2/2]: [16/20]  80%|#033[32m████████  #033[0m [00:14<00:03]#033[A\n",
      "Epoch [2/2]: [16/20]  80%|#033[32m████████  #033[0m [00:15<00:03]#033[A\n",
      "Epoch [2/2]: [17/20]  85%|#033[32m████████▌ #033[0m [00:15<00:02]#033[A\n",
      "Epoch [2/2]: [17/20]  85%|#033[32m████████▌ #033[0m [00:15<00:02]#033[A\n",
      "Epoch [2/2]: [18/20]  90%|#033[32m█████████ #033[0m [00:15<00:01]#033[A\n",
      "Epoch [2/2]: [18/20]  90%|#033[32m█████████ #033[0m [00:16<00:01]#033[A\n",
      "Epoch [2/2]: [19/20]  95%|#033[32m█████████▌#033[0m [00:16<00:00]#033[A\n",
      "Epoch [2/2]: [19/20]  95%|#033[32m█████████▌#033[0m [00:17<00:00]#033[A\n",
      "Epoch [2/2]: [20/20] 100%|#033[32m██████████#033[0m [00:17<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-17 09:00:02,341 - INFO - Epoch[2] Complete. Time taken: 00:00:23.655\n",
      "2025-07-17 09:00:02,342 - INFO - Engine run finished. Time taken: 00:00:23.735\n",
      "2025-07-17 09:00:02,410 - INFO - Epoch[2] Complete. Time taken: 00:05:24.817\n",
      "2025-07-17 09:00:12,098 - INFO - Epoch: 3/5, Iter: 1/119 -- train_loss: 0.9190\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [3/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [3/5]: [1/119]   1%|          , loss=0.919 [00:00<?]\n",
      "2025-07-17 09:00:12,102 - INFO - Current learning rate: 0.000947160794896505\n",
      "2025-07-17 09:00:12,233 - INFO - Epoch: 3/5, Iter: 2/119 -- train_loss: 0.9806\n",
      "Epoch [3/5]: [1/119]   1%|          , loss=0.919 [00:00<?]\n",
      "Epoch [3/5]: [1/119]   1%|          , loss=0.981 [00:00<?]\n",
      "Epoch [3/5]: [2/119]   2%|▏         , loss=0.981 [00:00<00:15]\n",
      "2025-07-17 09:00:12,234 - INFO - Current learning rate: 0.0009454606676133598\n",
      "2025-07-17 09:00:14,384 - INFO - Epoch: 3/5, Iter: 3/119 -- train_loss: 1.0024\n",
      "Epoch [3/5]: [2/119]   2%|▏         , loss=0.981 [00:02<00:15]\n",
      "Epoch [3/5]: [2/119]   2%|▏         , loss=1 [00:02<00:15]\n",
      "Epoch [3/5]: [3/119]   3%|▎         , loss=1 [00:02<02:33]\n",
      "2025-07-17 09:00:14,388 - INFO - Current learning rate: 0.0009437351963343782\n",
      "2025-07-17 09:00:14,562 - INFO - Epoch: 3/5, Iter: 4/119 -- train_loss: 1.0027\n",
      "Epoch [3/5]: [3/119]   3%|▎         , loss=1 [00:02<02:33]\n",
      "Epoch [3/5]: [3/119]   3%|▎         , loss=1 [00:02<02:33]\n",
      "Epoch [3/5]: [4/119]   3%|▎         , loss=1 [00:02<01:31]\n",
      "2025-07-17 09:00:14,564 - INFO - Current learning rate: 0.000941984479228796\n",
      "2025-07-17 09:00:18,746 - INFO - Epoch: 3/5, Iter: 5/119 -- train_loss: 1.0010\n",
      "Epoch [3/5]: [4/119]   3%|▎         , loss=1 [00:06<01:31]\n",
      "Epoch [3/5]: [4/119]   3%|▎         , loss=1 [00:06<01:31]\n",
      "Epoch [3/5]: [5/119]   4%|▍         , loss=1 [00:06<04:03]\n",
      "2025-07-17 09:00:18,748 - INFO - Current learning rate: 0.0009402086159021886\n",
      "2025-07-17 09:00:19,013 - INFO - Epoch: 3/5, Iter: 6/119 -- train_loss: 1.0009\n",
      "Epoch [3/5]: [5/119]   4%|▍         , loss=1 [00:06<04:03]\n",
      "Epoch [3/5]: [5/119]   4%|▍         , loss=1 [00:06<04:03]\n",
      "Epoch [3/5]: [6/119]   5%|▌         , loss=1 [00:06<02:45]\n",
      "2025-07-17 09:00:19,016 - INFO - Current learning rate: 0.0009384077073908053\n",
      "2025-07-17 09:00:23,761 - INFO - Epoch: 3/5, Iter: 7/119 -- train_loss: 0.9360\n",
      "Epoch [3/5]: [6/119]   5%|▌         , loss=1 [00:11<02:45]\n",
      "Epoch [3/5]: [6/119]   5%|▌         , loss=0.936 [00:11<02:45]\n",
      "Epoch [3/5]: [7/119]   6%|▌         , loss=0.936 [00:11<04:48]\n",
      "2025-07-17 09:00:23,763 - INFO - Current learning rate: 0.0009365818561558189\n",
      "2025-07-17 09:00:23,892 - INFO - Epoch: 3/5, Iter: 8/119 -- train_loss: 1.0012\n",
      "Epoch [3/5]: [7/119]   6%|▌         , loss=0.936 [00:11<04:48]\n",
      "Epoch [3/5]: [7/119]   6%|▌         , loss=1 [00:11<04:48]\n",
      "Epoch [3/5]: [8/119]   7%|▋         , loss=1 [00:11<03:17]\n",
      "2025-07-17 09:00:23,893 - INFO - Current learning rate: 0.0009347311660774986\n",
      "2025-07-17 09:00:26,832 - INFO - Epoch: 3/5, Iter: 9/119 -- train_loss: 1.0020\n",
      "Epoch [3/5]: [8/119]   7%|▋         , loss=1 [00:14<03:17]\n",
      "Epoch [3/5]: [8/119]   7%|▋         , loss=1 [00:14<03:17]\n",
      "Epoch [3/5]: [9/119]   8%|▊         , loss=1 [00:14<03:56]\n",
      "2025-07-17 09:00:26,835 - INFO - Current learning rate: 0.000932855742449298\n",
      "2025-07-17 09:00:28,379 - INFO - Epoch: 3/5, Iter: 10/119 -- train_loss: 0.9980\n",
      "Epoch [3/5]: [9/119]   8%|▊         , loss=1 [00:16<03:56]\n",
      "Epoch [3/5]: [9/119]   8%|▊         , loss=0.998 [00:16<03:56]\n",
      "Epoch [3/5]: [10/119]   8%|▊         , loss=0.998 [00:16<03:33]\n",
      "2025-07-17 09:00:28,382 - INFO - Current learning rate: 0.0009309556919718655\n",
      "2025-07-17 09:00:30,270 - INFO - Epoch: 3/5, Iter: 11/119 -- train_loss: 0.8079\n",
      "Epoch [3/5]: [10/119]   8%|▊         , loss=0.998 [00:18<03:33]\n",
      "Epoch [3/5]: [10/119]   8%|▊         , loss=0.808 [00:18<03:33]\n",
      "Epoch [3/5]: [11/119]   9%|▉         , loss=0.808 [00:18<03:29]\n",
      "2025-07-17 09:00:30,272 - INFO - Current learning rate: 0.0009290311227469733\n",
      "2025-07-17 09:00:30,840 - INFO - Epoch: 3/5, Iter: 12/119 -- train_loss: 1.0006\n",
      "Epoch [3/5]: [11/119]   9%|▉         , loss=0.808 [00:18<03:29]\n",
      "Epoch [3/5]: [11/119]   9%|▉         , loss=1 [00:18<03:29]\n",
      "Epoch [3/5]: [12/119]  10%|█         , loss=1 [00:18<02:42]\n",
      "2025-07-17 09:00:30,842 - INFO - Current learning rate: 0.0009270821442713667\n",
      "2025-07-17 09:00:39,799 - INFO - Epoch: 3/5, Iter: 13/119 -- train_loss: 1.0011\n",
      "Epoch [3/5]: [12/119]  10%|█         , loss=1 [00:27<02:42]\n",
      "Epoch [3/5]: [12/119]  10%|█         , loss=1 [00:27<02:42]\n",
      "Epoch [3/5]: [13/119]  11%|█         , loss=1 [00:27<06:41]\n",
      "2025-07-17 09:00:39,803 - INFO - Current learning rate: 0.0009251088674305356\n",
      "2025-07-17 09:00:39,924 - INFO - Epoch: 3/5, Iter: 14/119 -- train_loss: 0.6119\n",
      "Epoch [3/5]: [13/119]  11%|█         , loss=1 [00:27<06:41]\n",
      "Epoch [3/5]: [13/119]  11%|█         , loss=0.612 [00:27<06:41]\n",
      "Epoch [3/5]: [14/119]  12%|█▏        , loss=0.612 [00:27<04:40]\n",
      "2025-07-17 09:00:39,926 - INFO - Current learning rate: 0.000923111404492404\n",
      "2025-07-17 09:00:41,669 - INFO - Epoch: 3/5, Iter: 15/119 -- train_loss: 0.9936\n",
      "Epoch [3/5]: [14/119]  12%|█▏        , loss=0.612 [00:29<04:40]\n",
      "Epoch [3/5]: [14/119]  12%|█▏        , loss=0.994 [00:29<04:40]\n",
      "Epoch [3/5]: [15/119]  13%|█▎        , loss=0.994 [00:29<04:09]\n",
      "2025-07-17 09:00:41,672 - INFO - Current learning rate: 0.0009210898691009443\n",
      "2025-07-17 09:00:42,288 - INFO - Epoch: 3/5, Iter: 16/119 -- train_loss: 0.8784\n",
      "Epoch [3/5]: [15/119]  13%|█▎        , loss=0.994 [00:30<04:09]\n",
      "Epoch [3/5]: [15/119]  13%|█▎        , loss=0.878 [00:30<04:09]\n",
      "Epoch [3/5]: [16/119]  13%|█▎        , loss=0.878 [00:30<03:11]\n",
      "2025-07-17 09:00:42,290 - INFO - Current learning rate: 0.0009190443762697104\n",
      "2025-07-17 09:00:46,385 - INFO - Epoch: 3/5, Iter: 17/119 -- train_loss: 0.9362\n",
      "Epoch [3/5]: [16/119]  13%|█▎        , loss=0.878 [00:34<03:11]\n",
      "Epoch [3/5]: [16/119]  13%|█▎        , loss=0.936 [00:34<03:11]\n",
      "Epoch [3/5]: [17/119]  14%|█▍        , loss=0.936 [00:34<04:18]\n",
      "2025-07-17 09:00:46,387 - INFO - Current learning rate: 0.0009169750423752949\n",
      "2025-07-17 09:00:46,788 - INFO - Epoch: 3/5, Iter: 18/119 -- train_loss: 1.0034\n",
      "Epoch [3/5]: [17/119]  14%|█▍        , loss=0.936 [00:34<04:18]\n",
      "Epoch [3/5]: [17/119]  14%|█▍        , loss=1 [00:34<04:18]\n",
      "Epoch [3/5]: [18/119]  15%|█▌        , loss=1 [00:34<03:11]\n",
      "2025-07-17 09:00:46,790 - INFO - Current learning rate: 0.0009148819851507071\n",
      "2025-07-17 09:00:49,730 - INFO - Epoch: 3/5, Iter: 19/119 -- train_loss: 1.0017\n",
      "Epoch [3/5]: [18/119]  15%|█▌        , loss=1 [00:37<03:11]\n",
      "Epoch [3/5]: [18/119]  15%|█▌        , loss=1 [00:37<03:11]#015Epoch [3/5]: [19/119]  16%|█▌        , loss=1 [00:37<03:40]\n",
      "2025-07-17 09:00:49,732 - INFO - Current learning rate: 0.0009127653236786758\n",
      "2025-07-17 09:00:49,837 - INFO - Epoch: 3/5, Iter: 20/119 -- train_loss: 1.0024\n",
      "Epoch [3/5]: [19/119]  16%|█▌        , loss=1 [00:37<03:40]\n",
      "Epoch [3/5]: [19/119]  16%|█▌        , loss=1 [00:37<03:40]\n",
      "Epoch [3/5]: [20/119]  17%|█▋        , loss=1 [00:37<02:36]\n",
      "2025-07-17 09:00:49,839 - INFO - Current learning rate: 0.0009106251783848731\n",
      "2025-07-17 09:00:55,234 - INFO - Epoch: 3/5, Iter: 21/119 -- train_loss: 1.0020\n",
      "Epoch [3/5]: [20/119]  17%|█▋        , loss=1 [00:43<02:36]\n",
      "Epoch [3/5]: [20/119]  17%|█▋        , loss=1 [00:43<02:36]\n",
      "Epoch [3/5]: [21/119]  18%|█▊        , loss=1 [00:43<04:26]\n",
      "2025-07-17 09:00:55,236 - INFO - Current learning rate: 0.0009084616710310638\n",
      "2025-07-17 09:00:59,817 - INFO - Epoch: 3/5, Iter: 22/119 -- train_loss: 0.9181\n",
      "Epoch [3/5]: [21/119]  18%|█▊        , loss=1 [00:47<04:26]\n",
      "Epoch [3/5]: [21/119]  18%|█▊        , loss=0.918 [00:47<04:26]\n",
      "Epoch [3/5]: [22/119]  18%|█▊        , loss=0.918 [00:47<05:18]\n",
      "2025-07-17 09:00:59,819 - INFO - Current learning rate: 0.0009062749247081771\n",
      "2025-07-17 09:00:59,962 - INFO - Epoch: 3/5, Iter: 23/119 -- train_loss: 0.6006\n",
      "Epoch [3/5]: [22/119]  18%|█▊        , loss=0.918 [00:47<05:18]\n",
      "Epoch [3/5]: [22/119]  18%|█▊        , loss=0.601 [00:47<05:18]\n",
      "Epoch [3/5]: [23/119]  19%|█▉        , loss=0.601 [00:47<03:44]\n",
      "2025-07-17 09:00:59,964 - INFO - Current learning rate: 0.0009040650638293038\n",
      "2025-07-17 09:01:00,108 - INFO - Epoch: 3/5, Iter: 24/119 -- train_loss: 1.0030\n",
      "Epoch [3/5]: [23/119]  19%|█▉        , loss=0.601 [00:48<03:44]\n",
      "Epoch [3/5]: [23/119]  19%|█▉        , loss=1 [00:48<03:44]\n",
      "Epoch [3/5]: [24/119]  20%|██        , loss=1 [00:48<02:39]\n",
      "2025-07-17 09:01:00,109 - INFO - Current learning rate: 0.0009018322141226183\n",
      "2025-07-17 09:01:09,688 - INFO - Epoch: 3/5, Iter: 25/119 -- train_loss: 0.9564\n",
      "Epoch [3/5]: [24/119]  20%|██        , loss=1 [00:57<02:39]\n",
      "Epoch [3/5]: [24/119]  20%|██        , loss=0.956 [00:57<02:39]\n",
      "Epoch [3/5]: [25/119]  21%|██        , loss=0.956 [00:57<06:20]\n",
      "2025-07-17 09:01:09,690 - INFO - Current learning rate: 0.0008995765026242245\n",
      "2025-07-17 09:01:11,671 - INFO - Epoch: 3/5, Iter: 26/119 -- train_loss: 0.8758\n",
      "Epoch [3/5]: [25/119]  21%|██        , loss=0.956 [00:59<06:20]\n",
      "Epoch [3/5]: [25/119]  21%|██        , loss=0.876 [00:59<06:20]\n",
      "Epoch [3/5]: [26/119]  22%|██▏       , loss=0.876 [00:59<05:19]\n",
      "2025-07-17 09:01:11,672 - INFO - Current learning rate: 0.0008972980576709286\n",
      "2025-07-17 09:01:11,818 - INFO - Epoch: 3/5, Iter: 27/119 -- train_loss: 1.0020\n",
      "Epoch [3/5]: [26/119]  22%|██▏       , loss=0.876 [00:59<05:19]\n",
      "Epoch [3/5]: [26/119]  22%|██▏       , loss=1 [00:59<05:19]\n",
      "Epoch [3/5]: [27/119]  23%|██▎       , loss=1 [00:59<03:45]\n",
      "2025-07-17 09:01:11,819 - INFO - Current learning rate: 0.0008949970088929384\n",
      "2025-07-17 09:01:12,003 - INFO - Epoch: 3/5, Iter: 28/119 -- train_loss: 1.0012\n",
      "Epoch [3/5]: [27/119]  23%|██▎       , loss=1 [00:59<03:45]\n",
      "Epoch [3/5]: [27/119]  23%|██▎       , loss=1 [00:59<03:45]\n",
      "Epoch [3/5]: [28/119]  24%|██▎       , loss=1 [00:59<02:40]\n",
      "2025-07-17 09:01:12,005 - INFO - Current learning rate: 0.0008926734872064864\n",
      "2025-07-17 09:01:20,288 - INFO - Epoch: 3/5, Iter: 29/119 -- train_loss: 1.0025\n",
      "Epoch [3/5]: [28/119]  24%|██▎       , loss=1 [01:08<02:40]\n",
      "Epoch [3/5]: [28/119]  24%|██▎       , loss=1 [01:08<02:40]\n",
      "Epoch [3/5]: [29/119]  24%|██▍       , loss=1 [01:08<05:35]\n",
      "2025-07-17 09:01:20,291 - INFO - Current learning rate: 0.0008903276248063826\n",
      "2025-07-17 09:01:20,577 - INFO - Epoch: 3/5, Iter: 30/119 -- train_loss: 0.9715\n",
      "Epoch [3/5]: [29/119]  24%|██▍       , loss=1 [01:08<05:35]\n",
      "Epoch [3/5]: [29/119]  24%|██▍       , loss=0.972 [01:08<05:35]\n",
      "Epoch [3/5]: [30/119]  25%|██▌       , loss=0.972 [01:08<03:59]\n",
      "2025-07-17 09:01:20,579 - INFO - Current learning rate: 0.0008879595551584934\n",
      "2025-07-17 09:01:20,710 - INFO - Epoch: 3/5, Iter: 31/119 -- train_loss: 0.8308\n",
      "Epoch [3/5]: [30/119]  25%|██▌       , loss=0.972 [01:08<03:59]\n",
      "Epoch [3/5]: [30/119]  25%|██▌       , loss=0.831 [01:08<03:59]\n",
      "Epoch [3/5]: [31/119]  26%|██▌       , loss=0.831 [01:08<02:49]\n",
      "2025-07-17 09:01:20,712 - INFO - Current learning rate: 0.0008855694129921473\n",
      "2025-07-17 09:01:20,846 - INFO - Epoch: 3/5, Iter: 32/119 -- train_loss: 1.0010\n",
      "Epoch [3/5]: [31/119]  26%|██▌       , loss=0.831 [01:08<02:49]\n",
      "Epoch [3/5]: [31/119]  26%|██▌       , loss=1 [01:08<02:49]\n",
      "Epoch [3/5]: [32/119]  27%|██▋       , loss=1 [01:08<02:00]\n",
      "2025-07-17 09:01:20,848 - INFO - Current learning rate: 0.0008831573342924707\n",
      "2025-07-17 09:01:32,204 - INFO - Epoch: 3/5, Iter: 33/119 -- train_loss: 1.0006\n",
      "Epoch [3/5]: [32/119]  27%|██▋       , loss=1 [01:20<02:00]\n",
      "Epoch [3/5]: [32/119]  27%|██▋       , loss=1 [01:20<02:00]#015Epoch [3/5]: [33/119]  28%|██▊       , loss=1 [01:20<06:16]\n",
      "2025-07-17 09:01:32,206 - INFO - Current learning rate: 0.0008807234562926504\n",
      "2025-07-17 09:01:32,327 - INFO - Epoch: 3/5, Iter: 34/119 -- train_loss: 1.0009\n",
      "Epoch [3/5]: [33/119]  28%|██▊       , loss=1 [01:20<06:16]\n",
      "Epoch [3/5]: [33/119]  28%|██▊       , loss=1 [01:20<06:16]\n",
      "Epoch [3/5]: [34/119]  29%|██▊       , loss=1 [01:20<04:23]\n",
      "2025-07-17 09:01:32,328 - INFO - Current learning rate: 0.0008782679174661258\n",
      "2025-07-17 09:01:32,458 - INFO - Epoch: 3/5, Iter: 35/119 -- train_loss: 0.9215\n",
      "Epoch [3/5]: [34/119]  29%|██▊       , loss=1 [01:20<04:23]\n",
      "Epoch [3/5]: [34/119]  29%|██▊       , loss=0.922 [01:20<04:23]\n",
      "Epoch [3/5]: [35/119]  29%|██▉       , loss=0.922 [01:20<03:05]\n",
      "2025-07-17 09:01:32,460 - INFO - Current learning rate: 0.0008757908575187108\n",
      "2025-07-17 09:01:32,563 - INFO - Epoch: 3/5, Iter: 36/119 -- train_loss: 0.9349\n",
      "Epoch [3/5]: [35/119]  29%|██▉       , loss=0.922 [01:20<03:05]\n",
      "Epoch [3/5]: [35/119]  29%|██▉       , loss=0.935 [01:20<03:05]\n",
      "Epoch [3/5]: [36/119]  30%|███       , loss=0.935 [01:20<02:11]\n",
      "2025-07-17 09:01:32,564 - INFO - Current learning rate: 0.0008732924173806459\n",
      "2025-07-17 09:01:42,404 - INFO - Epoch: 3/5, Iter: 37/119 -- train_loss: 1.0009\n",
      "Epoch [3/5]: [36/119]  30%|███       , loss=0.935 [01:30<02:11]\n",
      "Epoch [3/5]: [36/119]  30%|███       , loss=1 [01:30<02:11]\n",
      "Epoch [3/5]: [37/119]  31%|███       , loss=1 [01:30<05:32]2025-07-17 09:01:42,405 - INFO - Current learning rate: 0.000870772739198579\n",
      "2025-07-17 09:01:42,508 - INFO - Epoch: 3/5, Iter: 38/119 -- train_loss: 0.7967\n",
      "Epoch [3/5]: [37/119]  31%|███       , loss=1 [01:30<05:32]\n",
      "Epoch [3/5]: [37/119]  31%|███       , loss=0.797 [01:30<05:32]\n",
      "Epoch [3/5]: [38/119]  32%|███▏      , loss=0.797 [01:30<03:52]\n",
      "2025-07-17 09:01:42,510 - INFO - Current learning rate: 0.0008682319663274788\n",
      "2025-07-17 09:01:42,613 - INFO - Epoch: 3/5, Iter: 39/119 -- train_loss: 1.0008\n",
      "Epoch [3/5]: [38/119]  32%|███▏      , loss=0.797 [01:30<03:52]\n",
      "Epoch [3/5]: [38/119]  32%|███▏      , loss=1 [01:30<03:52]\n",
      "Epoch [3/5]: [39/119]  33%|███▎      , loss=1 [01:30<02:43]\n",
      "2025-07-17 09:01:42,614 - INFO - Current learning rate: 0.0008656702433224786\n",
      "2025-07-17 09:01:42,744 - INFO - Epoch: 3/5, Iter: 40/119 -- train_loss: 1.0007\n",
      "Epoch [3/5]: [39/119]  33%|███▎      , loss=1 [01:30<02:43]\n",
      "Epoch [3/5]: [39/119]  33%|███▎      , loss=1 [01:30<02:43]\n",
      "Epoch [3/5]: [40/119]  34%|███▎      , loss=1 [01:30<01:56]\n",
      "2025-07-17 09:01:42,747 - INFO - Current learning rate: 0.0008630877159306519\n",
      "2025-07-17 09:01:51,136 - INFO - Epoch: 3/5, Iter: 41/119 -- train_loss: 0.9836\n",
      "Epoch [3/5]: [40/119]  34%|███▎      , loss=1 [01:39<01:56]\n",
      "Epoch [3/5]: [40/119]  34%|███▎      , loss=0.984 [01:39<01:56]#015Epoch [3/5]: [41/119]  34%|███▍      , loss=0.984 [01:39<04:36]\n",
      "2025-07-17 09:01:51,138 - INFO - Current learning rate: 0.0008604845310827203\n",
      "2025-07-17 09:02:02,773 - INFO - Epoch: 3/5, Iter: 42/119 -- train_loss: 0.8236\n",
      "Epoch [3/5]: [41/119]  34%|███▍      , loss=0.984 [01:50<04:36]\n",
      "Epoch [3/5]: [41/119]  34%|███▍      , loss=0.824 [01:50<04:36]\n",
      "Epoch [3/5]: [42/119]  35%|███▌      , loss=0.824 [01:50<07:39]\n",
      "2025-07-17 09:02:02,774 - INFO - Current learning rate: 0.0008578608368846938\n",
      "2025-07-17 09:02:02,904 - INFO - Epoch: 3/5, Iter: 43/119 -- train_loss: 1.0021\n",
      "Epoch [3/5]: [42/119]  35%|███▌      , loss=0.824 [01:50<07:39]\n",
      "Epoch [3/5]: [42/119]  35%|███▌      , loss=1 [01:50<07:39]\n",
      "Epoch [3/5]: [43/119]  36%|███▌      , loss=1 [01:50<05:20]\n",
      "2025-07-17 09:02:02,906 - INFO - Current learning rate: 0.0008552167826094451\n",
      "2025-07-17 09:02:03,057 - INFO - Epoch: 3/5, Iter: 44/119 -- train_loss: 0.9578\n",
      "Epoch [3/5]: [43/119]  36%|███▌      , loss=1 [01:50<05:20]\n",
      "Epoch [3/5]: [43/119]  36%|███▌      , loss=0.958 [01:50<05:20]\n",
      "Epoch [3/5]: [44/119]  37%|███▋      , loss=0.958 [01:50<03:45]\n",
      "2025-07-17 09:02:03,060 - INFO - Current learning rate: 0.0008525525186882157\n",
      "2025-07-17 09:02:03,240 - INFO - Epoch: 3/5, Iter: 45/119 -- train_loss: 0.8684\n",
      "Epoch [3/5]: [44/119]  37%|███▋      , loss=0.958 [01:51<03:45]\n",
      "Epoch [3/5]: [44/119]  37%|███▋      , loss=0.868 [01:51<03:45]\n",
      "Epoch [3/5]: [45/119]  38%|███▊      , loss=0.868 [01:51<02:39]\n",
      "2025-07-17 09:02:03,243 - INFO - Current learning rate: 0.0008498681967020581\n",
      "2025-07-17 09:02:08,960 - INFO - Epoch: 3/5, Iter: 46/119 -- train_loss: 1.0080\n",
      "Epoch [3/5]: [45/119]  38%|███▊      , loss=0.868 [01:56<02:39]\n",
      "Epoch [3/5]: [45/119]  38%|███▊      , loss=1.01 [01:56<02:39]\n",
      "Epoch [3/5]: [46/119]  39%|███▊      , loss=1.01 [01:56<03:55]\n",
      "2025-07-17 09:02:08,961 - INFO - Current learning rate: 0.0008471639693732121\n",
      "2025-07-17 09:02:09,068 - INFO - Epoch: 3/5, Iter: 47/119 -- train_loss: 1.0008\n",
      "Epoch [3/5]: [46/119]  39%|███▊      , loss=1.01 [01:56<03:55]\n",
      "Epoch [3/5]: [46/119]  39%|███▊      , loss=1 [01:56<03:55]\n",
      "Epoch [3/5]: [47/119]  39%|███▉      , loss=1 [01:56<02:44]\n",
      "2025-07-17 09:02:09,070 - INFO - Current learning rate: 0.0008444399905564142\n",
      "2025-07-17 09:02:09,181 - INFO - Epoch: 3/5, Iter: 48/119 -- train_loss: 0.9845\n",
      "Epoch [3/5]: [47/119]  39%|███▉      , loss=1 [01:57<02:44]\n",
      "Epoch [3/5]: [47/119]  39%|███▉      , loss=0.984 [01:57<02:44]\n",
      "Epoch [3/5]: [48/119]  40%|████      , loss=0.984 [01:57<01:56]\n",
      "2025-07-17 09:02:09,184 - INFO - Current learning rate: 0.0008416964152301461\n",
      "2025-07-17 09:02:09,323 - INFO - Epoch: 3/5, Iter: 49/119 -- train_loss: 1.0007\n",
      "Epoch [3/5]: [48/119]  40%|████      , loss=0.984 [01:57<01:56]\n",
      "Epoch [3/5]: [48/119]  40%|████      , loss=1 [01:57<01:56]\n",
      "Epoch [3/5]: [49/119]  41%|████      , loss=1 [01:57<01:23]\n",
      "2025-07-17 09:02:09,324 - INFO - Current learning rate: 0.0008389333994878158\n",
      "2025-07-17 09:02:14,607 - INFO - Epoch: 3/5, Iter: 50/119 -- train_loss: 0.9816\n",
      "Epoch [3/5]: [49/119]  41%|████      , loss=1 [02:02<01:23]\n",
      "Epoch [3/5]: [49/119]  41%|████      , loss=0.982 [02:02<01:23]\n",
      "Epoch [3/5]: [50/119]  42%|████▏     , loss=0.982 [02:02<02:46]\n",
      "2025-07-17 09:02:14,610 - INFO - Current learning rate: 0.0008361511005288778\n",
      "2025-07-17 09:02:14,749 - INFO - Epoch: 3/5, Iter: 51/119 -- train_loss: 1.0074\n",
      "Epoch [3/5]: [50/119]  42%|████▏     , loss=0.982 [02:02<02:46]\n",
      "Epoch [3/5]: [50/119]  42%|████▏     , loss=1.01 [02:02<02:46]\n",
      "Epoch [3/5]: [51/119]  43%|████▎     , loss=1.01 [02:02<01:57]\n",
      "2025-07-17 09:02:14,751 - INFO - Current learning rate: 0.0008333496766498886\n",
      "2025-07-17 09:02:14,876 - INFO - Epoch: 3/5, Iter: 52/119 -- train_loss: 1.0015\n",
      "Epoch [3/5]: [51/119]  43%|████▎     , loss=1.01 [02:02<01:57]\n",
      "Epoch [3/5]: [51/119]  43%|████▎     , loss=1 [02:02<01:57]\n",
      "Epoch [3/5]: [52/119]  44%|████▎     , loss=1 [02:02<01:23]\n",
      "2025-07-17 09:02:14,878 - INFO - Current learning rate: 0.0008305292872355011\n",
      "2025-07-17 09:02:19,410 - INFO - Epoch: 3/5, Iter: 53/119 -- train_loss: 0.9789\n",
      "Epoch [3/5]: [52/119]  44%|████▎     , loss=1 [02:07<01:23]\n",
      "Epoch [3/5]: [52/119]  44%|████▎     , loss=0.979 [02:07<01:23]\n",
      "Epoch [3/5]: [53/119]  45%|████▍     , loss=0.979 [02:07<02:27]\n",
      "2025-07-17 09:02:19,412 - INFO - Current learning rate: 0.0008276900927493964\n",
      "2025-07-17 09:02:20,975 - INFO - Epoch: 3/5, Iter: 54/119 -- train_loss: 0.8727\n",
      "Epoch [3/5]: [53/119]  45%|████▍     , loss=0.979 [02:08<02:27]\n",
      "Epoch [3/5]: [53/119]  45%|████▍     , loss=0.873 [02:08<02:27]\n",
      "Epoch [3/5]: [54/119]  45%|████▌     , loss=0.873 [02:08<02:12]\n",
      "2025-07-17 09:02:20,977 - INFO - Current learning rate: 0.0008248322547251544\n",
      "2025-07-17 09:02:21,102 - INFO - Epoch: 3/5, Iter: 55/119 -- train_loss: 0.8943\n",
      "Epoch [3/5]: [54/119]  45%|████▌     , loss=0.873 [02:09<02:12]\n",
      "Epoch [3/5]: [54/119]  45%|████▌     , loss=0.894 [02:09<02:12]\n",
      "Epoch [3/5]: [55/119]  46%|████▌     , loss=0.894 [02:09<01:33]\n",
      "2025-07-17 09:02:21,103 - INFO - Current learning rate: 0.0008219559357570634\n",
      "2025-07-17 09:02:23,184 - INFO - Epoch: 3/5, Iter: 56/119 -- train_loss: 0.7695\n",
      "Epoch [3/5]: [55/119]  46%|████▌     , loss=0.894 [02:11<01:33]\n",
      "Epoch [3/5]: [55/119]  46%|████▌     , loss=0.77 [02:11<01:33]\n",
      "Epoch [3/5]: [56/119]  47%|████▋     , loss=0.77 [02:11<01:43]\n",
      "2025-07-17 09:02:23,185 - INFO - Current learning rate: 0.0008190612994908691\n",
      "2025-07-17 09:02:27,716 - INFO - Epoch: 3/5, Iter: 57/119 -- train_loss: 1.0011\n",
      "Epoch [3/5]: [56/119]  47%|████▋     , loss=0.77 [02:15<01:43]\n",
      "Epoch [3/5]: [56/119]  47%|████▋     , loss=1 [02:15<01:43]\n",
      "Epoch [3/5]: [57/119]  48%|████▊     , loss=1 [02:15<02:35]\n",
      "2025-07-17 09:02:27,719 - INFO - Current learning rate: 0.000816148510614465\n",
      "2025-07-17 09:02:30,022 - INFO - Epoch: 3/5, Iter: 58/119 -- train_loss: 0.9241\n",
      "Epoch [3/5]: [57/119]  48%|████▊     , loss=1 [02:17<02:35]\n",
      "Epoch [3/5]: [57/119]  48%|████▊     , loss=0.924 [02:17<02:35]\n",
      "Epoch [3/5]: [58/119]  49%|████▊     , loss=0.924 [02:17<02:29]\n",
      "2025-07-17 09:02:30,025 - INFO - Current learning rate: 0.0008132177348485214\n",
      "2025-07-17 09:02:30,180 - INFO - Epoch: 3/5, Iter: 59/119 -- train_loss: 0.9436\n",
      "Epoch [3/5]: [58/119]  49%|████▊     , loss=0.924 [02:18<02:29]\n",
      "Epoch [3/5]: [58/119]  49%|████▊     , loss=0.944 [02:18<02:29]\n",
      "Epoch [3/5]: [59/119]  50%|████▉     , loss=0.944 [02:18<01:45]\n",
      "2025-07-17 09:02:30,183 - INFO - Current learning rate: 0.0008102691389370582\n",
      "2025-07-17 09:02:35,069 - INFO - Epoch: 3/5, Iter: 60/119 -- train_loss: 1.0008\n",
      "Epoch [3/5]: [59/119]  50%|████▉     , loss=0.944 [02:22<01:45]\n",
      "Epoch [3/5]: [59/119]  50%|████▉     , loss=1 [02:22<01:45]\n",
      "Epoch [3/5]: [60/119]  50%|█████     , loss=1 [02:22<02:39]\n",
      "2025-07-17 09:02:35,071 - INFO - Current learning rate: 0.000807302890637957\n",
      "2025-07-17 09:02:36,172 - INFO - Epoch: 3/5, Iter: 61/119 -- train_loss: 0.8480\n",
      "Epoch [3/5]: [60/119]  50%|█████     , loss=1 [02:24<02:39]\n",
      "Epoch [3/5]: [60/119]  50%|█████     , loss=0.848 [02:24<02:39]\n",
      "Epoch [3/5]: [61/119]  51%|█████▏    , loss=0.848 [02:24<02:08]\n",
      "2025-07-17 09:02:36,174 - INFO - Current learning rate: 0.0008043191587134176\n",
      "2025-07-17 09:02:40,743 - INFO - Epoch: 3/5, Iter: 62/119 -- train_loss: 1.0011\n",
      "Epoch [3/5]: [61/119]  51%|█████▏    , loss=0.848 [02:28<02:08]\n",
      "Epoch [3/5]: [61/119]  51%|█████▏    , loss=1 [02:28<02:08]\n",
      "Epoch [3/5]: [62/119]  52%|█████▏    , loss=1 [02:28<02:46]\n",
      "2025-07-17 09:02:40,745 - INFO - Current learning rate: 0.0008013181129203554\n",
      "2025-07-17 09:02:40,861 - INFO - Epoch: 3/5, Iter: 63/119 -- train_loss: 0.8129\n",
      "Epoch [3/5]: [62/119]  52%|█████▏    , loss=1 [02:28<02:46]\n",
      "Epoch [3/5]: [62/119]  52%|█████▏    , loss=0.813 [02:28<02:46]\n",
      "Epoch [3/5]: [63/119]  53%|█████▎    , loss=0.813 [02:28<01:56]\n",
      "2025-07-17 09:02:40,862 - INFO - Current learning rate: 0.000798299924000744\n",
      "2025-07-17 09:02:42,321 - INFO - Epoch: 3/5, Iter: 64/119 -- train_loss: 1.0033\n",
      "Epoch [3/5]: [63/119]  53%|█████▎    , loss=0.813 [02:30<01:56]\n",
      "Epoch [3/5]: [63/119]  53%|█████▎    , loss=1 [02:30<01:56]\n",
      "Epoch [3/5]: [64/119]  54%|█████▍    , loss=1 [02:30<01:44]\n",
      "2025-07-17 09:02:42,324 - INFO - Current learning rate: 0.0007952647636719012\n",
      "2025-07-17 09:02:48,866 - INFO - Epoch: 3/5, Iter: 65/119 -- train_loss: 1.0046\n",
      "Epoch [3/5]: [64/119]  54%|█████▍    , loss=1 [02:36<01:44]\n",
      "Epoch [3/5]: [64/119]  54%|█████▍    , loss=1 [02:36<01:44]\n",
      "Epoch [3/5]: [65/119]  55%|█████▍    , loss=1 [02:36<02:57]\n",
      "2025-07-17 09:02:48,869 - INFO - Current learning rate: 0.000792212804616718\n",
      "2025-07-17 09:02:51,690 - INFO - Epoch: 3/5, Iter: 66/119 -- train_loss: 0.7743\n",
      "Epoch [3/5]: [65/119]  55%|█████▍    , loss=1 [02:39<02:57]\n",
      "Epoch [3/5]: [65/119]  55%|█████▍    , loss=0.774 [02:39<02:57]\n",
      "Epoch [3/5]: [66/119]  55%|█████▌    , loss=0.774 [02:39<02:47]\n",
      "2025-07-17 09:02:51,693 - INFO - Current learning rate: 0.0007891442204738353\n",
      "2025-07-17 09:02:51,862 - INFO - Epoch: 3/5, Iter: 67/119 -- train_loss: 0.8590\n",
      "Epoch [3/5]: [66/119]  55%|█████▌    , loss=0.774 [02:39<02:47]\n",
      "Epoch [3/5]: [66/119]  55%|█████▌    , loss=0.859 [02:39<02:47]\n",
      "Epoch [3/5]: [67/119]  56%|█████▋    , loss=0.859 [02:39<01:57]\n",
      "2025-07-17 09:02:51,864 - INFO - Current learning rate: 0.0007860591858277645\n",
      "2025-07-17 09:02:52,009 - INFO - Epoch: 3/5, Iter: 68/119 -- train_loss: 1.0015\n",
      "Epoch [3/5]: [67/119]  56%|█████▋    , loss=0.859 [02:39<01:57]\n",
      "Epoch [3/5]: [67/119]  56%|█████▋    , loss=1 [02:39<01:57]\n",
      "Epoch [3/5]: [68/119]  57%|█████▋    , loss=1 [02:39<01:22]\n",
      "2025-07-17 09:02:52,010 - INFO - Current learning rate: 0.0007829578761989541\n",
      "2025-07-17 09:02:58,591 - INFO - Epoch: 3/5, Iter: 69/119 -- train_loss: 1.0015\n",
      "Epoch [3/5]: [68/119]  57%|█████▋    , loss=1 [02:46<01:22]\n",
      "Epoch [3/5]: [68/119]  57%|█████▋    , loss=1 [02:46<01:22]\n",
      "Epoch [3/5]: [69/119]  58%|█████▊    , loss=1 [02:46<02:35]\n",
      "2025-07-17 09:02:58,592 - INFO - Current learning rate: 0.0007798404680338045\n",
      "2025-07-17 09:03:00,917 - INFO - Epoch: 3/5, Iter: 70/119 -- train_loss: 0.8864\n",
      "Epoch [3/5]: [69/119]  58%|█████▊    , loss=1 [02:48<02:35]\n",
      "Epoch [3/5]: [69/119]  58%|█████▊    , loss=0.886 [02:48<02:35]\n",
      "Epoch [3/5]: [70/119]  59%|█████▉    , loss=0.886 [02:48<02:21]\n",
      "2025-07-17 09:03:00,929 - INFO - Current learning rate: 0.0007767071386946281\n",
      "2025-07-17 09:03:01,107 - INFO - Epoch: 3/5, Iter: 71/119 -- train_loss: 0.9590\n",
      "Epoch [3/5]: [70/119]  59%|█████▉    , loss=0.886 [02:49<02:21]\n",
      "Epoch [3/5]: [70/119]  59%|█████▉    , loss=0.959 [02:49<02:21]\n",
      "Epoch [3/5]: [71/119]  60%|█████▉    , loss=0.959 [02:49<01:39]\n",
      "2025-07-17 09:03:01,109 - INFO - Current learning rate: 0.0007735580664495601\n",
      "2025-07-17 09:03:01,272 - INFO - Epoch: 3/5, Iter: 72/119 -- train_loss: 0.8157\n",
      "Epoch [3/5]: [71/119]  60%|█████▉    , loss=0.959 [02:49<01:39]\n",
      "Epoch [3/5]: [71/119]  60%|█████▉    , loss=0.816 [02:49<01:39]\n",
      "Epoch [3/5]: [72/119]  61%|██████    , loss=0.816 [02:49<01:10]2025-07-17 09:03:01,274 - INFO - Current learning rate: 0.000770393430462414\n",
      "2025-07-17 09:03:06,849 - INFO - Epoch: 3/5, Iter: 73/119 -- train_loss: 1.0017\n",
      "Epoch [3/5]: [72/119]  61%|██████    , loss=0.816 [02:54<01:10]\n",
      "Epoch [3/5]: [72/119]  61%|██████    , loss=1 [02:54<01:10]\n",
      "Epoch [3/5]: [73/119]  61%|██████▏   , loss=1 [02:54<02:05]\n",
      "2025-07-17 09:03:06,851 - INFO - Current learning rate: 0.00076721341078249\n",
      "2025-07-17 09:03:09,373 - INFO - Epoch: 3/5, Iter: 74/119 -- train_loss: 0.8439\n",
      "Epoch [3/5]: [73/119]  61%|██████▏   , loss=1 [02:57<02:05]\n",
      "Epoch [3/5]: [73/119]  61%|██████▏   , loss=0.844 [02:57<02:05]\n",
      "Epoch [3/5]: [74/119]  62%|██████▏   , loss=0.844 [02:57<01:59]\n",
      "2025-07-17 09:03:09,375 - INFO - Current learning rate: 0.0007640181883343305\n",
      "2025-07-17 09:03:09,504 - INFO - Epoch: 3/5, Iter: 75/119 -- train_loss: 0.6431\n",
      "Epoch [3/5]: [74/119]  62%|██████▏   , loss=0.844 [02:57<01:59]\n",
      "Epoch [3/5]: [74/119]  62%|██████▏   , loss=0.643 [02:57<01:59]\n",
      "Epoch [3/5]: [75/119]  63%|██████▎   , loss=0.643 [02:57<01:23]\n",
      "2025-07-17 09:03:09,505 - INFO - Current learning rate: 0.0007608079449074262\n",
      "2025-07-17 09:03:09,645 - INFO - Epoch: 3/5, Iter: 76/119 -- train_loss: 1.0621\n",
      "Epoch [3/5]: [75/119]  63%|██████▎   , loss=0.643 [02:57<01:23]\n",
      "Epoch [3/5]: [75/119]  63%|██████▎   , loss=1.06 [02:57<01:23]\n",
      "Epoch [3/5]: [76/119]  64%|██████▍   , loss=1.06 [02:57<00:59]\n",
      "2025-07-17 09:03:09,647 - INFO - Current learning rate: 0.000757582863145874\n",
      "2025-07-17 09:03:15,059 - INFO - Epoch: 3/5, Iter: 77/119 -- train_loss: 0.9556\n",
      "Epoch [3/5]: [76/119]  64%|██████▍   , loss=1.06 [03:02<00:59]\n",
      "Epoch [3/5]: [76/119]  64%|██████▍   , loss=0.956 [03:02<00:59]\n",
      "Epoch [3/5]: [77/119]  65%|██████▍   , loss=0.956 [03:02<01:48]\n",
      "2025-07-17 09:03:15,062 - INFO - Current learning rate: 0.0007543431265379855\n",
      "2025-07-17 09:03:21,752 - INFO - Epoch: 3/5, Iter: 78/119 -- train_loss: 1.0022\n",
      "Epoch [3/5]: [77/119]  65%|██████▍   , loss=0.956 [03:09<01:48]\n",
      "Epoch [3/5]: [77/119]  65%|██████▍   , loss=1 [03:09<01:48]    #015Epoch [3/5]: [78/119]  66%|██████▌   , loss=1 [03:09<02:36]\n",
      "2025-07-17 09:03:21,754 - INFO - Current learning rate: 0.0007510889194058472\n",
      "2025-07-17 09:03:21,883 - INFO - Epoch: 3/5, Iter: 79/119 -- train_loss: 1.0016\n",
      "Epoch [3/5]: [78/119]  66%|██████▌   , loss=1 [03:09<02:36]\n",
      "Epoch [3/5]: [78/119]  66%|██████▌   , loss=1 [03:09<02:36]\n",
      "Epoch [3/5]: [79/119]  66%|██████▋   , loss=1 [03:09<01:48]\n",
      "2025-07-17 09:03:21,884 - INFO - Current learning rate: 0.0007478204268948339\n",
      "2025-07-17 09:03:21,995 - INFO - Epoch: 3/5, Iter: 80/119 -- train_loss: 0.8308\n",
      "Epoch [3/5]: [79/119]  66%|██████▋   , loss=1 [03:09<01:48]\n",
      "Epoch [3/5]: [79/119]  66%|██████▋   , loss=0.831 [03:09<01:48]\n",
      "Epoch [3/5]: [80/119]  67%|██████▋   , loss=0.831 [03:09<01:15]\n",
      "2025-07-17 09:03:21,997 - INFO - Current learning rate: 0.0007445378349630751\n",
      "2025-07-17 09:03:25,469 - INFO - Epoch: 3/5, Iter: 81/119 -- train_loss: 1.0011\n",
      "Epoch [3/5]: [80/119]  67%|██████▋   , loss=0.831 [03:13<01:15]\n",
      "Epoch [3/5]: [80/119]  67%|██████▋   , loss=1 [03:13<01:15]\n",
      "Epoch [3/5]: [81/119]  68%|██████▊   , loss=1 [03:13<01:31]\n",
      "2025-07-17 09:03:25,473 - INFO - Current learning rate: 0.0007412413303708751\n",
      "2025-07-17 09:03:30,662 - INFO - Epoch: 3/5, Iter: 82/119 -- train_loss: 1.0010\n",
      "Epoch [3/5]: [81/119]  68%|██████▊   , loss=1 [03:18<01:31]\n",
      "Epoch [3/5]: [81/119]  68%|██████▊   , loss=1 [03:18<01:31]\n",
      "Epoch [3/5]: [82/119]  69%|██████▉   , loss=1 [03:18<01:59]\n",
      "2025-07-17 09:03:30,663 - INFO - Current learning rate: 0.000737931100670087\n",
      "2025-07-17 09:03:30,767 - INFO - Epoch: 3/5, Iter: 83/119 -- train_loss: 1.0014\n",
      "Epoch [3/5]: [82/119]  69%|██████▉   , loss=1 [03:18<01:59]\n",
      "Epoch [3/5]: [82/119]  69%|██████▉   , loss=1 [03:18<01:59]\n",
      "Epoch [3/5]: [83/119]  70%|██████▉   , loss=1 [03:18<01:22]\n",
      "2025-07-17 09:03:30,768 - INFO - Current learning rate: 0.0007346073341934426\n",
      "2025-07-17 09:03:30,872 - INFO - Epoch: 3/5, Iter: 84/119 -- train_loss: 0.8898\n",
      "Epoch [3/5]: [83/119]  70%|██████▉   , loss=1 [03:18<01:22]\n",
      "Epoch [3/5]: [83/119]  70%|██████▉   , loss=0.89 [03:18<01:22]\n",
      "Epoch [3/5]: [84/119]  71%|███████   , loss=0.89 [03:18<00:57]\n",
      "2025-07-17 09:03:30,873 - INFO - Current learning rate: 0.0007312702200438372\n",
      "2025-07-17 09:03:34,014 - INFO - Epoch: 3/5, Iter: 85/119 -- train_loss: 0.9921\n",
      "Epoch [3/5]: [84/119]  71%|███████   , loss=0.89 [03:21<00:57]\n",
      "Epoch [3/5]: [84/119]  71%|███████   , loss=0.992 [03:21<00:57]\n",
      "Epoch [3/5]: [85/119]  71%|███████▏  , loss=0.992 [03:21<01:11]\n",
      "2025-07-17 09:03:34,016 - INFO - Current learning rate: 0.0007279199480835705\n",
      "2025-07-17 09:03:45,591 - INFO - Epoch: 3/5, Iter: 86/119 -- train_loss: 1.0019\n",
      "Epoch [3/5]: [85/119]  71%|███████▏  , loss=0.992 [03:33<01:11]\n",
      "Epoch [3/5]: [85/119]  71%|███████▏  , loss=1 [03:33<01:11]\n",
      "Epoch [3/5]: [86/119]  72%|███████▏  , loss=1 [03:33<02:42]\n",
      "2025-07-17 09:03:45,593 - INFO - Current learning rate: 0.0007245567089235448\n",
      "2025-07-17 09:03:45,732 - INFO - Epoch: 3/5, Iter: 87/119 -- train_loss: 0.9878\n",
      "Epoch [3/5]: [86/119]  72%|███████▏  , loss=1 [03:33<02:42]\n",
      "Epoch [3/5]: [86/119]  72%|███████▏  , loss=0.988 [03:33<02:42]\n",
      "Epoch [3/5]: [87/119]  73%|███████▎  , loss=0.988 [03:33<01:51]\n",
      "2025-07-17 09:03:45,733 - INFO - Current learning rate: 0.0007211806939124207\n",
      "2025-07-17 09:03:45,875 - INFO - Epoch: 3/5, Iter: 88/119 -- train_loss: 1.0020\n",
      "Epoch [3/5]: [87/119]  73%|███████▎  , loss=0.988 [03:33<01:51]\n",
      "Epoch [3/5]: [87/119]  73%|███████▎  , loss=1 [03:33<01:51]\n",
      "Epoch [3/5]: [88/119]  74%|███████▍  , loss=1 [03:33<01:17]\n",
      "2025-07-17 09:03:45,876 - INFO - Current learning rate: 0.0007177920951257296\n",
      "2025-07-17 09:03:46,029 - INFO - Epoch: 3/5, Iter: 89/119 -- train_loss: 1.0011\n",
      "Epoch [3/5]: [88/119]  74%|███████▍  , loss=1 [03:33<01:17]\n",
      "Epoch [3/5]: [88/119]  74%|███████▍  , loss=1 [03:33<01:17]\n",
      "Epoch [3/5]: [89/119]  75%|███████▍  , loss=1 [03:33<00:53]\n",
      "2025-07-17 09:03:46,031 - INFO - Current learning rate: 0.0007143911053549464\n",
      "2025-07-17 09:03:57,462 - INFO - Epoch: 3/5, Iter: 90/119 -- train_loss: 1.0013\n",
      "Epoch [3/5]: [89/119]  75%|███████▍  , loss=1 [03:45<00:53]\n",
      "Epoch [3/5]: [89/119]  75%|███████▍  , loss=1 [03:45<00:53]\n",
      "Epoch [3/5]: [90/119]  76%|███████▌  , loss=1 [03:45<02:15]\n",
      "2025-07-17 09:03:57,463 - INFO - Current learning rate: 0.0007109779180965208\n",
      "2025-07-17 09:03:57,568 - INFO - Epoch: 3/5, Iter: 91/119 -- train_loss: 0.7774\n",
      "Epoch [3/5]: [90/119]  76%|███████▌  , loss=1 [03:45<02:15]\n",
      "Epoch [3/5]: [90/119]  76%|███████▌  , loss=0.777 [03:45<02:15]\n",
      "Epoch [3/5]: [91/119]  76%|███████▋  , loss=0.777 [03:45<01:32]\n",
      "2025-07-17 09:03:57,569 - INFO - Current learning rate: 0.0007075527275408679\n",
      "2025-07-17 09:03:57,679 - INFO - Epoch: 3/5, Iter: 92/119 -- train_loss: 0.9366\n",
      "Epoch [3/5]: [91/119]  76%|███████▋  , loss=0.777 [03:45<01:32]\n",
      "Epoch [3/5]: [91/119]  76%|███████▋  , loss=0.937 [03:45<01:32]#015Epoch [3/5]: [92/119]  77%|███████▋  , loss=0.937 [03:45<01:03]2025-07-17 09:03:57,681 - INFO - Current learning rate: 0.0007041157285613206\n",
      "2025-07-17 09:03:57,806 - INFO - Epoch: 3/5, Iter: 93/119 -- train_loss: 0.9118\n",
      "Epoch [3/5]: [92/119]  77%|███████▋  , loss=0.937 [03:45<01:03]\n",
      "Epoch [3/5]: [92/119]  77%|███████▋  , loss=0.912 [03:45<01:03]\n",
      "Epoch [3/5]: [93/119]  78%|███████▊  , loss=0.912 [03:45<00:43]\n",
      "2025-07-17 09:03:57,808 - INFO - Current learning rate: 0.000700667116703042\n",
      "2025-07-17 09:04:03,637 - INFO - Epoch: 3/5, Iter: 94/119 -- train_loss: 1.0017\n",
      "Epoch [3/5]: [93/119]  78%|███████▊  , loss=0.912 [03:51<00:43]\n",
      "Epoch [3/5]: [93/119]  78%|███████▊  , loss=1 [03:51<00:43]\n",
      "Epoch [3/5]: [94/119]  79%|███████▉  , loss=1 [03:51<01:13]\n",
      "2025-07-17 09:04:03,638 - INFO - Current learning rate: 0.0006972070881719002\n",
      "2025-07-17 09:04:03,753 - INFO - Epoch: 3/5, Iter: 95/119 -- train_loss: 0.9871\n",
      "Epoch [3/5]: [94/119]  79%|███████▉  , loss=1 [03:51<01:13]\n",
      "Epoch [3/5]: [94/119]  79%|███████▉  , loss=0.987 [03:51<01:13]\n",
      "Epoch [3/5]: [95/119]  80%|███████▉  , loss=0.987 [03:51<00:50]\n",
      "2025-07-17 09:04:03,754 - INFO - Current learning rate: 0.0006937358398233053\n",
      "2025-07-17 09:04:03,882 - INFO - Epoch: 3/5, Iter: 96/119 -- train_loss: 1.0016\n",
      "Epoch [3/5]: [95/119]  80%|███████▉  , loss=0.987 [03:51<00:50]\n",
      "Epoch [3/5]: [95/119]  80%|███████▉  , loss=1 [03:51<00:50]\n",
      "Epoch [3/5]: [96/119]  81%|████████  , loss=1 [03:51<00:34]\n",
      "2025-07-17 09:04:03,884 - INFO - Current learning rate: 0.0006902535691510095\n",
      "2025-07-17 09:04:04,013 - INFO - Epoch: 3/5, Iter: 97/119 -- train_loss: 0.9313\n",
      "Epoch [3/5]: [96/119]  81%|████████  , loss=1 [03:51<00:34]\n",
      "Epoch [3/5]: [96/119]  81%|████████  , loss=0.931 [03:51<00:34]\n",
      "Epoch [3/5]: [97/119]  82%|████████▏ , loss=0.931 [03:51<00:23]\n",
      "2025-07-17 09:04:04,015 - INFO - Current learning rate: 0.0006867604742758704\n",
      "2025-07-17 09:04:09,217 - INFO - Epoch: 3/5, Iter: 98/119 -- train_loss: 1.0015\n",
      "Epoch [3/5]: [97/119]  82%|████████▏ , loss=0.931 [03:57<00:23]\n",
      "Epoch [3/5]: [97/119]  82%|████████▏ , loss=1 [03:57<00:23]\n",
      "Epoch [3/5]: [98/119]  82%|████████▏ , loss=1 [03:57<00:48]\n",
      "2025-07-17 09:04:09,220 - INFO - Current learning rate: 0.0006832567539345805\n",
      "2025-07-17 09:04:11,217 - INFO - Epoch: 3/5, Iter: 99/119 -- train_loss: 0.9918\n",
      "Epoch [3/5]: [98/119]  82%|████████▏ , loss=1 [03:59<00:48]\n",
      "Epoch [3/5]: [98/119]  82%|████████▏ , loss=0.992 [03:59<00:48]\n",
      "Epoch [3/5]: [99/119]  83%|████████▎ , loss=0.992 [03:59<00:44]\n",
      "2025-07-17 09:04:11,218 - INFO - Current learning rate: 0.0006797426074683582\n",
      "2025-07-17 09:04:11,359 - INFO - Epoch: 3/5, Iter: 100/119 -- train_loss: 1.0036\n",
      "Epoch [3/5]: [99/119]  83%|████████▎ , loss=0.992 [03:59<00:44]\n",
      "Epoch [3/5]: [99/119]  83%|████████▎ , loss=1 [03:59<00:44]\n",
      "Epoch [3/5]: [100/119]  84%|████████▍ , loss=1 [03:59<00:30]\n",
      "2025-07-17 09:04:11,361 - INFO - Current learning rate: 0.0006762182348116081\n",
      "2025-07-17 09:04:11,488 - INFO - Epoch: 3/5, Iter: 101/119 -- train_loss: 0.8944\n",
      "Epoch [3/5]: [100/119]  84%|████████▍ , loss=1 [03:59<00:30]\n",
      "Epoch [3/5]: [100/119]  84%|████████▍ , loss=0.894 [03:59<00:30]\n",
      "Epoch [3/5]: [101/119]  85%|████████▍ , loss=0.894 [03:59<00:20]\n",
      "2025-07-17 09:04:11,490 - INFO - Current learning rate: 0.0006726838364805449\n",
      "2025-07-17 09:04:19,447 - INFO - Epoch: 3/5, Iter: 102/119 -- train_loss: 1.0027\n",
      "Epoch [3/5]: [101/119]  85%|████████▍ , loss=0.894 [04:07<00:20]\n",
      "Epoch [3/5]: [101/119]  85%|████████▍ , loss=1 [04:07<00:20]\n",
      "Epoch [3/5]: [102/119]  86%|████████▌ , loss=1 [04:07<00:54]\n",
      "2025-07-17 09:04:19,449 - INFO - Current learning rate: 0.0006691396135617861\n",
      "2025-07-17 09:04:19,580 - INFO - Epoch: 3/5, Iter: 103/119 -- train_loss: 0.5957\n",
      "Epoch [3/5]: [102/119]  86%|████████▌ , loss=1 [04:07<00:54]\n",
      "Epoch [3/5]: [102/119]  86%|████████▌ , loss=0.596 [04:07<00:54]\n",
      "Epoch [3/5]: [103/119]  87%|████████▋ , loss=0.596 [04:07<00:36]\n",
      "2025-07-17 09:04:19,582 - INFO - Current learning rate: 0.0006655857677009106\n",
      "2025-07-17 09:04:21,624 - INFO - Epoch: 3/5, Iter: 104/119 -- train_loss: 1.0027\n",
      "Epoch [3/5]: [103/119]  87%|████████▋ , loss=0.596 [04:09<00:36]\n",
      "Epoch [3/5]: [103/119]  87%|████████▋ , loss=1 [04:09<00:36]\n",
      "Epoch [3/5]: [104/119]  87%|████████▋ , loss=1 [04:09<00:33]\n",
      "2025-07-17 09:04:21,626 - INFO - Current learning rate: 0.0006620225010909862\n",
      "2025-07-17 09:04:21,757 - INFO - Epoch: 3/5, Iter: 105/119 -- train_loss: 0.6229\n",
      "Epoch [3/5]: [104/119]  87%|████████▋ , loss=1 [04:09<00:33]\n",
      "Epoch [3/5]: [104/119]  87%|████████▋ , loss=0.623 [04:09<00:33]\n",
      "Epoch [3/5]: [105/119]  88%|████████▊ , loss=0.623 [04:09<00:22]\n",
      "2025-07-17 09:04:21,759 - INFO - Current learning rate: 0.0006584500164610668\n",
      "2025-07-17 09:04:27,532 - INFO - Epoch: 3/5, Iter: 106/119 -- train_loss: 1.0088\n",
      "Epoch [3/5]: [105/119]  88%|████████▊ , loss=0.623 [04:15<00:22]\n",
      "Epoch [3/5]: [105/119]  88%|████████▊ , loss=1.01 [04:15<00:22]\n",
      "Epoch [3/5]: [106/119]  89%|████████▉ , loss=1.01 [04:15<00:36]\n",
      "2025-07-17 09:04:27,535 - INFO - Current learning rate: 0.0006548685170646574\n",
      "2025-07-17 09:04:27,713 - INFO - Epoch: 3/5, Iter: 107/119 -- train_loss: 0.7765\n",
      "Epoch [3/5]: [106/119]  89%|████████▉ , loss=1.01 [04:15<00:36]\n",
      "Epoch [3/5]: [106/119]  89%|████████▉ , loss=0.777 [04:15<00:36]\n",
      "Epoch [3/5]: [107/119]  90%|████████▉ , loss=0.777 [04:15<00:24]\n",
      "2025-07-17 09:04:27,715 - INFO - Current learning rate: 0.0006512782066681507\n",
      "2025-07-17 09:04:38,902 - INFO - Epoch: 3/5, Iter: 108/119 -- train_loss: 1.0020\n",
      "Epoch [3/5]: [107/119]  90%|████████▉ , loss=0.777 [04:26<00:24]\n",
      "Epoch [3/5]: [107/119]  90%|████████▉ , loss=1 [04:26<00:24]\n",
      "Epoch [3/5]: [108/119]  91%|█████████ , loss=1 [04:26<00:52]\n",
      "2025-07-17 09:04:38,904 - INFO - Current learning rate: 0.0006476792895392338\n",
      "2025-07-17 09:04:39,034 - INFO - Epoch: 3/5, Iter: 109/119 -- train_loss: 1.0027\n",
      "Epoch [3/5]: [108/119]  91%|█████████ , loss=1 [04:26<00:52]\n",
      "Epoch [3/5]: [108/119]  91%|█████████ , loss=1 [04:26<00:52]\n",
      "Epoch [3/5]: [109/119]  92%|█████████▏, loss=1 [04:26<00:33]\n",
      "2025-07-17 09:04:39,036 - INFO - Current learning rate: 0.0006440719704352667\n",
      "2025-07-17 09:04:39,182 - INFO - Epoch: 3/5, Iter: 110/119 -- train_loss: 0.9998\n",
      "Epoch [3/5]: [109/119]  92%|█████████▏, loss=1 [04:27<00:33]\n",
      "Epoch [3/5]: [109/119]  92%|█████████▏, loss=1 [04:27<00:33]\n",
      "Epoch [3/5]: [110/119]  92%|█████████▏, loss=1 [04:27<00:21]\n",
      "2025-07-17 09:04:39,184 - INFO - Current learning rate: 0.0006404564545916326\n",
      "2025-07-17 09:04:39,336 - INFO - Epoch: 3/5, Iter: 111/119 -- train_loss: 0.9703\n",
      "Epoch [3/5]: [110/119]  92%|█████████▏, loss=1 [04:27<00:21]\n",
      "Epoch [3/5]: [110/119]  92%|█████████▏, loss=0.97 [04:27<00:21]\n",
      "Epoch [3/5]: [111/119]  93%|█████████▎, loss=0.97 [04:27<00:13]\n",
      "2025-07-17 09:04:39,338 - INFO - Current learning rate: 0.0006368329477100613\n",
      "2025-07-17 09:04:49,049 - INFO - Epoch: 3/5, Iter: 112/119 -- train_loss: 1.0026\n",
      "Epoch [3/5]: [111/119]  93%|█████████▎, loss=0.97 [04:36<00:13]\n",
      "Epoch [3/5]: [111/119]  93%|█████████▎, loss=1 [04:36<00:13]   #015Epoch [3/5]: [112/119]  94%|█████████▍, loss=1 [04:36<00:28]\n",
      "2025-07-17 09:04:49,051 - INFO - Current learning rate: 0.0006332016559469259\n",
      "2025-07-17 09:04:49,155 - INFO - Epoch: 3/5, Iter: 113/119 -- train_loss: 0.9087\n",
      "Epoch [3/5]: [112/119]  94%|█████████▍, loss=1 [04:37<00:28]\n",
      "Epoch [3/5]: [112/119]  94%|█████████▍, loss=0.909 [04:37<00:28]\n",
      "Epoch [3/5]: [113/119]  95%|█████████▍, loss=0.909 [04:37<00:17]\n",
      "2025-07-17 09:04:49,156 - INFO - Current learning rate: 0.000629562785901514\n",
      "2025-07-17 09:04:49,286 - INFO - Epoch: 3/5, Iter: 114/119 -- train_loss: 1.0027\n",
      "Epoch [3/5]: [113/119]  95%|█████████▍, loss=0.909 [04:37<00:17]\n",
      "Epoch [3/5]: [113/119]  95%|█████████▍, loss=1 [04:37<00:17]\n",
      "Epoch [3/5]: [114/119]  96%|█████████▌, loss=1 [04:37<00:10]\n",
      "2025-07-17 09:04:49,287 - INFO - Current learning rate: 0.0006259165446042728\n",
      "2025-07-17 09:04:49,392 - INFO - Epoch: 3/5, Iter: 115/119 -- train_loss: 1.0018\n",
      "Epoch [3/5]: [114/119]  96%|█████████▌, loss=1 [04:37<00:10]\n",
      "Epoch [3/5]: [114/119]  96%|█████████▌, loss=1 [04:37<00:10]\n",
      "Epoch [3/5]: [115/119]  97%|█████████▋, loss=1 [04:37<00:05]\n",
      "2025-07-17 09:04:49,394 - INFO - Current learning rate: 0.0006222631395050313\n",
      "2025-07-17 09:04:56,774 - INFO - Epoch: 3/5, Iter: 116/119 -- train_loss: 1.0014\n",
      "Epoch [3/5]: [115/119]  97%|█████████▋, loss=1 [04:44<00:05]\n",
      "Epoch [3/5]: [115/119]  97%|█████████▋, loss=1 [04:44<00:05]\n",
      "Epoch [3/5]: [116/119]  97%|█████████▋, loss=1 [04:44<00:09]\n",
      "2025-07-17 09:04:56,775 - INFO - Current learning rate: 0.0006186027784611962\n",
      "2025-07-17 09:04:56,876 - INFO - Epoch: 3/5, Iter: 117/119 -- train_loss: 1.0014\n",
      "Epoch [3/5]: [116/119]  97%|█████████▋, loss=1 [04:44<00:09]\n",
      "Epoch [3/5]: [116/119]  97%|█████████▋, loss=1 [04:44<00:09]\n",
      "Epoch [3/5]: [117/119]  98%|█████████▊, loss=1 [04:44<00:04]\n",
      "2025-07-17 09:04:56,877 - INFO - Current learning rate: 0.0006149356697259276\n",
      "2025-07-17 09:04:56,979 - INFO - Epoch: 3/5, Iter: 118/119 -- train_loss: 1.0005\n",
      "Epoch [3/5]: [117/119]  98%|█████████▊, loss=1 [04:44<00:04]\n",
      "Epoch [3/5]: [117/119]  98%|█████████▊, loss=1 [04:44<00:04]\n",
      "Epoch [3/5]: [118/119]  99%|█████████▉, loss=1 [04:44<00:01]\n",
      "2025-07-17 09:04:56,980 - INFO - Current learning rate: 0.0006112620219362892\n",
      "2025-07-17 09:04:57,081 - INFO - Epoch: 3/5, Iter: 119/119 -- train_loss: 0.7328\n",
      "Epoch [3/5]: [118/119]  99%|█████████▉, loss=1 [04:44<00:01]\n",
      "Epoch [3/5]: [118/119]  99%|█████████▉, loss=0.733 [04:44<00:01]\n",
      "Epoch [3/5]: [119/119] 100%|██████████, loss=0.733 [04:44<00:00]\n",
      "2025-07-17 09:04:57,082 - INFO - Current learning rate: 0.0006075820441013791\n",
      "2025-07-17 09:04:57,082 - INFO - Engine run resuming from iteration 0, epoch 2 until 3 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]\n",
      "#033[A\n",
      "Epoch [3/3]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]\n",
      "#033[A\n",
      "Epoch [3/3]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]\n",
      "#033[A\n",
      "Epoch [3/3]: [2/20]  10%|#033[32m█         #033[0m [00:00<00:13]\n",
      "#033[A\n",
      "Epoch [3/3]: [2/20]  10%|#033[32m█         #033[0m [00:01<00:13]\n",
      "#033[A\n",
      "Epoch [3/3]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:12]\n",
      "#033[A\n",
      "Epoch [3/3]: [3/20]  15%|#033[32m█▌        #033[0m [00:02<00:12]#033[A\n",
      "Epoch [3/3]: [4/20]  20%|#033[32m██        #033[0m [00:02<00:11]#033[A\n",
      "Epoch [3/3]: [4/20]  20%|#033[32m██        #033[0m [00:03<00:11]#033[A\n",
      "Epoch [3/3]: [5/20]  25%|#033[32m██▌       #033[0m [00:03<00:16]#033[A\n",
      "Epoch [3/3]: [5/20]  25%|#033[32m██▌       #033[0m [00:04<00:16]#033[A\n",
      "Epoch [3/3]: [6/20]  30%|#033[32m███       #033[0m [00:04<00:13]#033[A\n",
      "Epoch [3/3]: [6/20]  30%|#033[32m███       #033[0m [00:05<00:13]\n",
      "#033[A\n",
      "Epoch [3/3]: [7/20]  35%|#033[32m███▌      #033[0m [00:05<00:11]\n",
      "#033[A\n",
      "Epoch [3/3]: [7/20]  35%|#033[32m███▌      #033[0m [00:06<00:11]#033[A\n",
      "#015Epoch [3/3]: [8/20]  40%|#033[32m████      #033[0m [00:06<00:10]#033[A\n",
      "Epoch [3/3]: [8/20]  40%|#033[32m████      #033[0m [00:07<00:10]\n",
      "#033[A\n",
      "Epoch [3/3]: [9/20]  45%|#033[32m████▌     #033[0m [00:07<00:11]#033[A\n",
      "Epoch [3/3]: [9/20]  45%|#033[32m████▌     #033[0m [00:09<00:11]\n",
      "#033[A\n",
      "Epoch [3/3]: [10/20]  50%|#033[32m█████     #033[0m [00:09<00:12]\n",
      "#033[A\n",
      "Epoch [3/3]: [10/20]  50%|#033[32m█████     #033[0m [00:10<00:12]#033[A\n",
      "#015Epoch [3/3]: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:11]#033[A\n",
      "Epoch [3/3]: [11/20]  55%|#033[32m█████▌    #033[0m [00:11<00:11]#033[A\n",
      "Epoch [3/3]: [12/20]  60%|#033[32m██████    #033[0m [00:11<00:08]#033[A\n",
      "Epoch [3/3]: [12/20]  60%|#033[32m██████    #033[0m [00:12<00:08]#033[A\n",
      "Epoch [3/3]: [13/20]  65%|#033[32m██████▌   #033[0m [00:12<00:08]#033[A\n",
      "Epoch [3/3]: [13/20]  65%|#033[32m██████▌   #033[0m [00:13<00:08]#033[A\n",
      "Epoch [3/3]: [14/20]  70%|#033[32m███████   #033[0m [00:13<00:06]#033[A\n",
      "Epoch [3/3]: [14/20]  70%|#033[32m███████   #033[0m [00:14<00:06]\n",
      "#033[A\n",
      "Epoch [3/3]: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:04]\n",
      "#033[A\n",
      "Epoch [3/3]: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:04]#033[A\n",
      "Epoch [3/3]: [16/20]  80%|#033[32m████████  #033[0m [00:14<00:03]#033[A\n",
      "Epoch [3/3]: [16/20]  80%|#033[32m████████  #033[0m [00:16<00:03]#033[A\n",
      "Epoch [3/3]: [17/20]  85%|#033[32m████████▌ #033[0m [00:16<00:03]#033[A\n",
      "Epoch [3/3]: [17/20]  85%|#033[32m████████▌ #033[0m [00:16<00:03]#033[A\n",
      "Epoch [3/3]: [18/20]  90%|#033[32m█████████ #033[0m [00:16<00:01]#033[A\n",
      "Epoch [3/3]: [18/20]  90%|#033[32m█████████ #033[0m [00:17<00:01]#033[A\n",
      "Epoch [3/3]: [19/20]  95%|#033[32m█████████▌#033[0m [00:17<00:00]#033[A\n",
      "Epoch [3/3]: [19/20]  95%|#033[32m█████████▌#033[0m [00:17<00:00]#033[A\n",
      "Epoch [3/3]: [20/20] 100%|#033[32m██████████#033[0m [00:17<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-17 09:05:20,991 - INFO - Epoch[3] Complete. Time taken: 00:00:23.825\n",
      "2025-07-17 09:05:20,991 - INFO - Engine run finished. Time taken: 00:00:23.909\n",
      "2025-07-17 09:05:21,070 - INFO - Epoch[3] Complete. Time taken: 00:05:18.659\n",
      "2025-07-17 09:05:31,466 - INFO - Epoch: 4/5, Iter: 1/119 -- train_loss: 0.9999\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [4/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [4/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "2025-07-17 09:05:31,470 - INFO - Current learning rate: 0.0006038959455904377\n",
      "2025-07-17 09:05:31,632 - INFO - Epoch: 4/5, Iter: 2/119 -- train_loss: 1.0022\n",
      "Epoch [4/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [4/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [4/5]: [2/119]   2%|▏         , loss=1 [00:00<00:19]\n",
      "2025-07-17 09:05:31,633 - INFO - Current learning rate: 0.0006002039361209363\n",
      "2025-07-17 09:05:31,759 - INFO - Epoch: 4/5, Iter: 3/119 -- train_loss: 0.9467\n",
      "Epoch [4/5]: [2/119]   2%|▏         , loss=1 [00:00<00:19]\n",
      "Epoch [4/5]: [2/119]   2%|▏         , loss=0.947 [00:00<00:19]\n",
      "Epoch [4/5]: [3/119]   3%|▎         , loss=0.947 [00:00<00:16]\n",
      "2025-07-17 09:05:31,760 - INFO - Current learning rate: 0.0005965062257466451\n",
      "2025-07-17 09:05:31,890 - INFO - Epoch: 4/5, Iter: 4/119 -- train_loss: 1.0014\n",
      "Epoch [4/5]: [3/119]   3%|▎         , loss=0.947 [00:00<00:16]\n",
      "Epoch [4/5]: [3/119]   3%|▎         , loss=1 [00:00<00:16]\n",
      "Epoch [4/5]: [4/119]   3%|▎         , loss=1 [00:00<00:15]\n",
      "2025-07-17 09:05:31,892 - INFO - Current learning rate: 0.0005928030248456825\n",
      "2025-07-17 09:05:44,435 - INFO - Epoch: 4/5, Iter: 5/119 -- train_loss: 0.8486\n",
      "Epoch [4/5]: [4/119]   3%|▎         , loss=1 [00:12<00:15]\n",
      "Epoch [4/5]: [4/119]   3%|▎         , loss=0.849 [00:12<00:15]\n",
      "Epoch [4/5]: [5/119]   4%|▍         , loss=0.849 [00:12<09:34]\n",
      "2025-07-17 09:05:44,438 - INFO - Current learning rate: 0.000589094544108546\n",
      "2025-07-17 09:05:44,595 - INFO - Epoch: 4/5, Iter: 6/119 -- train_loss: 1.0006\n",
      "Epoch [4/5]: [5/119]   4%|▍         , loss=0.849 [00:13<09:34]\n",
      "Epoch [4/5]: [5/119]   4%|▍         , loss=1 [00:13<09:34]\n",
      "Epoch [4/5]: [6/119]   5%|▌         , loss=1 [00:13<06:10]\n",
      "2025-07-17 09:05:44,597 - INFO - Current learning rate: 0.0005853809945261245\n",
      "2025-07-17 09:05:44,753 - INFO - Epoch: 4/5, Iter: 7/119 -- train_loss: 0.9039\n",
      "Epoch [4/5]: [6/119]   5%|▌         , loss=1 [00:13<06:10]\n",
      "Epoch [4/5]: [6/119]   5%|▌         , loss=0.904 [00:13<06:10]\n",
      "Epoch [4/5]: [7/119]   6%|▌         , loss=0.904 [00:13<04:08]\n",
      "2025-07-17 09:05:44,755 - INFO - Current learning rate: 0.000581662587377695\n",
      "2025-07-17 09:05:44,902 - INFO - Epoch: 4/5, Iter: 8/119 -- train_loss: 0.8893\n",
      "Epoch [4/5]: [7/119]   6%|▌         , loss=0.904 [00:13<04:08]\n",
      "Epoch [4/5]: [7/119]   6%|▌         , loss=0.889 [00:13<04:08]\n",
      "Epoch [4/5]: [8/119]   7%|▋         , loss=0.889 [00:13<02:51]\n",
      "2025-07-17 09:05:44,904 - INFO - Current learning rate: 0.0005779395342189016\n",
      "2025-07-17 09:05:56,061 - INFO - Epoch: 4/5, Iter: 9/119 -- train_loss: 1.0010\n",
      "Epoch [4/5]: [8/119]   7%|▋         , loss=0.889 [00:24<02:51]\n",
      "Epoch [4/5]: [8/119]   7%|▋         , loss=1 [00:24<02:51]\n",
      "Epoch [4/5]: [9/119]   8%|▊         , loss=1 [00:24<08:26]\n",
      "2025-07-17 09:05:56,063 - INFO - Current learning rate: 0.0005742120468697188\n",
      "2025-07-17 09:05:56,188 - INFO - Epoch: 4/5, Iter: 10/119 -- train_loss: 1.0017\n",
      "Epoch [4/5]: [9/119]   8%|▊         , loss=1 [00:24<08:26]\n",
      "Epoch [4/5]: [9/119]   8%|▊         , loss=1 [00:24<08:26]\n",
      "Epoch [4/5]: [10/119]   8%|▊         , loss=1 [00:24<05:49]\n",
      "2025-07-17 09:05:56,190 - INFO - Current learning rate: 0.0005704803374024011\n",
      "2025-07-17 09:05:56,314 - INFO - Epoch: 4/5, Iter: 11/119 -- train_loss: 0.9529\n",
      "Epoch [4/5]: [10/119]   8%|▊         , loss=1 [00:24<05:49]\n",
      "Epoch [4/5]: [10/119]   8%|▊         , loss=0.953 [00:24<05:49]\n",
      "Epoch [4/5]: [11/119]   9%|▉         , loss=0.953 [00:24<04:03]\n",
      "2025-07-17 09:05:56,315 - INFO - Current learning rate: 0.0005667446181294167\n",
      "2025-07-17 09:05:56,438 - INFO - Epoch: 4/5, Iter: 12/119 -- train_loss: 1.0012\n",
      "Epoch [4/5]: [11/119]   9%|▉         , loss=0.953 [00:24<04:03]\n",
      "Epoch [4/5]: [11/119]   9%|▉         , loss=1 [00:24<04:03]\n",
      "Epoch [4/5]: [12/119]  10%|█         , loss=1 [00:24<02:51]\n",
      "2025-07-17 09:05:56,440 - INFO - Current learning rate: 0.0005630051015913684\n",
      "2025-07-17 09:06:04,142 - INFO - Epoch: 4/5, Iter: 13/119 -- train_loss: 0.7535\n",
      "Epoch [4/5]: [12/119]  10%|█         , loss=1 [00:32<02:51]\n",
      "Epoch [4/5]: [12/119]  10%|█         , loss=0.753 [00:32<02:51]\n",
      "Epoch [4/5]: [13/119]  11%|█         , loss=0.753 [00:32<06:06]\n",
      "2025-07-17 09:06:04,143 - INFO - Current learning rate: 0.000559262000544901\n",
      "2025-07-17 09:06:04,272 - INFO - Epoch: 4/5, Iter: 14/119 -- train_loss: 1.0032\n",
      "Epoch [4/5]: [13/119]  11%|█         , loss=0.753 [00:32<06:06]\n",
      "Epoch [4/5]: [13/119]  11%|█         , loss=1 [00:32<06:06]\n",
      "Epoch [4/5]: [14/119]  12%|█▏        , loss=1 [00:32<04:17]\n",
      "2025-07-17 09:06:04,273 - INFO - Current learning rate: 0.000555515527950597\n",
      "2025-07-17 09:06:04,642 - INFO - Epoch: 4/5, Iter: 15/119 -- train_loss: 1.0015\n",
      "Epoch [4/5]: [14/119]  12%|█▏        , loss=1 [00:33<04:17]\n",
      "Epoch [4/5]: [14/119]  12%|█▏        , loss=1 [00:33<04:17]\n",
      "Epoch [4/5]: [15/119]  13%|█▎        , loss=1 [00:33<03:09]\n",
      "2025-07-17 09:06:04,644 - INFO - Current learning rate: 0.0005517658969608601\n",
      "2025-07-17 09:06:04,797 - INFO - Epoch: 4/5, Iter: 16/119 -- train_loss: 1.0015\n",
      "Epoch [4/5]: [15/119]  13%|█▎        , loss=1 [00:33<03:09]\n",
      "Epoch [4/5]: [15/119]  13%|█▎        , loss=1 [00:33<03:09]\n",
      "Epoch [4/5]: [16/119]  13%|█▎        , loss=1 [00:33<02:15]\n",
      "2025-07-17 09:06:04,799 - INFO - Current learning rate: 0.000548013320907789\n",
      "2025-07-17 09:06:16,338 - INFO - Epoch: 4/5, Iter: 17/119 -- train_loss: 1.0006\n",
      "Epoch [4/5]: [16/119]  13%|█▎        , loss=1 [00:44<02:15]\n",
      "Epoch [4/5]: [16/119]  13%|█▎        , loss=1 [00:44<02:15]\n",
      "Epoch [4/5]: [17/119]  14%|█▍        , loss=1 [00:44<07:28]2025-07-17 09:06:16,340 - INFO - Current learning rate: 0.0005442580132910382\n",
      "2025-07-17 09:06:16,491 - INFO - Epoch: 4/5, Iter: 18/119 -- train_loss: 0.8815\n",
      "Epoch [4/5]: [17/119]  14%|█▍        , loss=1 [00:45<07:28]\n",
      "Epoch [4/5]: [17/119]  14%|█▍        , loss=0.882 [00:45<07:28]\n",
      "Epoch [4/5]: [18/119]  15%|█▌        , loss=0.882 [00:45<05:15]\n",
      "2025-07-17 09:06:16,493 - INFO - Current learning rate: 0.000540500187765673\n",
      "2025-07-17 09:06:16,655 - INFO - Epoch: 4/5, Iter: 19/119 -- train_loss: 1.0007\n",
      "Epoch [4/5]: [18/119]  15%|█▌        , loss=0.882 [00:45<05:15]\n",
      "Epoch [4/5]: [18/119]  15%|█▌        , loss=1 [00:45<05:15]\n",
      "Epoch [4/5]: [19/119]  16%|█▌        , loss=1 [00:45<03:43]\n",
      "2025-07-17 09:06:16,657 - INFO - Current learning rate: 0.0005367400581300126\n",
      "2025-07-17 09:06:16,808 - INFO - Epoch: 4/5, Iter: 20/119 -- train_loss: 1.0128\n",
      "Epoch [4/5]: [19/119]  16%|█▌        , loss=1 [00:45<03:43]\n",
      "Epoch [4/5]: [19/119]  16%|█▌        , loss=1.01 [00:45<03:43]\n",
      "Epoch [4/5]: [20/119]  17%|█▋        , loss=1.01 [00:45<02:39]\n",
      "2025-07-17 09:06:16,810 - INFO - Current learning rate: 0.0005329778383134667\n",
      "2025-07-17 09:06:22,399 - INFO - Epoch: 4/5, Iter: 21/119 -- train_loss: 0.9360\n",
      "Epoch [4/5]: [20/119]  17%|█▋        , loss=1.01 [00:50<02:39]\n",
      "Epoch [4/5]: [20/119]  17%|█▋        , loss=0.936 [00:50<02:39]\n",
      "Epoch [4/5]: [21/119]  18%|█▊        , loss=0.936 [00:50<04:34]\n",
      "2025-07-17 09:06:22,401 - INFO - Current learning rate: 0.0005292137423643644\n",
      "2025-07-17 09:06:22,531 - INFO - Epoch: 4/5, Iter: 22/119 -- train_loss: 0.6582\n",
      "Epoch [4/5]: [21/119]  18%|█▊        , loss=0.936 [00:51<04:34]\n",
      "Epoch [4/5]: [21/119]  18%|█▊        , loss=0.658 [00:51<04:34]\n",
      "Epoch [4/5]: [22/119]  18%|█▊        , loss=0.658 [00:51<03:14]\n",
      "2025-07-17 09:06:22,532 - INFO - Current learning rate: 0.0005254479844377753\n",
      "2025-07-17 09:06:22,670 - INFO - Epoch: 4/5, Iter: 23/119 -- train_loss: 0.9385\n",
      "Epoch [4/5]: [22/119]  18%|█▊        , loss=0.658 [00:51<03:14]\n",
      "Epoch [4/5]: [22/119]  18%|█▊        , loss=0.938 [00:51<03:14]\n",
      "Epoch [4/5]: [23/119]  19%|█▉        , loss=0.938 [00:51<02:18]\n",
      "2025-07-17 09:06:22,673 - INFO - Current learning rate: 0.000521680778783326\n",
      "2025-07-17 09:06:35,146 - INFO - Epoch: 4/5, Iter: 24/119 -- train_loss: 1.0011\n",
      "Epoch [4/5]: [23/119]  19%|█▉        , loss=0.938 [01:03<02:18]\n",
      "Epoch [4/5]: [23/119]  19%|█▉        , loss=1 [01:03<02:18]\n",
      "Epoch [4/5]: [24/119]  20%|██        , loss=1 [01:03<07:31]\n",
      "2025-07-17 09:06:35,147 - INFO - Current learning rate: 0.0005179123397330104\n",
      "2025-07-17 09:06:38,043 - INFO - Epoch: 4/5, Iter: 25/119 -- train_loss: 0.8224\n",
      "Epoch [4/5]: [24/119]  20%|██        , loss=1 [01:06<07:31]\n",
      "Epoch [4/5]: [24/119]  20%|██        , loss=0.822 [01:06<07:31]\n",
      "Epoch [4/5]: [25/119]  21%|██        , loss=0.822 [01:06<06:34]\n",
      "2025-07-17 09:06:38,044 - INFO - Current learning rate: 0.0005141428816889956\n",
      "2025-07-17 09:06:38,173 - INFO - Epoch: 4/5, Iter: 26/119 -- train_loss: 1.0013\n",
      "Epoch [4/5]: [25/119]  21%|██        , loss=0.822 [01:06<06:34]\n",
      "Epoch [4/5]: [25/119]  21%|██        , loss=1 [01:06<06:34]\n",
      "Epoch [4/5]: [26/119]  22%|██▏       , loss=1 [01:06<04:36]\n",
      "2025-07-17 09:06:38,174 - INFO - Current learning rate: 0.000510372619111423\n",
      "2025-07-17 09:06:38,336 - INFO - Epoch: 4/5, Iter: 27/119 -- train_loss: 1.0020\n",
      "Epoch [4/5]: [26/119]  22%|██▏       , loss=1 [01:06<04:36]\n",
      "Epoch [4/5]: [26/119]  22%|██▏       , loss=1 [01:06<04:36]\n",
      "Epoch [4/5]: [27/119]  23%|██▎       , loss=1 [01:06<03:16]\n",
      "2025-07-17 09:06:38,344 - INFO - Current learning rate: 0.0005066017665062078\n",
      "2025-07-17 09:06:44,493 - INFO - Epoch: 4/5, Iter: 28/119 -- train_loss: 1.0070\n",
      "Epoch [4/5]: [27/119]  23%|██▎       , loss=1 [01:13<03:16]\n",
      "Epoch [4/5]: [27/119]  23%|██▎       , loss=1.01 [01:13<03:16]\n",
      "Epoch [4/5]: [28/119]  24%|██▎       , loss=1.01 [01:13<05:03]\n",
      "2025-07-17 09:06:44,496 - INFO - Current learning rate: 0.0005028305384128339\n",
      "2025-07-17 09:06:51,611 - INFO - Epoch: 4/5, Iter: 29/119 -- train_loss: 1.0022\n",
      "Epoch [4/5]: [28/119]  24%|██▎       , loss=1.01 [01:20<05:03]\n",
      "Epoch [4/5]: [28/119]  24%|██▎       , loss=1 [01:20<05:03]\n",
      "Epoch [4/5]: [29/119]  24%|██▍       , loss=1 [01:20<06:42]\n",
      "2025-07-17 09:06:51,613 - INFO - Current learning rate: 0.0004990591493921485\n",
      "2025-07-17 09:06:51,738 - INFO - Epoch: 4/5, Iter: 30/119 -- train_loss: 0.9527\n",
      "Epoch [4/5]: [29/119]  24%|██▍       , loss=1 [01:20<06:42]\n",
      "Epoch [4/5]: [29/119]  24%|██▍       , loss=0.953 [01:20<06:42]\n",
      "Epoch [4/5]: [30/119]  25%|██▌       , loss=0.953 [01:20<04:42]\n",
      "2025-07-17 09:06:51,739 - INFO - Current learning rate: 0.0004952878140141545\n",
      "2025-07-17 09:06:51,842 - INFO - Epoch: 4/5, Iter: 31/119 -- train_loss: 0.7973\n",
      "Epoch [4/5]: [30/119]  25%|██▌       , loss=0.953 [01:20<04:42]\n",
      "Epoch [4/5]: [30/119]  25%|██▌       , loss=0.797 [01:20<04:42]\n",
      "Epoch [4/5]: [31/119]  26%|██▌       , loss=0.797 [01:20<03:17]\n",
      "2025-07-17 09:06:51,843 - INFO - Current learning rate: 0.0004915167468458031\n",
      "2025-07-17 09:06:51,977 - INFO - Epoch: 4/5, Iter: 32/119 -- train_loss: 1.0020\n",
      "Epoch [4/5]: [31/119]  26%|██▌       , loss=0.797 [01:20<03:17]\n",
      "Epoch [4/5]: [31/119]  26%|██▌       , loss=1 [01:20<03:17]\n",
      "Epoch [4/5]: [32/119]  27%|██▋       , loss=1 [01:20<02:20]\n",
      "2025-07-17 09:06:51,979 - INFO - Current learning rate: 0.0004877461624387859\n",
      "2025-07-17 09:07:00,169 - INFO - Epoch: 4/5, Iter: 33/119 -- train_loss: 0.9258\n",
      "Epoch [4/5]: [32/119]  27%|██▋       , loss=1 [01:28<02:20]\n",
      "Epoch [4/5]: [32/119]  27%|██▋       , loss=0.926 [01:28<02:20]\n",
      "Epoch [4/5]: [33/119]  28%|██▊       , loss=0.926 [01:28<05:08]\n",
      "2025-07-17 09:07:00,172 - INFO - Current learning rate: 0.00048397627531732753\n",
      "2025-07-17 09:07:00,347 - INFO - Epoch: 4/5, Iter: 34/119 -- train_loss: 1.0018\n",
      "Epoch [4/5]: [33/119]  28%|██▊       , loss=0.926 [01:28<05:08]\n",
      "Epoch [4/5]: [33/119]  28%|██▊       , loss=1 [01:28<05:08]\n",
      "Epoch [4/5]: [34/119]  29%|██▊       , loss=1 [01:28<03:38]\n",
      "2025-07-17 09:07:00,349 - INFO - Current learning rate: 0.00048020729996598225\n",
      "2025-07-17 09:07:00,513 - INFO - Epoch: 4/5, Iter: 35/119 -- train_loss: 0.9654\n",
      "Epoch [4/5]: [34/119]  29%|██▊       , loss=1 [01:29<03:38]\n",
      "Epoch [4/5]: [34/119]  29%|██▊       , loss=0.965 [01:29<03:38]\n",
      "Epoch [4/5]: [35/119]  29%|██▉       , loss=0.965 [01:29<02:35]\n",
      "2025-07-17 09:07:00,515 - INFO - Current learning rate: 0.0004764394508174288\n",
      "2025-07-17 09:07:02,450 - INFO - Epoch: 4/5, Iter: 36/119 -- train_loss: 0.8986\n",
      "Epoch [4/5]: [35/119]  29%|██▉       , loss=0.965 [01:30<02:35]\n",
      "Epoch [4/5]: [35/119]  29%|██▉       , loss=0.899 [01:30<02:35]\n",
      "Epoch [4/5]: [36/119]  30%|███       , loss=0.899 [01:30<02:35]\n",
      "2025-07-17 09:07:02,453 - INFO - Current learning rate: 0.0004726729422402727\n",
      "2025-07-17 09:07:13,993 - INFO - Epoch: 4/5, Iter: 37/119 -- train_loss: 0.6663\n",
      "Epoch [4/5]: [36/119]  30%|███       , loss=0.899 [01:42<02:35]\n",
      "Epoch [4/5]: [36/119]  30%|███       , loss=0.666 [01:42<02:35]\n",
      "Epoch [4/5]: [37/119]  31%|███       , loss=0.666 [01:42<06:31]\n",
      "2025-07-17 09:07:13,995 - INFO - Current learning rate: 0.0004689079885268477\n",
      "2025-07-17 09:07:14,099 - INFO - Epoch: 4/5, Iter: 38/119 -- train_loss: 1.0007\n",
      "Epoch [4/5]: [37/119]  31%|███       , loss=0.666 [01:42<06:31]\n",
      "Epoch [4/5]: [37/119]  31%|███       , loss=1 [01:42<06:31]\n",
      "Epoch [4/5]: [38/119]  32%|███▏      , loss=1 [01:42<04:33]\n",
      "2025-07-17 09:07:14,100 - INFO - Current learning rate: 0.0004651448038810258\n",
      "2025-07-17 09:07:14,227 - INFO - Epoch: 4/5, Iter: 39/119 -- train_loss: 1.0014\n",
      "Epoch [4/5]: [38/119]  32%|███▏      , loss=1 [01:42<04:33]\n",
      "Epoch [4/5]: [38/119]  32%|███▏      , loss=1 [01:42<04:33]\n",
      "Epoch [4/5]: [39/119]  33%|███▎      , loss=1 [01:42<03:11]\n",
      "2025-07-17 09:07:14,229 - INFO - Current learning rate: 0.00046138360240602854\n",
      "2025-07-17 09:07:14,390 - INFO - Epoch: 4/5, Iter: 40/119 -- train_loss: 1.0011\n",
      "Epoch [4/5]: [39/119]  33%|███▎      , loss=1 [01:42<03:11]\n",
      "Epoch [4/5]: [39/119]  33%|███▎      , loss=1 [01:42<03:11]\n",
      "Epoch [4/5]: [40/119]  34%|███▎      , loss=1 [01:42<02:16]\n",
      "2025-07-17 09:07:14,393 - INFO - Current learning rate: 0.0004576245980922476\n",
      "2025-07-17 09:07:22,212 - INFO - Epoch: 4/5, Iter: 41/119 -- train_loss: 1.0014\n",
      "Epoch [4/5]: [40/119]  34%|███▎      , loss=1 [01:50<02:16]\n",
      "Epoch [4/5]: [40/119]  34%|███▎      , loss=1 [01:50<02:16]\n",
      "Epoch [4/5]: [41/119]  34%|███▍      , loss=1 [01:50<04:37]2025-07-17 09:07:22,214 - INFO - Current learning rate: 0.000453868004805068\n",
      "2025-07-17 09:07:22,333 - INFO - Epoch: 4/5, Iter: 42/119 -- train_loss: 1.0014\n",
      "Epoch [4/5]: [41/119]  34%|███▍      , loss=1 [01:50<04:37]\n",
      "Epoch [4/5]: [41/119]  34%|███▍      , loss=1 [01:50<04:37]\n",
      "Epoch [4/5]: [42/119]  35%|███▌      , loss=1 [01:50<03:14]\n",
      "2025-07-17 09:07:22,334 - INFO - Current learning rate: 0.0004501140362727022\n",
      "2025-07-17 09:07:22,462 - INFO - Epoch: 4/5, Iter: 43/119 -- train_loss: 1.0015\n",
      "Epoch [4/5]: [42/119]  35%|███▌      , loss=1 [01:50<03:14]\n",
      "Epoch [4/5]: [42/119]  35%|███▌      , loss=1 [01:50<03:14]\n",
      "Epoch [4/5]: [43/119]  36%|███▌      , loss=1 [01:50<02:17]\n",
      "2025-07-17 09:07:22,465 - INFO - Current learning rate: 0.00044636290607402907\n",
      "2025-07-17 09:07:22,612 - INFO - Epoch: 4/5, Iter: 44/119 -- train_loss: 0.9163\n",
      "Epoch [4/5]: [43/119]  36%|███▌      , loss=1 [01:51<02:17]\n",
      "Epoch [4/5]: [43/119]  36%|███▌      , loss=0.916 [01:51<02:17]\n",
      "Epoch [4/5]: [44/119]  37%|███▋      , loss=0.916 [01:51<01:38]\n",
      "2025-07-17 09:07:22,614 - INFO - Current learning rate: 0.00044261482762644305\n",
      "2025-07-17 09:07:31,315 - INFO - Epoch: 4/5, Iter: 45/119 -- train_loss: 0.9219\n",
      "Epoch [4/5]: [44/119]  37%|███▋      , loss=0.916 [01:59<01:38]\n",
      "Epoch [4/5]: [44/119]  37%|███▋      , loss=0.922 [01:59<01:38]\n",
      "Epoch [4/5]: [45/119]  38%|███▊      , loss=0.922 [01:59<04:21]\n",
      "2025-07-17 09:07:31,317 - INFO - Current learning rate: 0.0004388700141737114\n",
      "2025-07-17 09:07:31,439 - INFO - Epoch: 4/5, Iter: 46/119 -- train_loss: 0.9587\n",
      "Epoch [4/5]: [45/119]  38%|███▊      , loss=0.922 [01:59<04:21]\n",
      "Epoch [4/5]: [45/119]  38%|███▊      , loss=0.959 [01:59<04:21]\n",
      "Epoch [4/5]: [46/119]  39%|███▊      , loss=0.959 [01:59<03:02]2025-07-17 09:07:31,441 - INFO - Current learning rate: 0.0004351286787738431\n",
      "2025-07-17 09:07:31,543 - INFO - Epoch: 4/5, Iter: 47/119 -- train_loss: 0.9952\n",
      "Epoch [4/5]: [46/119]  39%|███▊      , loss=0.959 [02:00<03:02]\n",
      "Epoch [4/5]: [46/119]  39%|███▊      , loss=0.995 [02:00<03:02]\n",
      "Epoch [4/5]: [47/119]  39%|███▉      , loss=0.995 [02:00<02:08]\n",
      "2025-07-17 09:07:31,545 - INFO - Current learning rate: 0.0004313910342869651\n",
      "2025-07-17 09:07:33,763 - INFO - Epoch: 4/5, Iter: 48/119 -- train_loss: 1.0010\n",
      "Epoch [4/5]: [47/119]  39%|███▉      , loss=0.995 [02:02<02:08]\n",
      "Epoch [4/5]: [47/119]  39%|███▉      , loss=1 [02:02<02:08]\n",
      "Epoch [4/5]: [48/119]  40%|████      , loss=1 [02:02<02:16]\n",
      "2025-07-17 09:07:33,765 - INFO - Current learning rate: 0.000427657293363214\n",
      "2025-07-17 09:07:37,132 - INFO - Epoch: 4/5, Iter: 49/119 -- train_loss: 1.0010\n",
      "Epoch [4/5]: [48/119]  40%|████      , loss=1 [02:05<02:16]\n",
      "Epoch [4/5]: [48/119]  40%|████      , loss=1 [02:05<02:16]\n",
      "Epoch [4/5]: [49/119]  41%|████      , loss=1 [02:05<02:44]\n",
      "2025-07-17 09:07:37,135 - INFO - Current learning rate: 0.00042392766843063554\n",
      "2025-07-17 09:07:46,493 - INFO - Epoch: 4/5, Iter: 50/119 -- train_loss: 1.0008\n",
      "Epoch [4/5]: [49/119]  41%|████      , loss=1 [02:15<02:44]\n",
      "Epoch [4/5]: [49/119]  41%|████      , loss=1 [02:15<02:44]\n",
      "Epoch [4/5]: [50/119]  42%|████▏     , loss=1 [02:15<05:07]\n",
      "2025-07-17 09:07:46,495 - INFO - Current learning rate: 0.00042020237168310053\n",
      "2025-07-17 09:07:46,611 - INFO - Epoch: 4/5, Iter: 51/119 -- train_loss: 1.0021\n",
      "Epoch [4/5]: [50/119]  42%|████▏     , loss=1 [02:15<05:07]\n",
      "Epoch [4/5]: [50/119]  42%|████▏     , loss=1 [02:15<05:07]\n",
      "Epoch [4/5]: [51/119]  43%|████▎     , loss=1 [02:15<03:34]\n",
      "2025-07-17 09:07:46,612 - INFO - Current learning rate: 0.0004164816150682305\n",
      "2025-07-17 09:07:46,716 - INFO - Epoch: 4/5, Iter: 52/119 -- train_loss: 1.0028\n",
      "Epoch [4/5]: [51/119]  43%|████▎     , loss=1 [02:15<03:34]\n",
      "Epoch [4/5]: [51/119]  43%|████▎     , loss=1 [02:15<03:34]\n",
      "Epoch [4/5]: [52/119]  44%|████▎     , loss=1 [02:15<02:30]\n",
      "2025-07-17 09:07:46,717 - INFO - Current learning rate: 0.0004127656102753405\n",
      "2025-07-17 09:07:46,833 - INFO - Epoch: 4/5, Iter: 53/119 -- train_loss: 0.8341\n",
      "Epoch [4/5]: [52/119]  44%|████▎     , loss=1 [02:15<02:30]\n",
      "Epoch [4/5]: [52/119]  44%|████▎     , loss=0.834 [02:15<02:30]\n",
      "Epoch [4/5]: [53/119]  45%|████▍     , loss=0.834 [02:15<01:45]\n",
      "2025-07-17 09:07:46,836 - INFO - Current learning rate: 0.0004090545687233947\n",
      "2025-07-17 09:07:54,451 - INFO - Epoch: 4/5, Iter: 54/119 -- train_loss: 0.6896\n",
      "Epoch [4/5]: [53/119]  45%|████▍     , loss=0.834 [02:22<01:45]\n",
      "Epoch [4/5]: [53/119]  45%|████▍     , loss=0.69 [02:22<01:45]\n",
      "Epoch [4/5]: [54/119]  45%|████▌     , loss=0.69 [02:22<03:41]\n",
      "2025-07-17 09:07:54,452 - INFO - Current learning rate: 0.0004053487015489772\n",
      "2025-07-17 09:07:54,588 - INFO - Epoch: 4/5, Iter: 55/119 -- train_loss: 1.0017\n",
      "Epoch [4/5]: [54/119]  45%|████▌     , loss=0.69 [02:23<03:41]\n",
      "Epoch [4/5]: [54/119]  45%|████▌     , loss=1 [02:23<03:41]\n",
      "Epoch [4/5]: [55/119]  46%|████▌     , loss=1 [02:23<02:35]\n",
      "2025-07-17 09:07:54,589 - INFO - Current learning rate: 0.0004016482195942813\n",
      "2025-07-17 09:07:54,746 - INFO - Epoch: 4/5, Iter: 56/119 -- train_loss: 1.0012\n",
      "Epoch [4/5]: [55/119]  46%|████▌     , loss=1 [02:23<02:35]\n",
      "Epoch [4/5]: [55/119]  46%|████▌     , loss=1 [02:23<02:35]\n",
      "Epoch [4/5]: [56/119]  47%|████▋     , loss=1 [02:23<01:49]\n",
      "2025-07-17 09:07:54,749 - INFO - Current learning rate: 0.00039795333339511153\n",
      "2025-07-17 09:07:55,051 - INFO - Epoch: 4/5, Iter: 57/119 -- train_loss: 1.0011\n",
      "Epoch [4/5]: [56/119]  47%|████▋     , loss=1 [02:23<01:49]\n",
      "Epoch [4/5]: [56/119]  47%|████▋     , loss=1 [02:23<01:49]\n",
      "Epoch [4/5]: [57/119]  48%|████▊     , loss=1 [02:23<01:21]\n",
      "2025-07-17 09:07:55,052 - INFO - Current learning rate: 0.00039426425316890773\n",
      "2025-07-17 09:08:13,473 - INFO - Epoch: 4/5, Iter: 58/119 -- train_loss: 1.0058\n",
      "Epoch [4/5]: [57/119]  48%|████▊     , loss=1 [02:42<01:21]\n",
      "Epoch [4/5]: [57/119]  48%|████▊     , loss=1.01 [02:42<01:21]\n",
      "Epoch [4/5]: [58/119]  49%|████▊     , loss=1.01 [02:42<06:33]\n",
      "2025-07-17 09:08:13,474 - INFO - Current learning rate: 0.00039058118880278296\n",
      "2025-07-17 09:08:13,608 - INFO - Epoch: 4/5, Iter: 59/119 -- train_loss: 1.0006\n",
      "Epoch [4/5]: [58/119]  49%|████▊     , loss=1.01 [02:42<06:33]\n",
      "Epoch [4/5]: [58/119]  49%|████▊     , loss=1 [02:42<06:33]\n",
      "Epoch [4/5]: [59/119]  50%|████▉     , loss=1 [02:42<04:33]2025-07-17 09:08:13,609 - INFO - Current learning rate: 0.00038690434984158374\n",
      "2025-07-17 09:08:13,737 - INFO - Epoch: 4/5, Iter: 60/119 -- train_loss: 1.0016\n",
      "Epoch [4/5]: [59/119]  50%|████▉     , loss=1 [02:42<04:33]\n",
      "Epoch [4/5]: [59/119]  50%|████▉     , loss=1 [02:42<04:33]\n",
      "Epoch [4/5]: [60/119]  50%|█████     , loss=1 [02:42<03:10]\n",
      "2025-07-17 09:08:13,739 - INFO - Current learning rate: 0.00038323394547596653\n",
      "2025-07-17 09:08:13,890 - INFO - Epoch: 4/5, Iter: 61/119 -- train_loss: 1.0015\n",
      "Epoch [4/5]: [60/119]  50%|█████     , loss=1 [02:42<03:10]\n",
      "Epoch [4/5]: [60/119]  50%|█████     , loss=1 [02:42<03:10]\n",
      "Epoch [4/5]: [61/119]  51%|█████▏    , loss=1 [02:42<02:13]\n",
      "2025-07-17 09:08:13,892 - INFO - Current learning rate: 0.0003795701845304978\n",
      "2025-07-17 09:08:20,684 - INFO - Epoch: 4/5, Iter: 62/119 -- train_loss: 1.0021\n",
      "Epoch [4/5]: [61/119]  51%|█████▏    , loss=1 [02:49<02:13]\n",
      "Epoch [4/5]: [61/119]  51%|█████▏    , loss=1 [02:49<02:13]\n",
      "Epoch [4/5]: [62/119]  52%|█████▏    , loss=1 [02:49<03:28]\n",
      "2025-07-17 09:08:20,686 - INFO - Current learning rate: 0.00037591327545177176\n",
      "2025-07-17 09:08:20,812 - INFO - Epoch: 4/5, Iter: 63/119 -- train_loss: 0.8621\n",
      "Epoch [4/5]: [62/119]  52%|█████▏    , loss=1 [02:49<03:28]\n",
      "Epoch [4/5]: [62/119]  52%|█████▏    , loss=0.862 [02:49<03:28]\n",
      "Epoch [4/5]: [63/119]  53%|█████▎    , loss=0.862 [02:49<02:25]\n",
      "2025-07-17 09:08:20,813 - INFO - Current learning rate: 0.00037226342629655124\n",
      "2025-07-17 09:08:20,937 - INFO - Epoch: 4/5, Iter: 64/119 -- train_loss: 0.9657\n",
      "Epoch [4/5]: [63/119]  53%|█████▎    , loss=0.862 [02:49<02:25]\n",
      "Epoch [4/5]: [63/119]  53%|█████▎    , loss=0.966 [02:49<02:25]\n",
      "Epoch [4/5]: [64/119]  54%|█████▍    , loss=0.966 [02:49<01:41]\n",
      "2025-07-17 09:08:20,939 - INFO - Current learning rate: 0.000368620844719931\n",
      "2025-07-17 09:08:21,064 - INFO - Epoch: 4/5, Iter: 65/119 -- train_loss: 0.9654\n",
      "Epoch [4/5]: [64/119]  54%|█████▍    , loss=0.966 [02:49<01:41]\n",
      "Epoch [4/5]: [64/119]  54%|█████▍    , loss=0.965 [02:49<01:41]\n",
      "Epoch [4/5]: [65/119]  55%|█████▍    , loss=0.965 [02:49<01:12]\n",
      "2025-07-17 09:08:21,066 - INFO - Current learning rate: 0.00036498573796352326\n",
      "2025-07-17 09:08:27,520 - INFO - Epoch: 4/5, Iter: 66/119 -- train_loss: 0.9358\n",
      "Epoch [4/5]: [65/119]  55%|█████▍    , loss=0.965 [02:56<01:12]\n",
      "Epoch [4/5]: [65/119]  55%|█████▍    , loss=0.936 [02:56<01:12]\n",
      "Epoch [4/5]: [66/119]  55%|█████▌    , loss=0.936 [02:56<02:32]\n",
      "2025-07-17 09:08:27,523 - INFO - Current learning rate: 0.0003613583128436658\n",
      "2025-07-17 09:08:27,696 - INFO - Epoch: 4/5, Iter: 67/119 -- train_loss: 0.9002\n",
      "Epoch [4/5]: [66/119]  55%|█████▌    , loss=0.936 [02:56<02:32]\n",
      "Epoch [4/5]: [66/119]  55%|█████▌    , loss=0.9 [02:56<02:32]\n",
      "Epoch [4/5]: [67/119]  56%|█████▋    , loss=0.9 [02:56<01:47]\n",
      "2025-07-17 09:08:27,698 - INFO - Current learning rate: 0.00035773877573965705\n",
      "2025-07-17 09:08:27,853 - INFO - Epoch: 4/5, Iter: 68/119 -- train_loss: 0.9968\n",
      "Epoch [4/5]: [67/119]  56%|█████▋    , loss=0.9 [02:56<01:47]\n",
      "Epoch [4/5]: [67/119]  56%|█████▋    , loss=0.997 [02:56<01:47]\n",
      "Epoch [4/5]: [68/119]  57%|█████▋    , loss=0.997 [02:56<01:16]\n",
      "2025-07-17 09:08:27,855 - INFO - Current learning rate: 0.00035412733258201285\n",
      "2025-07-17 09:08:32,223 - INFO - Epoch: 4/5, Iter: 69/119 -- train_loss: 1.0010\n",
      "Epoch [4/5]: [68/119]  57%|█████▋    , loss=0.997 [03:00<01:16]\n",
      "Epoch [4/5]: [68/119]  57%|█████▋    , loss=1 [03:00<01:16]\n",
      "Epoch [4/5]: [69/119]  58%|█████▊    , loss=1 [03:00<01:57]\n",
      "2025-07-17 09:08:32,226 - INFO - Current learning rate: 0.00035052418884075115\n",
      "2025-07-17 09:08:33,179 - INFO - Epoch: 4/5, Iter: 70/119 -- train_loss: 0.8260\n",
      "Epoch [4/5]: [69/119]  58%|█████▊    , loss=1 [03:01<01:57]\n",
      "Epoch [4/5]: [69/119]  58%|█████▊    , loss=0.826 [03:01<01:57]\n",
      "Epoch [4/5]: [70/119]  59%|█████▉    , loss=0.826 [03:01<01:34]\n",
      "2025-07-17 09:08:33,182 - INFO - Current learning rate: 0.0003469295495137012\n",
      "2025-07-17 09:08:33,358 - INFO - Epoch: 4/5, Iter: 71/119 -- train_loss: 0.8372\n",
      "Epoch [4/5]: [70/119]  59%|█████▉    , loss=0.826 [03:01<01:34]\n",
      "Epoch [4/5]: [70/119]  59%|█████▉    , loss=0.837 [03:01<01:34]\n",
      "Epoch [4/5]: [71/119]  60%|█████▉    , loss=0.837 [03:01<01:07]\n",
      "2025-07-17 09:08:33,360 - INFO - Current learning rate: 0.0003433436191148412\n",
      "2025-07-17 09:08:35,663 - INFO - Epoch: 4/5, Iter: 72/119 -- train_loss: 1.0021\n",
      "Epoch [4/5]: [71/119]  60%|█████▉    , loss=0.837 [03:04<01:07]\n",
      "Epoch [4/5]: [71/119]  60%|█████▉    , loss=1 [03:04<01:07]\n",
      "Epoch [4/5]: [72/119]  61%|██████    , loss=1 [03:04<01:18]\n",
      "2025-07-17 09:08:35,665 - INFO - Current learning rate: 0.00033976660166266167\n",
      "2025-07-17 09:08:41,355 - INFO - Epoch: 4/5, Iter: 73/119 -- train_loss: 0.8931\n",
      "Epoch [4/5]: [72/119]  61%|██████    , loss=1 [03:09<01:18]\n",
      "Epoch [4/5]: [72/119]  61%|██████    , loss=0.893 [03:09<01:18]\n",
      "Epoch [4/5]: [73/119]  61%|██████▏   , loss=0.893 [03:09<02:12]\n",
      "2025-07-17 09:08:41,358 - INFO - Current learning rate: 0.00033619870066855926\n",
      "2025-07-17 09:08:42,043 - INFO - Epoch: 4/5, Iter: 74/119 -- train_loss: 0.9204\n",
      "Epoch [4/5]: [73/119]  61%|██████▏   , loss=0.893 [03:10<02:12]\n",
      "Epoch [4/5]: [73/119]  61%|██████▏   , loss=0.92 [03:10<02:12]\n",
      "Epoch [4/5]: [74/119]  62%|██████▏   , loss=0.92 [03:10<01:40]2025-07-17 09:08:42,045 - INFO - Current learning rate: 0.00033264011912525663\n",
      "2025-07-17 09:08:48,946 - INFO - Epoch: 4/5, Iter: 75/119 -- train_loss: 1.0013\n",
      "Epoch [4/5]: [74/119]  62%|██████▏   , loss=0.92 [03:17<01:40]\n",
      "Epoch [4/5]: [74/119]  62%|██████▏   , loss=1 [03:17<01:40]\n",
      "Epoch [4/5]: [75/119]  63%|██████▎   , loss=1 [03:17<02:39]\n",
      "2025-07-17 09:08:48,947 - INFO - Current learning rate: 0.00032909105949525475\n",
      "2025-07-17 09:08:49,087 - INFO - Epoch: 4/5, Iter: 76/119 -- train_loss: 0.6683\n",
      "Epoch [4/5]: [75/119]  63%|██████▎   , loss=1 [03:17<02:39]\n",
      "Epoch [4/5]: [75/119]  63%|██████▎   , loss=0.668 [03:17<02:39]\n",
      "Epoch [4/5]: [76/119]  64%|██████▍   , loss=0.668 [03:17<01:51]\n",
      "2025-07-17 09:08:49,089 - INFO - Current learning rate: 0.0003255517236993131\n",
      "2025-07-17 09:08:49,212 - INFO - Epoch: 4/5, Iter: 77/119 -- train_loss: 1.0008\n",
      "Epoch [4/5]: [76/119]  64%|██████▍   , loss=0.668 [03:17<01:51]\n",
      "Epoch [4/5]: [76/119]  64%|██████▍   , loss=1 [03:17<01:51]\n",
      "Epoch [4/5]: [77/119]  65%|██████▍   , loss=1 [03:17<01:17]\n",
      "2025-07-17 09:08:49,213 - INFO - Current learning rate: 0.0003220223131049617\n",
      "2025-07-17 09:08:49,341 - INFO - Epoch: 4/5, Iter: 78/119 -- train_loss: 1.0007\n",
      "Epoch [4/5]: [77/119]  65%|██████▍   , loss=1 [03:17<01:17]\n",
      "Epoch [4/5]: [77/119]  65%|██████▍   , loss=1 [03:17<01:17]\n",
      "Epoch [4/5]: [78/119]  66%|██████▌   , loss=1 [03:17<00:54]\n",
      "2025-07-17 09:08:49,345 - INFO - Current learning rate: 0.00031850302851504464\n",
      "2025-07-17 09:09:09,327 - INFO - Epoch: 4/5, Iter: 79/119 -- train_loss: 1.0011\n",
      "Epoch [4/5]: [78/119]  66%|██████▌   , loss=1 [03:37<00:54]\n",
      "Epoch [4/5]: [78/119]  66%|██████▌   , loss=1 [03:37<00:54]\n",
      "Epoch [4/5]: [79/119]  66%|██████▋   , loss=1 [03:37<04:37]\n",
      "2025-07-17 09:09:09,328 - INFO - Current learning rate: 0.0003149940701562959\n",
      "2025-07-17 09:09:09,460 - INFO - Epoch: 4/5, Iter: 80/119 -- train_loss: 0.9468\n",
      "Epoch [4/5]: [79/119]  66%|██████▋   , loss=1 [03:37<04:37]\n",
      "Epoch [4/5]: [79/119]  66%|██████▋   , loss=0.947 [03:37<04:37]\n",
      "Epoch [4/5]: [80/119]  67%|██████▋   , loss=0.947 [03:37<03:10]\n",
      "2025-07-17 09:09:09,461 - INFO - Current learning rate: 0.00031149563766794656\n",
      "2025-07-17 09:09:09,594 - INFO - Epoch: 4/5, Iter: 81/119 -- train_loss: 0.9877\n",
      "Epoch [4/5]: [80/119]  67%|██████▋   , loss=0.947 [03:38<03:10]\n",
      "Epoch [4/5]: [80/119]  67%|██████▋   , loss=0.988 [03:38<03:10]\n",
      "Epoch [4/5]: [81/119]  68%|██████▊   , loss=0.988 [03:38<02:11]\n",
      "2025-07-17 09:09:09,596 - INFO - Current learning rate: 0.0003080079300903678\n",
      "2025-07-17 09:09:09,739 - INFO - Epoch: 4/5, Iter: 82/119 -- train_loss: 1.0023\n",
      "Epoch [4/5]: [81/119]  68%|██████▊   , loss=0.988 [03:38<02:11]\n",
      "Epoch [4/5]: [81/119]  68%|██████▊   , loss=1 [03:38<02:11]\n",
      "Epoch [4/5]: [82/119]  69%|██████▉   , loss=1 [03:38<01:31]\n",
      "2025-07-17 09:09:09,741 - INFO - Current learning rate: 0.0003045311458537454\n",
      "2025-07-17 09:09:15,490 - INFO - Epoch: 4/5, Iter: 83/119 -- train_loss: 1.0007\n",
      "Epoch [4/5]: [82/119]  69%|██████▉   , loss=1 [03:44<01:31]\n",
      "Epoch [4/5]: [82/119]  69%|██████▉   , loss=1 [03:44<01:31]\n",
      "Epoch [4/5]: [83/119]  70%|██████▉   , loss=1 [03:44<02:04]\n",
      "2025-07-17 09:09:15,492 - INFO - Current learning rate: 0.00030106548276679154\n",
      "2025-07-17 09:09:15,638 - INFO - Epoch: 4/5, Iter: 84/119 -- train_loss: 1.0012\n",
      "Epoch [4/5]: [83/119]  70%|██████▉   , loss=1 [03:44<02:04]\n",
      "Epoch [4/5]: [83/119]  70%|██████▉   , loss=1 [03:44<02:04]\n",
      "Epoch [4/5]: [84/119]  71%|███████   , loss=1 [03:44<01:26]\n",
      "2025-07-17 09:09:15,640 - INFO - Current learning rate: 0.00029761113800548934\n",
      "2025-07-17 09:09:15,776 - INFO - Epoch: 4/5, Iter: 85/119 -- train_loss: 0.5719\n",
      "Epoch [4/5]: [84/119]  71%|███████   , loss=1 [03:44<01:26]\n",
      "Epoch [4/5]: [84/119]  71%|███████   , loss=0.572 [03:44<01:26]\n",
      "Epoch [4/5]: [85/119]  71%|███████▏  , loss=0.572 [03:44<00:59]\n",
      "2025-07-17 09:09:15,778 - INFO - Current learning rate: 0.0002941683081018758\n",
      "2025-07-17 09:09:15,893 - INFO - Epoch: 4/5, Iter: 86/119 -- train_loss: 1.0110\n",
      "Epoch [4/5]: [85/119]  71%|███████▏  , loss=0.572 [03:44<00:59]\n",
      "Epoch [4/5]: [85/119]  71%|███████▏  , loss=1.01 [03:44<00:59]\n",
      "Epoch [4/5]: [86/119]  72%|███████▏  , loss=1.01 [03:44<00:41]\n",
      "2025-07-17 09:09:15,894 - INFO - Current learning rate: 0.0002907371889328593\n",
      "2025-07-17 09:09:25,935 - INFO - Epoch: 4/5, Iter: 87/119 -- train_loss: 1.0012\n",
      "Epoch [4/5]: [86/119]  72%|███████▏  , loss=1.01 [03:54<00:41]\n",
      "Epoch [4/5]: [86/119]  72%|███████▏  , loss=1 [03:54<00:41]\n",
      "Epoch [4/5]: [87/119]  73%|███████▎  , loss=1 [03:54<02:04]\n",
      "2025-07-17 09:09:25,937 - INFO - Current learning rate: 0.0002873179757090765\n",
      "2025-07-17 09:09:26,059 - INFO - Epoch: 4/5, Iter: 88/119 -- train_loss: 1.0019\n",
      "Epoch [4/5]: [87/119]  73%|███████▎  , loss=1 [03:54<02:04]\n",
      "Epoch [4/5]: [87/119]  73%|███████▎  , loss=1 [03:54<02:04]\n",
      "Epoch [4/5]: [88/119]  74%|███████▍  , loss=1 [03:54<01:25]\n",
      "2025-07-17 09:09:26,061 - INFO - Current learning rate: 0.0002839108629637845\n",
      "2025-07-17 09:09:26,169 - INFO - Epoch: 4/5, Iter: 89/119 -- train_loss: 0.7850\n",
      "Epoch [4/5]: [88/119]  74%|███████▍  , loss=1 [03:54<01:25]\n",
      "Epoch [4/5]: [88/119]  74%|███████▍  , loss=0.785 [03:54<01:25]\n",
      "Epoch [4/5]: [89/119]  75%|███████▍  , loss=0.785 [03:54<00:59]\n",
      "2025-07-17 09:09:26,171 - INFO - Current learning rate: 0.00028051604454179465\n",
      "2025-07-17 09:09:26,275 - INFO - Epoch: 4/5, Iter: 90/119 -- train_loss: 1.0004\n",
      "Epoch [4/5]: [89/119]  75%|███████▍  , loss=0.785 [03:54<00:59]\n",
      "Epoch [4/5]: [89/119]  75%|███████▍  , loss=1 [03:54<00:59]\n",
      "Epoch [4/5]: [90/119]  76%|███████▌  , loss=1 [03:54<00:40]\n",
      "2025-07-17 09:09:26,277 - INFO - Current learning rate: 0.00027713371358844245\n",
      "2025-07-17 09:09:35,236 - INFO - Epoch: 4/5, Iter: 91/119 -- train_loss: 1.0009\n",
      "Epoch [4/5]: [90/119]  76%|███████▌  , loss=1 [04:03<00:40]\n",
      "Epoch [4/5]: [90/119]  76%|███████▌  , loss=1 [04:03<00:40]#015Epoch [4/5]: [91/119]  76%|███████▋  , loss=1 [04:03<01:42]\n",
      "2025-07-17 09:09:35,238 - INFO - Current learning rate: 0.0002737640625386001\n",
      "2025-07-17 09:09:36,993 - INFO - Epoch: 4/5, Iter: 92/119 -- train_loss: 1.0008\n",
      "Epoch [4/5]: [91/119]  76%|███████▋  , loss=1 [04:05<01:42]\n",
      "Epoch [4/5]: [91/119]  76%|███████▋  , loss=1 [04:05<01:42]\n",
      "Epoch [4/5]: [92/119]  77%|███████▋  , loss=1 [04:05<01:23]\n",
      "2025-07-17 09:09:36,994 - INFO - Current learning rate: 0.00027040728310572654\n",
      "2025-07-17 09:09:37,097 - INFO - Epoch: 4/5, Iter: 93/119 -- train_loss: 1.0006\n",
      "Epoch [4/5]: [92/119]  77%|███████▋  , loss=1 [04:05<01:23]\n",
      "Epoch [4/5]: [92/119]  77%|███████▋  , loss=1 [04:05<01:23]\n",
      "Epoch [4/5]: [93/119]  78%|███████▊  , loss=1 [04:05<00:57]\n",
      "2025-07-17 09:09:37,099 - INFO - Current learning rate: 0.00026706356627096165\n",
      "2025-07-17 09:09:37,203 - INFO - Epoch: 4/5, Iter: 94/119 -- train_loss: 1.0066\n",
      "Epoch [4/5]: [93/119]  78%|███████▊  , loss=1 [04:05<00:57]\n",
      "Epoch [4/5]: [93/119]  78%|███████▊  , loss=1.01 [04:05<00:57]\n",
      "Epoch [4/5]: [94/119]  79%|███████▉  , loss=1.01 [04:05<00:39]\n",
      "2025-07-17 09:09:37,204 - INFO - Current learning rate: 0.0002637331022722593\n",
      "2025-07-17 09:09:43,963 - INFO - Epoch: 4/5, Iter: 95/119 -- train_loss: 1.0005\n",
      "Epoch [4/5]: [94/119]  79%|███████▉  , loss=1.01 [04:12<00:39]\n",
      "Epoch [4/5]: [94/119]  79%|███████▉  , loss=1 [04:12<00:39]   #015Epoch [4/5]: [95/119]  80%|███████▉  , loss=1 [04:12<01:15]\n",
      "2025-07-17 09:09:43,964 - INFO - Current learning rate: 0.00026041608059356483\n",
      "2025-07-17 09:09:49,582 - INFO - Epoch: 4/5, Iter: 96/119 -- train_loss: 1.0011\n",
      "Epoch [4/5]: [95/119]  80%|███████▉  , loss=1 [04:18<01:15]\n",
      "Epoch [4/5]: [95/119]  80%|███████▉  , loss=1 [04:18<01:15]\n",
      "Epoch [4/5]: [96/119]  81%|████████  , loss=1 [04:18<01:29]\n",
      "2025-07-17 09:09:49,584 - INFO - Current learning rate: 0.0002571126899540335\n",
      "2025-07-17 09:09:49,688 - INFO - Epoch: 4/5, Iter: 97/119 -- train_loss: 0.8669\n",
      "Epoch [4/5]: [96/119]  81%|████████  , loss=1 [04:18<01:29]\n",
      "Epoch [4/5]: [96/119]  81%|████████  , loss=0.867 [04:18<01:29]\n",
      "Epoch [4/5]: [97/119]  82%|████████▏ , loss=0.867 [04:18<01:00]\n",
      "2025-07-17 09:09:49,689 - INFO - Current learning rate: 0.00025382311829729457\n",
      "2025-07-17 09:09:49,816 - INFO - Epoch: 4/5, Iter: 98/119 -- train_loss: 0.9586\n",
      "Epoch [4/5]: [97/119]  82%|████████▏ , loss=0.867 [04:18<01:00]\n",
      "Epoch [4/5]: [97/119]  82%|████████▏ , loss=0.959 [04:18<01:00]\n",
      "Epoch [4/5]: [98/119]  82%|████████▏ , loss=0.959 [04:18<00:41]\n",
      "2025-07-17 09:09:49,819 - INFO - Current learning rate: 0.000250547552780758\n",
      "2025-07-17 09:09:54,583 - INFO - Epoch: 4/5, Iter: 99/119 -- train_loss: 0.8401\n",
      "Epoch [4/5]: [98/119]  82%|████████▏ , loss=0.959 [04:23<00:41]\n",
      "Epoch [4/5]: [98/119]  82%|████████▏ , loss=0.84 [04:23<00:41]\n",
      "Epoch [4/5]: [99/119]  83%|████████▎ , loss=0.84 [04:23<00:56]\n",
      "2025-07-17 09:09:54,586 - INFO - Current learning rate: 0.0002472861797649657\n",
      "2025-07-17 09:09:58,674 - INFO - Epoch: 4/5, Iter: 100/119 -- train_loss: 1.0004\n",
      "Epoch [4/5]: [99/119]  83%|████████▎ , loss=0.84 [04:27<00:56]\n",
      "Epoch [4/5]: [99/119]  83%|████████▎ , loss=1 [04:27<00:56]\n",
      "Epoch [4/5]: [100/119]  84%|████████▍ , loss=1 [04:27<01:00]\n",
      "2025-07-17 09:09:58,675 - INFO - Current learning rate: 0.00024403918480298947\n",
      "2025-07-17 09:09:58,779 - INFO - Epoch: 4/5, Iter: 101/119 -- train_loss: 0.8008\n",
      "Epoch [4/5]: [100/119]  84%|████████▍ , loss=1 [04:27<01:00]\n",
      "Epoch [4/5]: [100/119]  84%|████████▍ , loss=0.801 [04:27<01:00]\n",
      "Epoch [4/5]: [101/119]  85%|████████▍ , loss=0.801 [04:27<00:40]\n",
      "2025-07-17 09:09:58,780 - INFO - Current learning rate: 0.0002408067526298741\n",
      "2025-07-17 09:09:58,908 - INFO - Epoch: 4/5, Iter: 102/119 -- train_loss: 1.0007\n",
      "Epoch [4/5]: [101/119]  85%|████████▍ , loss=0.801 [04:27<00:40]\n",
      "Epoch [4/5]: [101/119]  85%|████████▍ , loss=1 [04:27<00:40]\n",
      "Epoch [4/5]: [102/119]  86%|████████▌ , loss=1 [04:27<00:27]\n",
      "2025-07-17 09:09:58,910 - INFO - Current learning rate: 0.00023758906715212614\n",
      "2025-07-17 09:10:04,357 - INFO - Epoch: 4/5, Iter: 103/119 -- train_loss: 1.0007\n",
      "Epoch [4/5]: [102/119]  86%|████████▌ , loss=1 [04:32<00:27]\n",
      "Epoch [4/5]: [102/119]  86%|████████▌ , loss=1 [04:32<00:27]\n",
      "Epoch [4/5]: [103/119]  87%|████████▋ , loss=1 [04:32<00:44]\n",
      "2025-07-17 09:10:04,360 - INFO - Current learning rate: 0.00023438631143725205\n",
      "2025-07-17 09:10:13,783 - INFO - Epoch: 4/5, Iter: 104/119 -- train_loss: 0.8938\n",
      "Epoch [4/5]: [103/119]  87%|████████▋ , loss=1 [04:42<00:44]\n",
      "Epoch [4/5]: [103/119]  87%|████████▋ , loss=0.894 [04:42<00:44]\n",
      "Epoch [4/5]: [104/119]  87%|████████▋ , loss=0.894 [04:42<01:11]\n",
      "2025-07-17 09:10:13,785 - INFO - Current learning rate: 0.0002311986677033413\n",
      "2025-07-17 09:10:13,909 - INFO - Epoch: 4/5, Iter: 105/119 -- train_loss: 0.6788\n",
      "Epoch [4/5]: [104/119]  87%|████████▋ , loss=0.894 [04:42<01:11]\n",
      "Epoch [4/5]: [104/119]  87%|████████▋ , loss=0.679 [04:42<01:11]\n",
      "Epoch [4/5]: [105/119]  88%|████████▊ , loss=0.679 [04:42<00:47]\n",
      "2025-07-17 09:10:13,911 - INFO - Current learning rate: 0.00022802631730870062\n",
      "2025-07-17 09:10:14,046 - INFO - Epoch: 4/5, Iter: 106/119 -- train_loss: 1.0012\n",
      "Epoch [4/5]: [105/119]  88%|████████▊ , loss=0.679 [04:42<00:47]\n",
      "Epoch [4/5]: [105/119]  88%|████████▊ , loss=1 [04:42<00:47]\n",
      "Epoch [4/5]: [106/119]  89%|████████▉ , loss=1 [04:42<00:31]\n",
      "2025-07-17 09:10:14,048 - INFO - Current learning rate: 0.00022486944074153432\n",
      "2025-07-17 09:10:14,177 - INFO - Epoch: 4/5, Iter: 107/119 -- train_loss: 0.7390\n",
      "Epoch [4/5]: [106/119]  89%|████████▉ , loss=1 [04:42<00:31]\n",
      "Epoch [4/5]: [106/119]  89%|████████▉ , loss=0.739 [04:42<00:31]\n",
      "Epoch [4/5]: [107/119]  90%|████████▉ , loss=0.739 [04:42<00:20]\n",
      "2025-07-17 09:10:14,179 - INFO - Current learning rate: 0.00022172821760967716\n",
      "2025-07-17 09:10:21,519 - INFO - Epoch: 4/5, Iter: 108/119 -- train_loss: 0.8370\n",
      "Epoch [4/5]: [107/119]  90%|████████▉ , loss=0.739 [04:50<00:20]\n",
      "Epoch [4/5]: [107/119]  90%|████████▉ , loss=0.837 [04:50<00:20]\n",
      "Epoch [4/5]: [108/119]  91%|█████████ , loss=0.837 [04:50<00:37]\n",
      "2025-07-17 09:10:21,522 - INFO - Current learning rate: 0.00021860282663037402\n",
      "2025-07-17 09:10:21,647 - INFO - Epoch: 4/5, Iter: 109/119 -- train_loss: 1.0006\n",
      "Epoch [4/5]: [108/119]  91%|█████████ , loss=0.837 [04:50<00:37]\n",
      "Epoch [4/5]: [108/119]  91%|█████████ , loss=1 [04:50<00:37]\n",
      "Epoch [4/5]: [109/119]  92%|█████████▏, loss=1 [04:50<00:24]2025-07-17 09:10:21,649 - INFO - Current learning rate: 0.00021549344562011365\n",
      "2025-07-17 09:10:21,794 - INFO - Epoch: 4/5, Iter: 110/119 -- train_loss: 0.7041\n",
      "Epoch [4/5]: [109/119]  92%|█████████▏, loss=1 [04:50<00:24]\n",
      "Epoch [4/5]: [109/119]  92%|█████████▏, loss=0.704 [04:50<00:24]\n",
      "Epoch [4/5]: [110/119]  92%|█████████▏, loss=0.704 [04:50<00:15]\n",
      "2025-07-17 09:10:21,796 - INFO - Current learning rate: 0.00021240025148451026\n",
      "2025-07-17 09:10:21,934 - INFO - Epoch: 4/5, Iter: 111/119 -- train_loss: 0.8231\n",
      "Epoch [4/5]: [110/119]  92%|█████████▏, loss=0.704 [04:50<00:15]\n",
      "Epoch [4/5]: [110/119]  92%|█████████▏, loss=0.823 [04:50<00:15]\n",
      "Epoch [4/5]: [111/119]  93%|█████████▎, loss=0.823 [04:50<00:10]\n",
      "2025-07-17 09:10:21,936 - INFO - Current learning rate: 0.00020932342020824017\n",
      "2025-07-17 09:10:26,650 - INFO - Epoch: 4/5, Iter: 112/119 -- train_loss: 1.0012\n",
      "Epoch [4/5]: [111/119]  93%|█████████▎, loss=0.823 [04:55<00:10]\n",
      "Epoch [4/5]: [111/119]  93%|█████████▎, loss=1 [04:55<00:10]\n",
      "Epoch [4/5]: [112/119]  94%|█████████▍, loss=1 [04:55<00:16]\n",
      "2025-07-17 09:10:26,652 - INFO - Current learning rate: 0.00020626312684502798\n",
      "2025-07-17 09:10:26,782 - INFO - Epoch: 4/5, Iter: 113/119 -- train_loss: 1.0003\n",
      "Epoch [4/5]: [112/119]  94%|█████████▍, loss=1 [04:55<00:16]\n",
      "Epoch [4/5]: [112/119]  94%|█████████▍, loss=1 [04:55<00:16]\n",
      "Epoch [4/5]: [113/119]  95%|█████████▍, loss=1 [04:55<00:09]\n",
      "2025-07-17 09:10:26,784 - INFO - Current learning rate: 0.00020321954550768837\n",
      "2025-07-17 09:10:26,908 - INFO - Epoch: 4/5, Iter: 114/119 -- train_loss: 1.0011\n",
      "Epoch [4/5]: [113/119]  95%|█████████▍, loss=1 [04:55<00:09]\n",
      "Epoch [4/5]: [113/119]  95%|█████████▍, loss=1 [04:55<00:09]\n",
      "Epoch [4/5]: [114/119]  96%|█████████▌, loss=1 [04:55<00:05]\n",
      "2025-07-17 09:10:26,909 - INFO - Current learning rate: 0.00020019284935821852\n",
      "2025-07-17 09:10:28,497 - INFO - Epoch: 4/5, Iter: 115/119 -- train_loss: 1.0031\n",
      "Epoch [4/5]: [114/119]  96%|█████████▌, loss=1 [04:57<00:05]\n",
      "Epoch [4/5]: [114/119]  96%|█████████▌, loss=1 [04:57<00:05]\n",
      "Epoch [4/5]: [115/119]  97%|█████████▋, loss=1 [04:57<00:05]\n",
      "2025-07-17 09:10:28,499 - INFO - Current learning rate: 0.00019718321059794783\n",
      "2025-07-17 09:10:32,602 - INFO - Epoch: 4/5, Iter: 116/119 -- train_loss: 0.8034\n",
      "Epoch [4/5]: [115/119]  97%|█████████▋, loss=1 [05:01<00:05]\n",
      "Epoch [4/5]: [115/119]  97%|█████████▋, loss=0.803 [05:01<00:05]\n",
      "Epoch [4/5]: [116/119]  97%|█████████▋, loss=0.803 [05:01<00:06]\n",
      "2025-07-17 09:10:32,603 - INFO - Current learning rate: 0.0001941908004577393\n",
      "2025-07-17 09:10:32,705 - INFO - Epoch: 4/5, Iter: 117/119 -- train_loss: 1.0096\n",
      "Epoch [4/5]: [116/119]  97%|█████████▋, loss=0.803 [05:01<00:06]\n",
      "Epoch [4/5]: [116/119]  97%|█████████▋, loss=1.01 [05:01<00:06]\n",
      "Epoch [4/5]: [117/119]  98%|█████████▊, loss=1.01 [05:01<00:03]\n",
      "2025-07-17 09:10:32,706 - INFO - Current learning rate: 0.00019121578918824866\n",
      "2025-07-17 09:10:32,807 - INFO - Epoch: 4/5, Iter: 118/119 -- train_loss: 1.0008\n",
      "Epoch [4/5]: [117/119]  98%|█████████▊, loss=1.01 [05:01<00:03]\n",
      "Epoch [4/5]: [117/119]  98%|█████████▊, loss=1 [05:01<00:03]\n",
      "Epoch [4/5]: [118/119]  99%|█████████▉, loss=1 [05:01<00:01]\n",
      "2025-07-17 09:10:32,809 - INFO - Current learning rate: 0.00018825834605023698\n",
      "2025-07-17 09:10:33,966 - INFO - Epoch: 4/5, Iter: 119/119 -- train_loss: 0.9109\n",
      "Epoch [4/5]: [118/119]  99%|█████████▉, loss=1 [05:02<00:01]\n",
      "Epoch [4/5]: [118/119]  99%|█████████▉, loss=0.911 [05:02<00:01]\n",
      "Epoch [4/5]: [119/119] 100%|██████████, loss=0.911 [05:02<00:00]\n",
      "2025-07-17 09:10:33,967 - INFO - Current learning rate: 0.00018531863930494187\n",
      "2025-07-17 09:10:33,967 - INFO - Engine run resuming from iteration 0, epoch 3 until 4 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [4/4]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [4/4]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [4/4]: [2/20]  10%|#033[32m█         #033[0m [00:00<00:11]#033[A\n",
      "Epoch [4/4]: [2/20]  10%|#033[32m█         #033[0m [00:01<00:11]#033[A\n",
      "Epoch [4/4]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:11]#033[A\n",
      "Epoch [4/4]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:11]\n",
      "#033[A\n",
      "Epoch [4/4]: [4/20]  20%|#033[32m██        #033[0m [00:01<00:09]\n",
      "#033[A\n",
      "Epoch [4/4]: [4/20]  20%|#033[32m██        #033[0m [00:03<00:09]#033[A\n",
      "Epoch [4/4]: [5/20]  25%|#033[32m██▌       #033[0m [00:03<00:13]#033[A\n",
      "Epoch [4/4]: [5/20]  25%|#033[32m██▌       #033[0m [00:03<00:13]#033[A\n",
      "Epoch [4/4]: [6/20]  30%|#033[32m███       #033[0m [00:03<00:11]#033[A\n",
      "Epoch [4/4]: [6/20]  30%|#033[32m███       #033[0m [00:05<00:11]#033[A\n",
      "#015Epoch [4/4]: [7/20]  35%|#033[32m███▌      #033[0m [00:05<00:12]#033[A\n",
      "Epoch [4/4]: [7/20]  35%|#033[32m███▌      #033[0m [00:05<00:12]#033[A\n",
      "Epoch [4/4]: [8/20]  40%|#033[32m████      #033[0m [00:05<00:10]#033[A\n",
      "Epoch [4/4]: [8/20]  40%|#033[32m████      #033[0m [00:07<00:10]\n",
      "#033[A\n",
      "Epoch [4/4]: [9/20]  45%|#033[32m████▌     #033[0m [00:07<00:12]\n",
      "#033[A\n",
      "Epoch [4/4]: [9/20]  45%|#033[32m████▌     #033[0m [00:08<00:12]\n",
      "#033[A\n",
      "Epoch [4/4]: [10/20]  50%|#033[32m█████     #033[0m [00:08<00:11]\n",
      "#033[A\n",
      "Epoch [4/4]: [10/20]  50%|#033[32m█████     #033[0m [00:10<00:11]#033[A\n",
      "#015Epoch [4/4]: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:11]\n",
      "#033[A\n",
      "Epoch [4/4]: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:11]#033[A\n",
      "Epoch [4/4]: [12/20]  60%|#033[32m██████    #033[0m [00:10<00:08]#033[A\n",
      "Epoch [4/4]: [12/20]  60%|#033[32m██████    #033[0m [00:13<00:08]\n",
      "#033[A\n",
      "Epoch [4/4]: [13/20]  65%|#033[32m██████▌   #033[0m [00:13<00:10]\n",
      "#033[A\n",
      "Epoch [4/4]: [13/20]  65%|#033[32m██████▌   #033[0m [00:14<00:10]#033[A\n",
      "Epoch [4/4]: [14/20]  70%|#033[32m███████   #033[0m [00:14<00:07]#033[A\n",
      "Epoch [4/4]: [14/20]  70%|#033[32m███████   #033[0m [00:14<00:07]#033[A\n",
      "Epoch [4/4]: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:05]#033[A\n",
      "Epoch [4/4]: [15/20]  75%|#033[32m███████▌  #033[0m [00:15<00:05]#033[A\n",
      "Epoch [4/4]: [16/20]  80%|#033[32m████████  #033[0m [00:15<00:03]#033[A\n",
      "Epoch [4/4]: [16/20]  80%|#033[32m████████  #033[0m [00:16<00:03]#033[A\n",
      "Epoch [4/4]: [17/20]  85%|#033[32m████████▌ #033[0m [00:16<00:03]#033[A\n",
      "Epoch [4/4]: [17/20]  85%|#033[32m████████▌ #033[0m [00:17<00:03]#033[A\n",
      "Epoch [4/4]: [18/20]  90%|#033[32m█████████ #033[0m [00:17<00:01]#033[A\n",
      "Epoch [4/4]: [18/20]  90%|#033[32m█████████ #033[0m [00:17<00:01]#033[A\n",
      "Epoch [4/4]: [19/20]  95%|#033[32m█████████▌#033[0m [00:17<00:00]#033[A\n",
      "Epoch [4/4]: [19/20]  95%|#033[32m█████████▌#033[0m [00:18<00:00]#033[A\n",
      "Epoch [4/4]: [20/20] 100%|#033[32m██████████#033[0m [00:18<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-17 09:10:58,583 - INFO - Epoch[4] Complete. Time taken: 00:00:24.538\n",
      "2025-07-17 09:10:58,584 - INFO - Engine run finished. Time taken: 00:00:24.617\n",
      "2025-07-17 09:10:58,641 - INFO - Epoch[4] Complete. Time taken: 00:05:37.571\n",
      "2025-07-17 09:11:16,299 - INFO - Epoch: 5/5, Iter: 1/119 -- train_loss: 1.0026\n",
      "[1/119]   1%|           [00:00<?]\n",
      "Epoch [5/5]: [1/119]   1%|           [00:00<?]\n",
      "Epoch [5/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "2025-07-17 09:11:16,301 - INFO - Current learning rate: 0.00018239683620450308\n",
      "2025-07-17 09:11:16,404 - INFO - Epoch: 5/5, Iter: 2/119 -- train_loss: 1.0049\n",
      "Epoch [5/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [5/5]: [1/119]   1%|          , loss=1 [00:00<?]\n",
      "Epoch [5/5]: [2/119]   2%|▏         , loss=1 [00:00<00:12]\n",
      "2025-07-17 09:11:16,405 - INFO - Current learning rate: 0.00017949310298244839\n",
      "2025-07-17 09:11:16,511 - INFO - Epoch: 5/5, Iter: 3/119 -- train_loss: 1.0016\n",
      "Epoch [5/5]: [2/119]   2%|▏         , loss=1 [00:00<00:12]\n",
      "Epoch [5/5]: [2/119]   2%|▏         , loss=1 [00:00<00:12]\n",
      "Epoch [5/5]: [3/119]   3%|▎         , loss=1 [00:00<00:12]\n",
      "2025-07-17 09:11:16,512 - INFO - Current learning rate: 0.00017660760484423425\n",
      "2025-07-17 09:11:16,641 - INFO - Epoch: 5/5, Iter: 4/119 -- train_loss: 0.9716\n",
      "Epoch [5/5]: [3/119]   3%|▎         , loss=1 [00:00<00:12]\n",
      "Epoch [5/5]: [3/119]   3%|▎         , loss=0.972 [00:00<00:12]\n",
      "Epoch [5/5]: [4/119]   3%|▎         , loss=0.972 [00:00<00:13]\n",
      "2025-07-17 09:11:16,644 - INFO - Current learning rate: 0.00017374050595784816\n",
      "2025-07-17 09:11:26,776 - INFO - Epoch: 5/5, Iter: 5/119 -- train_loss: 1.0020\n",
      "Epoch [5/5]: [4/119]   3%|▎         , loss=0.972 [00:10<00:13]\n",
      "Epoch [5/5]: [4/119]   3%|▎         , loss=1 [00:10<00:13]\n",
      "Epoch [5/5]: [5/119]   4%|▍         , loss=1 [00:10<07:44]\n",
      "2025-07-17 09:11:26,777 - INFO - Current learning rate: 0.00017089196944446698\n",
      "2025-07-17 09:11:26,882 - INFO - Epoch: 5/5, Iter: 6/119 -- train_loss: 1.0020\n",
      "Epoch [5/5]: [5/119]   4%|▍         , loss=1 [00:10<07:44]\n",
      "Epoch [5/5]: [5/119]   4%|▍         , loss=1 [00:10<07:44]\n",
      "Epoch [5/5]: [6/119]   5%|▌         , loss=1 [00:10<04:58]\n",
      "2025-07-17 09:11:26,883 - INFO - Current learning rate: 0.0001680621573691775\n",
      "2025-07-17 09:11:26,987 - INFO - Epoch: 5/5, Iter: 7/119 -- train_loss: 1.0027\n",
      "Epoch [5/5]: [6/119]   5%|▌         , loss=1 [00:10<04:58]\n",
      "Epoch [5/5]: [6/119]   5%|▌         , loss=1 [00:10<04:58]\n",
      "Epoch [5/5]: [7/119]   6%|▌         , loss=1 [00:10<03:19]\n",
      "2025-07-17 09:11:26,989 - INFO - Current learning rate: 0.00016525123073175493\n",
      "2025-07-17 09:11:27,093 - INFO - Epoch: 5/5, Iter: 8/119 -- train_loss: 0.7240\n",
      "Epoch [5/5]: [7/119]   6%|▌         , loss=1 [00:10<03:19]\n",
      "Epoch [5/5]: [7/119]   6%|▌         , loss=0.724 [00:10<03:19]\n",
      "Epoch [5/5]: [8/119]   7%|▋         , loss=0.724 [00:10<02:16]\n",
      "2025-07-17 09:11:27,094 - INFO - Current learning rate: 0.00016245934945750378\n",
      "2025-07-17 09:11:34,871 - INFO - Epoch: 5/5, Iter: 9/119 -- train_loss: 1.0011\n",
      "Epoch [5/5]: [8/119]   7%|▋         , loss=0.724 [00:18<02:16]\n",
      "Epoch [5/5]: [8/119]   7%|▋         , loss=1 [00:18<02:16]\n",
      "Epoch [5/5]: [9/119]   8%|▊         , loss=1 [00:18<06:04]\n",
      "2025-07-17 09:11:34,872 - INFO - Current learning rate: 0.0001596866723881587\n",
      "2025-07-17 09:11:34,992 - INFO - Epoch: 5/5, Iter: 10/119 -- train_loss: 1.0087\n",
      "Epoch [5/5]: [9/119]   8%|▊         , loss=1 [00:18<06:04]\n",
      "Epoch [5/5]: [9/119]   8%|▊         , loss=1.01 [00:18<06:04]\n",
      "Epoch [5/5]: [10/119]   8%|▊         , loss=1.01 [00:18<04:12]\n",
      "2025-07-17 09:11:34,995 - INFO - Current learning rate: 0.00015693335727284658\n",
      "2025-07-17 09:11:35,121 - INFO - Epoch: 5/5, Iter: 11/119 -- train_loss: 1.0021\n",
      "Epoch [5/5]: [10/119]   8%|▊         , loss=1.01 [00:18<04:12]\n",
      "Epoch [5/5]: [10/119]   8%|▊         , loss=1 [00:18<04:12]\n",
      "Epoch [5/5]: [11/119]   9%|▉         , loss=1 [00:18<02:57]\n",
      "2025-07-17 09:11:35,122 - INFO - Current learning rate: 0.00015419956075911302\n",
      "2025-07-17 09:11:35,251 - INFO - Epoch: 5/5, Iter: 12/119 -- train_loss: 0.9093\n",
      "Epoch [5/5]: [11/119]   9%|▉         , loss=1 [00:18<02:57]\n",
      "Epoch [5/5]: [11/119]   9%|▉         , loss=0.909 [00:18<02:57]\n",
      "Epoch [5/5]: [12/119]  10%|█         , loss=0.909 [00:18<02:06]\n",
      "2025-07-17 09:11:35,253 - INFO - Current learning rate: 0.0001514854383840091\n",
      "2025-07-17 09:11:41,330 - INFO - Epoch: 5/5, Iter: 13/119 -- train_loss: 1.0014\n",
      "Epoch [5/5]: [12/119]  10%|█         , loss=0.909 [00:25<02:06]\n",
      "Epoch [5/5]: [12/119]  10%|█         , loss=1 [00:25<02:06]\n",
      "Epoch [5/5]: [13/119]  11%|█         , loss=1 [00:25<04:42]\n",
      "2025-07-17 09:11:41,332 - INFO - Current learning rate: 0.0001487911445652419\n",
      "2025-07-17 09:11:41,461 - INFO - Epoch: 5/5, Iter: 14/119 -- train_loss: 1.0012\n",
      "Epoch [5/5]: [13/119]  11%|█         , loss=1 [00:25<04:42]\n",
      "Epoch [5/5]: [13/119]  11%|█         , loss=1 [00:25<04:42]\n",
      "Epoch [5/5]: [14/119]  12%|█▏        , loss=1 [00:25<03:19]\n",
      "2025-07-17 09:11:41,463 - INFO - Current learning rate: 0.0001461168325923899\n",
      "2025-07-17 09:11:41,610 - INFO - Epoch: 5/5, Iter: 15/119 -- train_loss: 1.0057\n",
      "Epoch [5/5]: [14/119]  12%|█▏        , loss=1 [00:25<03:19]\n",
      "Epoch [5/5]: [14/119]  12%|█▏        , loss=1.01 [00:25<03:19]\n",
      "Epoch [5/5]: [15/119]  13%|█▎        , loss=1.01 [00:25<02:22]\n",
      "2025-07-17 09:11:41,611 - INFO - Current learning rate: 0.0001434626546181809\n",
      "2025-07-17 09:11:41,752 - INFO - Epoch: 5/5, Iter: 16/119 -- train_loss: 0.9862\n",
      "Epoch [5/5]: [15/119]  13%|█▎        , loss=1.01 [00:25<02:22]\n",
      "Epoch [5/5]: [15/119]  13%|█▎        , loss=0.986 [00:25<02:22]\n",
      "Epoch [5/5]: [16/119]  13%|█▎        , loss=0.986 [00:25<01:43]\n",
      "2025-07-17 09:11:41,755 - INFO - Current learning rate: 0.00014082876164983645\n",
      "2025-07-17 09:11:53,711 - INFO - Epoch: 5/5, Iter: 17/119 -- train_loss: 0.8482\n",
      "Epoch [5/5]: [16/119]  13%|█▎        , loss=0.986 [00:37<01:43]\n",
      "Epoch [5/5]: [16/119]  13%|█▎        , loss=0.848 [00:37<01:43]\n",
      "Epoch [5/5]: [17/119]  14%|█▍        , loss=0.848 [00:37<07:18]\n",
      "2025-07-17 09:11:53,712 - INFO - Current learning rate: 0.00013821530354047896\n",
      "2025-07-17 09:11:53,845 - INFO - Epoch: 5/5, Iter: 18/119 -- train_loss: 1.0015\n",
      "Epoch [5/5]: [17/119]  14%|█▍        , loss=0.848 [00:37<07:18]\n",
      "Epoch [5/5]: [17/119]  14%|█▍        , loss=1 [00:37<07:18]\n",
      "Epoch [5/5]: [18/119]  15%|█▌        , loss=1 [00:37<05:07]\n",
      "2025-07-17 09:11:53,846 - INFO - Current learning rate: 0.00013562242898060745\n",
      "2025-07-17 09:11:53,976 - INFO - Epoch: 5/5, Iter: 19/119 -- train_loss: 0.8266\n",
      "Epoch [5/5]: [18/119]  15%|█▌        , loss=1 [00:37<05:07]\n",
      "Epoch [5/5]: [18/119]  15%|█▌        , loss=0.827 [00:37<05:07]\n",
      "Epoch [5/5]: [19/119]  16%|█▌        , loss=0.827 [00:37<03:37]\n",
      "2025-07-17 09:11:53,977 - INFO - Current learning rate: 0.00013305028548963658\n",
      "2025-07-17 09:11:54,111 - INFO - Epoch: 5/5, Iter: 20/119 -- train_loss: 0.9597\n",
      "Epoch [5/5]: [19/119]  16%|█▌        , loss=0.827 [00:37<03:37]\n",
      "Epoch [5/5]: [19/119]  16%|█▌        , loss=0.96 [00:37<03:37]\n",
      "Epoch [5/5]: [20/119]  17%|█▋        , loss=0.96 [00:37<02:34]\n",
      "2025-07-17 09:11:54,114 - INFO - Current learning rate: 0.00013049901940750497\n",
      "2025-07-17 09:11:59,859 - INFO - Epoch: 5/5, Iter: 21/119 -- train_loss: 1.0016\n",
      "Epoch [5/5]: [20/119]  17%|█▋        , loss=0.96 [00:43<02:34]\n",
      "Epoch [5/5]: [20/119]  17%|█▋        , loss=1 [00:43<02:34]\n",
      "Epoch [5/5]: [21/119]  18%|█▊        , loss=1 [00:43<04:36]\n",
      "2025-07-17 09:11:59,861 - INFO - Current learning rate: 0.00012796877588634801\n",
      "2025-07-17 09:11:59,998 - INFO - Epoch: 5/5, Iter: 22/119 -- train_loss: 0.5992\n",
      "Epoch [5/5]: [21/119]  18%|█▊        , loss=1 [00:43<04:36]\n",
      "Epoch [5/5]: [21/119]  18%|█▊        , loss=0.599 [00:43<04:36]\n",
      "Epoch [5/5]: [22/119]  18%|█▊        , loss=0.599 [00:43<03:15]\n",
      "2025-07-17 09:12:00,000 - INFO - Current learning rate: 0.00012545969888224073\n",
      "2025-07-17 09:12:00,159 - INFO - Epoch: 5/5, Iter: 23/119 -- train_loss: 0.8714\n",
      "Epoch [5/5]: [22/119]  18%|█▊        , loss=0.599 [00:43<03:15]\n",
      "Epoch [5/5]: [22/119]  18%|█▊        , loss=0.871 [00:43<03:15]\n",
      "Epoch [5/5]: [23/119]  19%|█▉        , loss=0.871 [00:43<02:19]\n",
      "2025-07-17 09:12:00,161 - INFO - Current learning rate: 0.00012297193114700652\n",
      "2025-07-17 09:12:00,319 - INFO - Epoch: 5/5, Iter: 24/119 -- train_loss: 0.8434\n",
      "Epoch [5/5]: [23/119]  19%|█▉        , loss=0.871 [00:44<02:19]\n",
      "Epoch [5/5]: [23/119]  19%|█▉        , loss=0.843 [00:44<02:19]\n",
      "Epoch [5/5]: [24/119]  20%|██        , loss=0.843 [00:44<01:41]\n",
      "2025-07-17 09:12:00,321 - INFO - Current learning rate: 0.00012050561422009632\n",
      "2025-07-17 09:12:12,089 - INFO - Epoch: 5/5, Iter: 25/119 -- train_loss: 0.9291\n",
      "Epoch [5/5]: [24/119]  20%|██        , loss=0.843 [00:55<01:41]\n",
      "Epoch [5/5]: [24/119]  20%|██        , loss=0.929 [00:55<01:41]\n",
      "Epoch [5/5]: [25/119]  21%|██        , loss=0.929 [00:55<06:42]\n",
      "2025-07-17 09:12:12,091 - INFO - Current learning rate: 0.00011806088842053486\n",
      "2025-07-17 09:12:12,219 - INFO - Epoch: 5/5, Iter: 26/119 -- train_loss: 1.0016\n",
      "Epoch [5/5]: [25/119]  21%|██        , loss=0.929 [00:55<06:42]\n",
      "Epoch [5/5]: [25/119]  21%|██        , loss=1 [00:55<06:42]\n",
      "Epoch [5/5]: [26/119]  22%|██▏       , loss=1 [00:55<04:42]\n",
      "2025-07-17 09:12:12,220 - INFO - Current learning rate: 0.00011563789283893834\n",
      "2025-07-17 09:12:12,325 - INFO - Epoch: 5/5, Iter: 27/119 -- train_loss: 0.9398\n",
      "Epoch [5/5]: [26/119]  22%|██▏       , loss=1 [00:56<04:42]\n",
      "Epoch [5/5]: [26/119]  22%|██▏       , loss=0.94 [00:56<04:42]\n",
      "Epoch [5/5]: [27/119]  23%|██▎       , loss=0.94 [00:56<03:18]\n",
      "2025-07-17 09:12:12,326 - INFO - Current learning rate: 0.00011323676532960039\n",
      "2025-07-17 09:12:12,493 - INFO - Epoch: 5/5, Iter: 28/119 -- train_loss: 1.0010\n",
      "Epoch [5/5]: [27/119]  23%|██▎       , loss=0.94 [00:56<03:18]\n",
      "Epoch [5/5]: [27/119]  23%|██▎       , loss=1 [00:56<03:18]\n",
      "Epoch [5/5]: [28/119]  24%|██▎       , loss=1 [00:56<02:21]\n",
      "2025-07-17 09:12:12,497 - INFO - Current learning rate: 0.0001108576425026488\n",
      "2025-07-17 09:12:20,309 - INFO - Epoch: 5/5, Iter: 29/119 -- train_loss: 1.0015\n",
      "Epoch [5/5]: [28/119]  24%|██▎       , loss=1 [01:04<02:21]\n",
      "Epoch [5/5]: [28/119]  24%|██▎       , loss=1 [01:04<02:21]\n",
      "Epoch [5/5]: [29/119]  24%|██▍       , loss=1 [01:04<05:09]2025-07-17 09:12:20,311 - INFO - Current learning rate: 0.00010850065971627389\n",
      "2025-07-17 09:12:20,436 - INFO - Epoch: 5/5, Iter: 30/119 -- train_loss: 0.9311\n",
      "Epoch [5/5]: [29/119]  24%|██▍       , loss=1 [01:04<05:09]\n",
      "Epoch [5/5]: [29/119]  24%|██▍       , loss=0.931 [01:04<05:09]\n",
      "Epoch [5/5]: [30/119]  25%|██▌       , loss=0.931 [01:04<03:37]\n",
      "2025-07-17 09:12:20,438 - INFO - Current learning rate: 0.0001061659510690266\n",
      "2025-07-17 09:12:20,547 - INFO - Epoch: 5/5, Iter: 31/119 -- train_loss: 1.0010\n",
      "Epoch [5/5]: [30/119]  25%|██▌       , loss=0.931 [01:04<03:37]\n",
      "Epoch [5/5]: [30/119]  25%|██▌       , loss=1 [01:04<03:37]\n",
      "Epoch [5/5]: [31/119]  26%|██▌       , loss=1 [01:04<02:33]\n",
      "2025-07-17 09:12:20,549 - INFO - Current learning rate: 0.0001038536493921899\n",
      "2025-07-17 09:12:20,680 - INFO - Epoch: 5/5, Iter: 32/119 -- train_loss: 0.8765\n",
      "Epoch [5/5]: [31/119]  26%|██▌       , loss=1 [01:04<02:33]\n",
      "Epoch [5/5]: [31/119]  26%|██▌       , loss=0.877 [01:04<02:33]\n",
      "Epoch [5/5]: [32/119]  27%|██▋       , loss=0.877 [01:04<01:49]\n",
      "2025-07-17 09:12:20,681 - INFO - Current learning rate: 0.0001015638862422206\n",
      "2025-07-17 09:12:27,114 - INFO - Epoch: 5/5, Iter: 33/119 -- train_loss: 0.7513\n",
      "Epoch [5/5]: [32/119]  27%|██▋       , loss=0.877 [01:10<01:49]\n",
      "Epoch [5/5]: [32/119]  27%|██▋       , loss=0.751 [01:10<01:49]\n",
      "Epoch [5/5]: [33/119]  28%|██▊       , loss=0.751 [01:10<04:01]\n",
      "2025-07-17 09:12:27,116 - INFO - Current learning rate: 9.929679189326547e-05\n",
      "2025-07-17 09:12:27,220 - INFO - Epoch: 5/5, Iter: 34/119 -- train_loss: 1.0023\n",
      "Epoch [5/5]: [33/119]  28%|██▊       , loss=0.751 [01:10<04:01]\n",
      "Epoch [5/5]: [33/119]  28%|██▊       , loss=1 [01:10<04:01]\n",
      "Epoch [5/5]: [34/119]  29%|██▊       , loss=1 [01:10<02:50]\n",
      "2025-07-17 09:12:27,221 - INFO - Current learning rate: 9.705249532974846e-05\n",
      "2025-07-17 09:12:27,326 - INFO - Epoch: 5/5, Iter: 35/119 -- train_loss: 1.0015\n",
      "Epoch [5/5]: [34/119]  29%|██▊       , loss=1 [01:11<02:50]\n",
      "Epoch [5/5]: [34/119]  29%|██▊       , loss=1 [01:11<02:50]\n",
      "Epoch [5/5]: [35/119]  29%|██▉       , loss=1 [01:11<02:00]\n",
      "2025-07-17 09:12:27,327 - INFO - Current learning rate: 9.483112423903319e-05\n",
      "2025-07-17 09:12:27,455 - INFO - Epoch: 5/5, Iter: 36/119 -- train_loss: 1.0018\n",
      "Epoch [5/5]: [35/119]  29%|██▉       , loss=1 [01:11<02:00]\n",
      "Epoch [5/5]: [35/119]  29%|██▉       , loss=1 [01:11<02:00]\n",
      "Epoch [5/5]: [36/119]  30%|███       , loss=1 [01:11<01:26]\n",
      "2025-07-17 09:12:27,456 - INFO - Current learning rate: 9.263280500415739e-05\n",
      "2025-07-17 09:12:33,862 - INFO - Epoch: 5/5, Iter: 37/119 -- train_loss: 0.8246\n",
      "Epoch [5/5]: [36/119]  30%|███       , loss=1 [01:17<01:26]\n",
      "Epoch [5/5]: [36/119]  30%|███       , loss=0.825 [01:17<01:26]\n",
      "Epoch [5/5]: [37/119]  31%|███       , loss=0.825 [01:17<03:37]\n",
      "2025-07-17 09:12:33,864 - INFO - Current learning rate: 9.045766269664314e-05\n",
      "2025-07-17 09:12:34,014 - INFO - Epoch: 5/5, Iter: 38/119 -- train_loss: 1.0016\n",
      "Epoch [5/5]: [37/119]  31%|███       , loss=0.825 [01:17<03:37]\n",
      "Epoch [5/5]: [37/119]  31%|███       , loss=1 [01:17<03:37]\n",
      "Epoch [5/5]: [38/119]  32%|███▏      , loss=1 [01:17<02:34]\n",
      "2025-07-17 09:12:34,016 - INFO - Current learning rate: 8.830582106938051e-05\n",
      "2025-07-17 09:12:37,552 - INFO - Epoch: 5/5, Iter: 39/119 -- train_loss: 1.0047\n",
      "Epoch [5/5]: [38/119]  32%|███▏      , loss=1 [01:21<02:34]\n",
      "Epoch [5/5]: [38/119]  32%|███▏      , loss=1 [01:21<02:34]\n",
      "Epoch [5/5]: [39/119]  33%|███▎      , loss=1 [01:21<03:11]\n",
      "2025-07-17 09:12:37,554 - INFO - Current learning rate: 8.617740254958729e-05\n",
      "2025-07-17 09:12:37,675 - INFO - Epoch: 5/5, Iter: 40/119 -- train_loss: 0.9552\n",
      "Epoch [5/5]: [39/119]  33%|███▎      , loss=1 [01:21<03:11]\n",
      "Epoch [5/5]: [39/119]  33%|███▎      , loss=0.955 [01:21<03:11]\n",
      "Epoch [5/5]: [40/119]  34%|███▎      , loss=0.955 [01:21<02:15]\n",
      "2025-07-17 09:12:37,676 - INFO - Current learning rate: 8.407252823184297e-05\n",
      "2025-07-17 09:12:39,882 - INFO - Epoch: 5/5, Iter: 41/119 -- train_loss: 0.9663\n",
      "Epoch [5/5]: [40/119]  34%|███▎      , loss=0.955 [01:23<02:15]\n",
      "Epoch [5/5]: [40/119]  34%|███▎      , loss=0.966 [01:23<02:15]\n",
      "Epoch [5/5]: [41/119]  34%|███▍      , loss=0.966 [01:23<02:25]\n",
      "2025-07-17 09:12:39,885 - INFO - Current learning rate: 8.199131787119973e-05\n",
      "2025-07-17 09:12:40,034 - INFO - Epoch: 5/5, Iter: 42/119 -- train_loss: 1.0026\n",
      "Epoch [5/5]: [41/119]  34%|███▍      , loss=0.966 [01:23<02:25]\n",
      "Epoch [5/5]: [41/119]  34%|███▍      , loss=1 [01:23<02:25]\n",
      "Epoch [5/5]: [42/119]  35%|███▌      , loss=1 [01:23<01:43]\n",
      "2025-07-17 09:12:40,036 - INFO - Current learning rate: 7.993388987636881e-05\n",
      "2025-07-17 09:12:43,797 - INFO - Epoch: 5/5, Iter: 43/119 -- train_loss: 0.9180\n",
      "Epoch [5/5]: [42/119]  35%|███▌      , loss=1 [01:27<01:43]\n",
      "Epoch [5/5]: [42/119]  35%|███▌      , loss=0.918 [01:27<01:43]\n",
      "Epoch [5/5]: [43/119]  36%|███▌      , loss=0.918 [01:27<02:37]\n",
      "2025-07-17 09:12:43,810 - INFO - Current learning rate: 7.790036130298354e-05\n",
      "2025-07-17 09:12:43,951 - INFO - Epoch: 5/5, Iter: 44/119 -- train_loss: 0.8876\n",
      "Epoch [5/5]: [43/119]  36%|███▌      , loss=0.918 [01:27<02:37]\n",
      "Epoch [5/5]: [43/119]  36%|███▌      , loss=0.888 [01:27<02:37]\n",
      "Epoch [5/5]: [44/119]  37%|███▋      , loss=0.888 [01:27<01:52]\n",
      "2025-07-17 09:12:43,953 - INFO - Current learning rate: 7.589084784694004e-05\n",
      "2025-07-17 09:12:45,087 - INFO - Epoch: 5/5, Iter: 45/119 -- train_loss: 0.9159\n",
      "Epoch [5/5]: [44/119]  37%|███▋      , loss=0.888 [01:28<01:52]\n",
      "Epoch [5/5]: [44/119]  37%|███▋      , loss=0.916 [01:28<01:52]\n",
      "Epoch [5/5]: [45/119]  38%|███▊      , loss=0.916 [01:28<01:42]\n",
      "2025-07-17 09:12:45,101 - INFO - Current learning rate: 7.39054638378146e-05\n",
      "2025-07-17 09:12:45,233 - INFO - Epoch: 5/5, Iter: 46/119 -- train_loss: 1.0019\n",
      "Epoch [5/5]: [45/119]  38%|███▊      , loss=0.916 [01:28<01:42]\n",
      "Epoch [5/5]: [45/119]  38%|███▊      , loss=1 [01:28<01:42]\n",
      "Epoch [5/5]: [46/119]  39%|███▊      , loss=1 [01:28<01:14]\n",
      "2025-07-17 09:12:45,235 - INFO - Current learning rate: 7.194432223235858e-05\n",
      "2025-07-17 09:12:51,484 - INFO - Epoch: 5/5, Iter: 47/119 -- train_loss: 1.0009\n",
      "Epoch [5/5]: [46/119]  39%|███▊      , loss=1 [01:35<01:14]\n",
      "Epoch [5/5]: [46/119]  39%|███▊      , loss=1 [01:35<01:14]\n",
      "Epoch [5/5]: [47/119]  39%|███▉      , loss=1 [01:35<03:06]\n",
      "2025-07-17 09:12:51,485 - INFO - Current learning rate: 7.000753460807261e-05\n",
      "2025-07-17 09:12:51,616 - INFO - Epoch: 5/5, Iter: 48/119 -- train_loss: 0.8221\n",
      "Epoch [5/5]: [47/119]  39%|███▉      , loss=1 [01:35<03:06]\n",
      "Epoch [5/5]: [47/119]  39%|███▉      , loss=0.822 [01:35<03:06]\n",
      "Epoch [5/5]: [48/119]  40%|████      , loss=0.822 [01:35<02:11]\n",
      "2025-07-17 09:12:51,617 - INFO - Current learning rate: 6.809521115685772e-05\n",
      "2025-07-17 09:12:55,914 - INFO - Epoch: 5/5, Iter: 49/119 -- train_loss: 0.8458\n",
      "Epoch [5/5]: [48/119]  40%|████      , loss=0.822 [01:39<02:11]\n",
      "Epoch [5/5]: [48/119]  40%|████      , loss=0.846 [01:39<02:11]\n",
      "Epoch [5/5]: [49/119]  41%|████      , loss=0.846 [01:39<03:00]\n",
      "2025-07-17 09:12:55,915 - INFO - Current learning rate: 6.620746067874675e-05\n",
      "2025-07-17 09:12:56,672 - INFO - Epoch: 5/5, Iter: 50/119 -- train_loss: 1.0010\n",
      "Epoch [5/5]: [49/119]  41%|████      , loss=0.846 [01:40<03:00]\n",
      "Epoch [5/5]: [49/119]  41%|████      , loss=1 [01:40<03:00]\n",
      "Epoch [5/5]: [50/119]  42%|████▏     , loss=1 [01:40<02:20]\n",
      "2025-07-17 09:12:56,674 - INFO - Current learning rate: 6.434439057571353e-05\n",
      "2025-07-17 09:12:59,715 - INFO - Epoch: 5/5, Iter: 51/119 -- train_loss: 1.0014\n",
      "Epoch [5/5]: [50/119]  42%|████▏     , loss=1 [01:43<02:20]\n",
      "Epoch [5/5]: [50/119]  42%|████▏     , loss=1 [01:43<02:20]\n",
      "Epoch [5/5]: [51/119]  43%|████▎     , loss=1 [01:43<02:39]\n",
      "2025-07-17 09:12:59,718 - INFO - Current learning rate: 6.250610684556319e-05\n",
      "2025-07-17 09:12:59,891 - INFO - Epoch: 5/5, Iter: 52/119 -- train_loss: 1.0030\n",
      "Epoch [5/5]: [51/119]  43%|████▎     , loss=1 [01:43<02:39]\n",
      "Epoch [5/5]: [51/119]  43%|████▎     , loss=1 [01:43<02:39]\n",
      "Epoch [5/5]: [52/119]  44%|████▎     , loss=1 [01:43<01:53]\n",
      "2025-07-17 09:12:59,894 - INFO - Current learning rate: 6.069271407590054e-05\n",
      "2025-07-17 09:13:05,395 - INFO - Epoch: 5/5, Iter: 53/119 -- train_loss: 1.0013\n",
      "Epoch [5/5]: [52/119]  44%|████▎     , loss=1 [01:49<01:53]\n",
      "Epoch [5/5]: [52/119]  44%|████▎     , loss=1 [01:49<01:53]\n",
      "Epoch [5/5]: [53/119]  45%|████▍     , loss=1 [01:49<03:07]\n",
      "2025-07-17 09:13:05,397 - INFO - Current learning rate: 5.890431543818061e-05\n",
      "2025-07-17 09:13:10,631 - INFO - Epoch: 5/5, Iter: 54/119 -- train_loss: 1.0010\n",
      "Epoch [5/5]: [53/119]  45%|████▍     , loss=1 [01:54<03:07]\n",
      "Epoch [5/5]: [53/119]  45%|████▍     , loss=1 [01:54<03:07]\n",
      "Epoch [5/5]: [54/119]  45%|████▌     , loss=1 [01:54<03:51]\n",
      "2025-07-17 09:13:10,633 - INFO - Current learning rate: 5.7141012681838016e-05\n",
      "2025-07-17 09:13:12,363 - INFO - Epoch: 5/5, Iter: 55/119 -- train_loss: 0.9062\n",
      "Epoch [5/5]: [54/119]  45%|████▌     , loss=1 [01:56<03:51]\n",
      "Epoch [5/5]: [54/119]  45%|████▌     , loss=0.906 [01:56<03:51]\n",
      "Epoch [5/5]: [55/119]  46%|████▌     , loss=0.906 [01:56<03:12]\n",
      "2025-07-17 09:13:12,365 - INFO - Current learning rate: 5.540290612849853e-05\n",
      "2025-07-17 09:13:18,645 - INFO - Epoch: 5/5, Iter: 56/119 -- train_loss: 0.6516\n",
      "Epoch [5/5]: [55/119]  46%|████▌     , loss=0.906 [02:02<03:12]\n",
      "Epoch [5/5]: [55/119]  46%|████▌     , loss=0.652 [02:02<03:12]\n",
      "Epoch [5/5]: [56/119]  47%|████▋     , loss=0.652 [02:02<04:11]\n",
      "2025-07-17 09:13:18,647 - INFO - Current learning rate: 5.3690094666271054e-05\n",
      "2025-07-17 09:13:18,764 - INFO - Epoch: 5/5, Iter: 57/119 -- train_loss: 1.0042\n",
      "Epoch [5/5]: [56/119]  47%|████▋     , loss=0.652 [02:02<04:11]\n",
      "Epoch [5/5]: [56/119]  47%|████▋     , loss=1 [02:02<04:11]\n",
      "Epoch [5/5]: [57/119]  48%|████▊     , loss=1 [02:02<02:55]\n",
      "2025-07-17 09:13:18,765 - INFO - Current learning rate: 5.200267574412165e-05\n",
      "2025-07-17 09:13:19,598 - INFO - Epoch: 5/5, Iter: 58/119 -- train_loss: 0.8877\n",
      "Epoch [5/5]: [57/119]  48%|████▊     , loss=1 [02:03<02:55]\n",
      "Epoch [5/5]: [57/119]  48%|████▊     , loss=0.888 [02:03<02:55]\n",
      "Epoch [5/5]: [58/119]  49%|████▊     , loss=0.888 [02:03<02:16]\n",
      "2025-07-17 09:13:19,600 - INFO - Current learning rate: 5.034074536632907e-05\n",
      "2025-07-17 09:13:28,709 - INFO - Epoch: 5/5, Iter: 59/119 -- train_loss: 0.8606\n",
      "Epoch [5/5]: [58/119]  49%|████▊     , loss=0.888 [02:12<02:16]\n",
      "Epoch [5/5]: [58/119]  49%|████▊     , loss=0.861 [02:12<02:16]\n",
      "Epoch [5/5]: [59/119]  50%|████▉     , loss=0.861 [02:12<04:17]\n",
      "2025-07-17 09:13:28,712 - INFO - Current learning rate: 4.870439808702304e-05\n",
      "2025-07-17 09:13:29,046 - INFO - Epoch: 5/5, Iter: 60/119 -- train_loss: 0.7639\n",
      "Epoch [5/5]: [59/119]  50%|████▉     , loss=0.861 [02:12<04:17]\n",
      "Epoch [5/5]: [59/119]  50%|████▉     , loss=0.764 [02:12<04:17]\n",
      "Epoch [5/5]: [60/119]  50%|█████     , loss=0.764 [02:12<03:03]\n",
      "2025-07-17 09:13:29,048 - INFO - Current learning rate: 4.709372700480409e-05\n",
      "2025-07-17 09:13:29,178 - INFO - Epoch: 5/5, Iter: 61/119 -- train_loss: 1.0009\n",
      "Epoch [5/5]: [60/119]  50%|█████     , loss=0.764 [02:12<03:03]\n",
      "Epoch [5/5]: [60/119]  50%|█████     , loss=1 [02:12<03:03]\n",
      "Epoch [5/5]: [61/119]  51%|█████▏    , loss=1 [02:12<02:08]\n",
      "2025-07-17 09:13:29,180 - INFO - Current learning rate: 4.5508823757447466e-05\n",
      "2025-07-17 09:13:34,554 - INFO - Epoch: 5/5, Iter: 62/119 -- train_loss: 1.0025\n",
      "Epoch [5/5]: [61/119]  51%|█████▏    , loss=1 [02:18<02:08]\n",
      "Epoch [5/5]: [61/119]  51%|█████▏    , loss=1 [02:18<02:08]\n",
      "Epoch [5/5]: [62/119]  52%|█████▏    , loss=1 [02:18<03:00]\n",
      "2025-07-17 09:13:34,555 - INFO - Current learning rate: 4.394977851668893e-05\n",
      "2025-07-17 09:13:37,844 - INFO - Epoch: 5/5, Iter: 63/119 -- train_loss: 1.0011\n",
      "Epoch [5/5]: [62/119]  52%|█████▏    , loss=1 [02:21<03:00]\n",
      "Epoch [5/5]: [62/119]  52%|█████▏    , loss=1 [02:21<03:00]\n",
      "Epoch [5/5]: [63/119]  53%|█████▎    , loss=1 [02:21<02:59]\n",
      "2025-07-17 09:13:37,846 - INFO - Current learning rate: 4.24166799830949e-05\n",
      "2025-07-17 09:13:38,708 - INFO - Epoch: 5/5, Iter: 64/119 -- train_loss: 1.0028\n",
      "Epoch [5/5]: [63/119]  53%|█████▎    , loss=1 [02:22<02:59]\n",
      "Epoch [5/5]: [63/119]  53%|█████▎    , loss=1 [02:22<02:59]\n",
      "Epoch [5/5]: [64/119]  54%|█████▍    , loss=1 [02:22<02:17]\n",
      "2025-07-17 09:13:38,710 - INFO - Current learning rate: 4.090961538101549e-05\n",
      "2025-07-17 09:13:38,842 - INFO - Epoch: 5/5, Iter: 65/119 -- train_loss: 1.0008\n",
      "Epoch [5/5]: [64/119]  54%|█████▍    , loss=1 [02:22<02:17]\n",
      "Epoch [5/5]: [64/119]  54%|█████▍    , loss=1 [02:22<02:17]\n",
      "Epoch [5/5]: [65/119]  55%|█████▍    , loss=1 [02:22<01:36]\n",
      "2025-07-17 09:13:38,844 - INFO - Current learning rate: 3.942867045362252e-05\n",
      "2025-07-17 09:13:53,417 - INFO - Epoch: 5/5, Iter: 66/119 -- train_loss: 1.0012\n",
      "Epoch [5/5]: [65/119]  55%|█████▍    , loss=1 [02:37<01:36]\n",
      "Epoch [5/5]: [65/119]  55%|█████▍    , loss=1 [02:37<01:36]\n",
      "Epoch [5/5]: [66/119]  55%|█████▌    , loss=1 [02:37<04:58]2025-07-17 09:13:53,419 - INFO - Current learning rate: 3.797392945803054e-05\n",
      "2025-07-17 09:13:53,583 - INFO - Epoch: 5/5, Iter: 67/119 -- train_loss: 0.8828\n",
      "Epoch [5/5]: [66/119]  55%|█████▌    , loss=1 [02:37<04:58]\n",
      "Epoch [5/5]: [66/119]  55%|█████▌    , loss=0.883 [02:37<04:58]\n",
      "Epoch [5/5]: [67/119]  56%|█████▋    , loss=0.883 [02:37<03:27]\n",
      "2025-07-17 09:13:53,584 - INFO - Current learning rate: 3.6545475160503876e-05\n",
      "2025-07-17 09:13:53,692 - INFO - Epoch: 5/5, Iter: 68/119 -- train_loss: 1.0009\n",
      "Epoch [5/5]: [67/119]  56%|█████▋    , loss=0.883 [02:37<03:27]\n",
      "Epoch [5/5]: [67/119]  56%|█████▋    , loss=1 [02:37<03:27]\n",
      "Epoch [5/5]: [68/119]  57%|█████▋    , loss=1 [02:37<02:24]\n",
      "2025-07-17 09:13:53,694 - INFO - Current learning rate: 3.5143388831746917e-05\n",
      "2025-07-17 09:13:53,821 - INFO - Epoch: 5/5, Iter: 69/119 -- train_loss: 1.0058\n",
      "Epoch [5/5]: [68/119]  57%|█████▋    , loss=1 [02:37<02:24]\n",
      "Epoch [5/5]: [68/119]  57%|█████▋    , loss=1.01 [02:37<02:24]\n",
      "Epoch [5/5]: [69/119]  58%|█████▊    , loss=1.01 [02:37<01:40]\n",
      "2025-07-17 09:13:53,824 - INFO - Current learning rate: 3.3767750242280956e-05\n",
      "2025-07-17 09:14:00,305 - INFO - Epoch: 5/5, Iter: 70/119 -- train_loss: 0.7475\n",
      "Epoch [5/5]: [69/119]  58%|█████▊    , loss=1.01 [02:44<01:40]\n",
      "Epoch [5/5]: [69/119]  58%|█████▊    , loss=0.748 [02:44<01:40]\n",
      "Epoch [5/5]: [70/119]  59%|█████▉    , loss=0.748 [02:44<02:44]\n",
      "2025-07-17 09:14:00,307 - INFO - Current learning rate: 3.2418637657905245e-05\n",
      "2025-07-17 09:14:00,470 - INFO - Epoch: 5/5, Iter: 71/119 -- train_loss: 0.8840\n",
      "Epoch [5/5]: [70/119]  59%|█████▉    , loss=0.748 [02:44<02:44]\n",
      "Epoch [5/5]: [70/119]  59%|█████▉    , loss=0.884 [02:44<02:44]\n",
      "Epoch [5/5]: [71/119]  60%|█████▉    , loss=0.884 [02:44<01:55]\n",
      "2025-07-17 09:14:00,473 - INFO - Current learning rate: 3.109612783524434e-05\n",
      "2025-07-17 09:14:00,653 - INFO - Epoch: 5/5, Iter: 72/119 -- train_loss: 0.7843\n",
      "Epoch [5/5]: [71/119]  60%|█████▉    , loss=0.884 [02:44<01:55]\n",
      "Epoch [5/5]: [71/119]  60%|█████▉    , loss=0.784 [02:44<01:55]\n",
      "Epoch [5/5]: [72/119]  61%|██████    , loss=0.784 [02:44<01:21]\n",
      "2025-07-17 09:14:00,655 - INFO - Current learning rate: 2.9800296017381288e-05\n",
      "2025-07-17 09:14:00,793 - INFO - Epoch: 5/5, Iter: 73/119 -- train_loss: 0.6481\n",
      "Epoch [5/5]: [72/119]  61%|██████    , loss=0.784 [02:44<01:21]\n",
      "Epoch [5/5]: [72/119]  61%|██████    , loss=0.648 [02:44<01:21]\n",
      "Epoch [5/5]: [73/119]  61%|██████▏   , loss=0.648 [02:44<00:57]\n",
      "2025-07-17 09:14:00,794 - INFO - Current learning rate: 2.8531215929576077e-05\n",
      "2025-07-17 09:14:06,770 - INFO - Epoch: 5/5, Iter: 74/119 -- train_loss: 1.0012\n",
      "Epoch [5/5]: [73/119]  61%|██████▏   , loss=0.648 [02:50<00:57]\n",
      "Epoch [5/5]: [73/119]  61%|██████▏   , loss=1 [02:50<00:57]\n",
      "Epoch [5/5]: [74/119]  62%|██████▏   , loss=1 [02:50<02:00]\n",
      "2025-07-17 09:14:06,773 - INFO - Current learning rate: 2.7288959775071935e-05\n",
      "2025-07-17 09:14:07,053 - INFO - Epoch: 5/5, Iter: 75/119 -- train_loss: 0.9663\n",
      "Epoch [5/5]: [74/119]  62%|██████▏   , loss=1 [02:50<02:00]\n",
      "Epoch [5/5]: [74/119]  62%|██████▏   , loss=0.966 [02:50<02:00]\n",
      "Epoch [5/5]: [75/119]  63%|██████▎   , loss=0.966 [02:50<01:26]\n",
      "2025-07-17 09:14:07,055 - INFO - Current learning rate: 2.6073598230986913e-05\n",
      "2025-07-17 09:14:09,930 - INFO - Epoch: 5/5, Iter: 76/119 -- train_loss: 0.8535\n",
      "Epoch [5/5]: [75/119]  63%|██████▎   , loss=0.966 [02:53<01:26]\n",
      "Epoch [5/5]: [75/119]  63%|██████▎   , loss=0.854 [02:53<01:26]\n",
      "Epoch [5/5]: [76/119]  64%|██████▍   , loss=0.854 [02:53<01:35]\n",
      "2025-07-17 09:14:09,932 - INFO - Current learning rate: 2.488520044429263e-05\n",
      "2025-07-17 09:14:10,096 - INFO - Epoch: 5/5, Iter: 77/119 -- train_loss: 1.0018\n",
      "Epoch [5/5]: [76/119]  64%|██████▍   , loss=0.854 [02:53<01:35]\n",
      "Epoch [5/5]: [76/119]  64%|██████▍   , loss=1 [02:53<01:35]\n",
      "Epoch [5/5]: [77/119]  65%|██████▍   , loss=1 [02:53<01:07]\n",
      "2025-07-17 09:14:10,098 - INFO - Current learning rate: 2.3723834027880643e-05\n",
      "2025-07-17 09:14:17,705 - INFO - Epoch: 5/5, Iter: 78/119 -- train_loss: 1.0011\n",
      "Epoch [5/5]: [77/119]  65%|██████▍   , loss=1 [03:01<01:07]\n",
      "Epoch [5/5]: [77/119]  65%|██████▍   , loss=1 [03:01<01:07]\n",
      "Epoch [5/5]: [78/119]  66%|██████▌   , loss=1 [03:01<02:19]\n",
      "2025-07-17 09:14:17,709 - INFO - Current learning rate: 2.258956505671539e-05\n",
      "2025-07-17 09:14:17,857 - INFO - Epoch: 5/5, Iter: 79/119 -- train_loss: 0.9077\n",
      "Epoch [5/5]: [78/119]  66%|██████▌   , loss=1 [03:01<02:19]\n",
      "Epoch [5/5]: [78/119]  66%|██████▌   , loss=0.908 [03:01<02:19]\n",
      "Epoch [5/5]: [79/119]  66%|██████▋   , loss=0.908 [03:01<01:37]\n",
      "2025-07-17 09:14:17,858 - INFO - Current learning rate: 2.1482458064075025e-05\n",
      "2025-07-17 09:14:19,610 - INFO - Epoch: 5/5, Iter: 80/119 -- train_loss: 1.0011\n",
      "Epoch [5/5]: [79/119]  66%|██████▋   , loss=0.908 [03:03<01:37]\n",
      "Epoch [5/5]: [79/119]  66%|██████▋   , loss=1 [03:03<01:37]\n",
      "Epoch [5/5]: [80/119]  67%|██████▋   , loss=1 [03:03<01:26]\n",
      "2025-07-17 09:14:19,612 - INFO - Current learning rate: 2.040257603787961e-05\n",
      "2025-07-17 09:14:19,779 - INFO - Epoch: 5/5, Iter: 81/119 -- train_loss: 0.8708\n",
      "Epoch [5/5]: [80/119]  67%|██████▋   , loss=1 [03:03<01:26]\n",
      "Epoch [5/5]: [80/119]  67%|██████▋   , loss=0.871 [03:03<01:26]\n",
      "Epoch [5/5]: [81/119]  68%|██████▊   , loss=0.871 [03:03<01:01]\n",
      "2025-07-17 09:14:19,782 - INFO - Current learning rate: 1.9349980417107983e-05\n",
      "2025-07-17 09:14:35,315 - INFO - Epoch: 5/5, Iter: 82/119 -- train_loss: 1.0007\n",
      "Epoch [5/5]: [81/119]  68%|██████▊   , loss=0.871 [03:19<01:01]\n",
      "Epoch [5/5]: [81/119]  68%|██████▊   , loss=1 [03:19<01:01]\n",
      "Epoch [5/5]: [82/119]  69%|██████▉   , loss=1 [03:19<03:34]\n",
      "2025-07-17 09:14:35,317 - INFO - Current learning rate: 1.832473108830163e-05\n",
      "2025-07-17 09:14:35,421 - INFO - Epoch: 5/5, Iter: 83/119 -- train_loss: 1.0023\n",
      "Epoch [5/5]: [82/119]  69%|██████▉   , loss=1 [03:19<03:34]\n",
      "Epoch [5/5]: [82/119]  69%|██████▉   , loss=1 [03:19<03:34]\n",
      "Epoch [5/5]: [83/119]  70%|██████▉   , loss=1 [03:19<02:27]\n",
      "2025-07-17 09:14:35,422 - INFO - Current learning rate: 1.732688638215798e-05\n",
      "2025-07-17 09:14:35,528 - INFO - Epoch: 5/5, Iter: 84/119 -- train_loss: 0.7651\n",
      "Epoch [5/5]: [83/119]  70%|██████▉   , loss=1 [03:19<02:27]\n",
      "Epoch [5/5]: [83/119]  70%|██████▉   , loss=0.765 [03:19<02:27]\n",
      "Epoch [5/5]: [84/119]  71%|███████   , loss=0.765 [03:19<01:41]2025-07-17 09:14:35,529 - INFO - Current learning rate: 1.635650307021135e-05\n",
      "2025-07-17 09:14:35,658 - INFO - Epoch: 5/5, Iter: 85/119 -- train_loss: 0.5815\n",
      "Epoch [5/5]: [84/119]  71%|███████   , loss=0.765 [03:19<01:41]\n",
      "Epoch [5/5]: [84/119]  71%|███████   , loss=0.582 [03:19<01:41]\n",
      "Epoch [5/5]: [85/119]  71%|███████▏  , loss=0.582 [03:19<01:10]\n",
      "2025-07-17 09:14:35,660 - INFO - Current learning rate: 1.5413636361603376e-05\n",
      "2025-07-17 09:14:41,827 - INFO - Epoch: 5/5, Iter: 86/119 -- train_loss: 1.0022\n",
      "Epoch [5/5]: [85/119]  71%|███████▏  , loss=0.582 [03:25<01:10]\n",
      "Epoch [5/5]: [85/119]  71%|███████▏  , loss=1 [03:25<01:10]\n",
      "Epoch [5/5]: [86/119]  72%|███████▏  , loss=1 [03:25<01:48]\n",
      "2025-07-17 09:14:41,829 - INFO - Current learning rate: 1.4498339899941473e-05\n",
      "2025-07-17 09:14:42,007 - INFO - Epoch: 5/5, Iter: 87/119 -- train_loss: 0.9159\n",
      "Epoch [5/5]: [86/119]  72%|███████▏  , loss=1 [03:25<01:48]\n",
      "Epoch [5/5]: [86/119]  72%|███████▏  , loss=0.916 [03:25<01:48]\n",
      "Epoch [5/5]: [87/119]  73%|███████▎  , loss=0.916 [03:25<01:15]\n",
      "2025-07-17 09:14:42,009 - INFO - Current learning rate: 1.3610665760247248e-05\n",
      "2025-07-17 09:14:45,003 - INFO - Epoch: 5/5, Iter: 88/119 -- train_loss: 1.0009\n",
      "Epoch [5/5]: [87/119]  73%|███████▎  , loss=0.916 [03:28<01:15]\n",
      "Epoch [5/5]: [87/119]  73%|███████▎  , loss=1 [03:28<01:15]\n",
      "Epoch [5/5]: [88/119]  74%|███████▍  , loss=1 [03:28<01:19]\n",
      "2025-07-17 09:14:45,005 - INFO - Current learning rate: 1.275066444599343e-05\n",
      "2025-07-17 09:14:45,132 - INFO - Epoch: 5/5, Iter: 89/119 -- train_loss: 0.9499\n",
      "Epoch [5/5]: [88/119]  74%|███████▍  , loss=1 [03:28<01:19]\n",
      "Epoch [5/5]: [88/119]  74%|███████▍  , loss=0.95 [03:28<01:19]\n",
      "Epoch [5/5]: [89/119]  75%|███████▍  , loss=0.95 [03:28<00:54]\n",
      "2025-07-17 09:14:45,134 - INFO - Current learning rate: 1.1918384886230842e-05\n",
      "2025-07-17 09:14:50,326 - INFO - Epoch: 5/5, Iter: 90/119 -- train_loss: 0.9141\n",
      "Epoch [5/5]: [89/119]  75%|███████▍  , loss=0.95 [03:34<00:54]\n",
      "Epoch [5/5]: [89/119]  75%|███████▍  , loss=0.914 [03:34<00:54]\n",
      "Epoch [5/5]: [90/119]  76%|███████▌  , loss=0.914 [03:34<01:22]\n",
      "2025-07-17 09:14:50,328 - INFO - Current learning rate: 1.111387443280415e-05\n",
      "2025-07-17 09:14:50,464 - INFO - Epoch: 5/5, Iter: 91/119 -- train_loss: 0.9518\n",
      "Epoch [5/5]: [90/119]  76%|███████▌  , loss=0.914 [03:34<01:22]\n",
      "Epoch [5/5]: [90/119]  76%|███████▌  , loss=0.952 [03:34<01:22]\n",
      "Epoch [5/5]: [91/119]  76%|███████▋  , loss=0.952 [03:34<00:56]\n",
      "2025-07-17 09:14:50,466 - INFO - Current learning rate: 1.03371788576583e-05\n",
      "2025-07-17 09:14:53,917 - INFO - Epoch: 5/5, Iter: 92/119 -- train_loss: 1.0009\n",
      "Epoch [5/5]: [91/119]  76%|███████▋  , loss=0.952 [03:37<00:56]\n",
      "Epoch [5/5]: [91/119]  76%|███████▋  , loss=1 [03:37<00:56]\n",
      "Epoch [5/5]: [92/119]  77%|███████▋  , loss=1 [03:37<01:06]\n",
      "2025-07-17 09:14:53,920 - INFO - Current learning rate: 9.588342350234049e-06\n",
      "2025-07-17 09:14:54,066 - INFO - Epoch: 5/5, Iter: 93/119 -- train_loss: 1.0008\n",
      "Epoch [5/5]: [92/119]  77%|███████▋  , loss=1 [03:37<01:06]\n",
      "Epoch [5/5]: [92/119]  77%|███████▋  , loss=1 [03:37<01:06]\n",
      "Epoch [5/5]: [93/119]  78%|███████▊  , loss=1 [03:37<00:45]\n",
      "2025-07-17 09:14:54,068 - INFO - Current learning rate: 8.867407514954112e-06\n",
      "2025-07-17 09:14:57,148 - INFO - Epoch: 5/5, Iter: 94/119 -- train_loss: 1.0014\n",
      "Epoch [5/5]: [93/119]  78%|███████▊  , loss=1 [03:40<00:45]\n",
      "Epoch [5/5]: [93/119]  78%|███████▊  , loss=1 [03:40<00:45]\n",
      "Epoch [5/5]: [94/119]  79%|███████▉  , loss=1 [03:40<00:53]\n",
      "2025-07-17 09:14:57,150 - INFO - Current learning rate: 8.174415368798826e-06\n",
      "2025-07-17 09:14:57,331 - INFO - Epoch: 5/5, Iter: 95/119 -- train_loss: 1.0014\n",
      "Epoch [5/5]: [94/119]  79%|███████▉  , loss=1 [03:41<00:53]\n",
      "Epoch [5/5]: [94/119]  79%|███████▉  , loss=1 [03:41<00:53]\n",
      "Epoch [5/5]: [95/119]  80%|███████▉  , loss=1 [03:41<00:37]\n",
      "2025-07-17 09:14:57,333 - INFO - Current learning rate: 7.509405338972965e-06\n",
      "2025-07-17 09:15:02,565 - INFO - Epoch: 5/5, Iter: 96/119 -- train_loss: 0.9582\n",
      "Epoch [5/5]: [95/119]  80%|███████▉  , loss=1 [03:46<00:37]\n",
      "Epoch [5/5]: [95/119]  80%|███████▉  , loss=0.958 [03:46<00:37]\n",
      "Epoch [5/5]: [96/119]  81%|████████  , loss=0.958 [03:46<01:01]\n",
      "2025-07-17 09:15:02,567 - INFO - Current learning rate: 6.8724152606621624e-06\n",
      "2025-07-17 09:15:02,691 - INFO - Epoch: 5/5, Iter: 97/119 -- train_loss: 0.9768\n",
      "Epoch [5/5]: [96/119]  81%|████████  , loss=0.958 [03:46<01:01]\n",
      "Epoch [5/5]: [96/119]  81%|████████  , loss=0.977 [03:46<01:01]\n",
      "Epoch [5/5]: [97/119]  82%|████████▏ , loss=0.977 [03:46<00:41]\n",
      "2025-07-17 09:15:02,692 - INFO - Current learning rate: 6.26348137488075e-06\n",
      "2025-07-17 09:15:04,077 - INFO - Epoch: 5/5, Iter: 98/119 -- train_loss: 0.9871\n",
      "Epoch [5/5]: [97/119]  82%|████████▏ , loss=0.977 [03:47<00:41]\n",
      "Epoch [5/5]: [97/119]  82%|████████▏ , loss=0.987 [03:47<00:41]\n",
      "Epoch [5/5]: [98/119]  82%|████████▏ , loss=0.987 [03:47<00:36]\n",
      "2025-07-17 09:15:04,078 - INFO - Current learning rate: 5.682638326409303e-06\n",
      "2025-07-17 09:15:09,879 - INFO - Epoch: 5/5, Iter: 99/119 -- train_loss: 0.5608\n",
      "Epoch [5/5]: [98/119]  82%|████████▏ , loss=0.987 [03:53<00:36]\n",
      "Epoch [5/5]: [98/119]  82%|████████▏ , loss=0.561 [03:53<00:36]\n",
      "Epoch [5/5]: [99/119]  83%|████████▎ , loss=0.561 [03:53<00:59]\n",
      "2025-07-17 09:15:09,881 - INFO - Current learning rate: 5.1299191618241175e-06\n",
      "2025-07-17 09:15:10,015 - INFO - Epoch: 5/5, Iter: 100/119 -- train_loss: 1.0016\n",
      "Epoch [5/5]: [99/119]  83%|████████▎ , loss=0.561 [03:53<00:59]\n",
      "Epoch [5/5]: [99/119]  83%|████████▎ , loss=1 [03:53<00:59]\n",
      "Epoch [5/5]: [100/119]  84%|████████▍ , loss=1 [03:53<00:40]\n",
      "2025-07-17 09:15:10,018 - INFO - Current learning rate: 4.605355327616659e-06\n",
      "2025-07-17 09:15:10,189 - INFO - Epoch: 5/5, Iter: 101/119 -- train_loss: 1.0010\n",
      "Epoch [5/5]: [100/119]  84%|████████▍ , loss=1 [03:53<00:40]\n",
      "Epoch [5/5]: [100/119]  84%|████████▍ , loss=1 [03:53<00:40]\n",
      "Epoch [5/5]: [101/119]  85%|████████▍ , loss=1 [03:53<00:27]\n",
      "2025-07-17 09:15:10,191 - INFO - Current learning rate: 4.1089766684046205e-06\n",
      "2025-07-17 09:15:16,040 - INFO - Epoch: 5/5, Iter: 102/119 -- train_loss: 0.5467\n",
      "Epoch [5/5]: [101/119]  85%|████████▍ , loss=1 [03:59<00:27]\n",
      "Epoch [5/5]: [101/119]  85%|████████▍ , loss=0.547 [03:59<00:27]\n",
      "Epoch [5/5]: [102/119]  86%|████████▌ , loss=0.547 [03:59<00:48]\n",
      "2025-07-17 09:15:16,043 - INFO - Current learning rate: 3.6408114252338922e-06\n",
      "2025-07-17 09:15:21,934 - INFO - Epoch: 5/5, Iter: 103/119 -- train_loss: 1.0023\n",
      "Epoch [5/5]: [102/119]  86%|████████▌ , loss=0.547 [04:05<00:48]\n",
      "Epoch [5/5]: [102/119]  86%|████████▌ , loss=1 [04:05<00:48]\n",
      "Epoch [5/5]: [103/119]  87%|████████▋ , loss=1 [04:05<00:59]\n",
      "2025-07-17 09:15:21,937 - INFO - Current learning rate: 3.200886233971803e-06\n",
      "2025-07-17 09:15:22,106 - INFO - Epoch: 5/5, Iter: 104/119 -- train_loss: 0.7238\n",
      "Epoch [5/5]: [103/119]  87%|████████▋ , loss=1 [04:05<00:59]\n",
      "Epoch [5/5]: [103/119]  87%|████████▋ , loss=0.724 [04:05<00:59]\n",
      "Epoch [5/5]: [104/119]  87%|████████▋ , loss=0.724 [04:05<00:40]\n",
      "2025-07-17 09:15:22,108 - INFO - Current learning rate: 2.7892261237917216e-06\n",
      "2025-07-17 09:15:22,263 - INFO - Epoch: 5/5, Iter: 105/119 -- train_loss: 1.0051\n",
      "Epoch [5/5]: [104/119]  87%|████████▋ , loss=0.724 [04:05<00:40]\n",
      "Epoch [5/5]: [104/119]  87%|████████▋ , loss=1.01 [04:05<00:40]\n",
      "Epoch [5/5]: [105/119]  88%|████████▊ , loss=1.01 [04:05<00:26]\n",
      "2025-07-17 09:15:22,264 - INFO - Current learning rate: 2.4058545157490397e-06\n",
      "2025-07-17 09:15:24,406 - INFO - Epoch: 5/5, Iter: 106/119 -- train_loss: 0.7199\n",
      "Epoch [5/5]: [105/119]  88%|████████▊ , loss=1.01 [04:08<00:26]\n",
      "Epoch [5/5]: [105/119]  88%|████████▊ , loss=0.72 [04:08<00:26]\n",
      "Epoch [5/5]: [106/119]  89%|████████▉ , loss=0.72 [04:08<00:25]\n",
      "2025-07-17 09:15:24,408 - INFO - Current learning rate: 2.0507932214485736e-06\n",
      "2025-07-17 09:15:31,399 - INFO - Epoch: 5/5, Iter: 107/119 -- train_loss: 0.8682\n",
      "Epoch [5/5]: [106/119]  89%|████████▉ , loss=0.72 [04:15<00:25]\n",
      "Epoch [5/5]: [106/119]  89%|████████▉ , loss=0.868 [04:15<00:25]\n",
      "Epoch [5/5]: [107/119]  90%|████████▉ , loss=0.868 [04:15<00:41]\n",
      "2025-07-17 09:15:31,401 - INFO - Current learning rate: 1.7240624418037309e-06\n",
      "2025-07-17 09:15:31,527 - INFO - Epoch: 5/5, Iter: 108/119 -- train_loss: 1.0013\n",
      "Epoch [5/5]: [107/119]  90%|████████▉ , loss=0.868 [04:15<00:41]\n",
      "Epoch [5/5]: [107/119]  90%|████████▉ , loss=1 [04:15<00:41]\n",
      "Epoch [5/5]: [108/119]  91%|█████████ , loss=1 [04:15<00:27]\n",
      "2025-07-17 09:15:31,529 - INFO - Current learning rate: 1.4256807658871537e-06\n",
      "2025-07-17 09:15:31,666 - INFO - Epoch: 5/5, Iter: 109/119 -- train_loss: 0.9600\n",
      "Epoch [5/5]: [108/119]  91%|█████████ , loss=1 [04:15<00:27]\n",
      "Epoch [5/5]: [108/119]  91%|█████████ , loss=0.96 [04:15<00:27]\n",
      "Epoch [5/5]: [109/119]  92%|█████████▏, loss=0.96 [04:15<00:17]\n",
      "2025-07-17 09:15:31,668 - INFO - Current learning rate: 1.155665169873016e-06\n",
      "2025-07-17 09:15:32,458 - INFO - Epoch: 5/5, Iter: 110/119 -- train_loss: 1.0008\n",
      "Epoch [5/5]: [109/119]  92%|█████████▏, loss=0.96 [04:16<00:17]\n",
      "Epoch [5/5]: [109/119]  92%|█████████▏, loss=1 [04:16<00:17]\n",
      "Epoch [5/5]: [110/119]  92%|█████████▏, loss=1 [04:16<00:13]\n",
      "2025-07-17 09:15:32,461 - INFO - Current learning rate: 9.140310160712988e-07\n",
      "2025-07-17 09:15:44,168 - INFO - Epoch: 5/5, Iter: 111/119 -- train_loss: 1.0008\n",
      "Epoch [5/5]: [110/119]  92%|█████████▏, loss=1 [04:27<00:13]\n",
      "Epoch [5/5]: [110/119]  92%|█████████▏, loss=1 [04:27<00:13]#015Epoch [5/5]: [111/119]  93%|█████████▎, loss=1 [04:27<00:36]\n",
      "2025-07-17 09:15:44,170 - INFO - Current learning rate: 7.007920520538265e-07\n",
      "2025-07-17 09:15:44,332 - INFO - Epoch: 5/5, Iter: 112/119 -- train_loss: 1.0012\n",
      "Epoch [5/5]: [111/119]  93%|█████████▎, loss=1 [04:28<00:36]\n",
      "Epoch [5/5]: [111/119]  93%|█████████▎, loss=1 [04:28<00:36]\n",
      "Epoch [5/5]: [112/119]  94%|█████████▍, loss=1 [04:28<00:22]\n",
      "2025-07-17 09:15:44,333 - INFO - Current learning rate: 5.159604098718399e-07\n",
      "2025-07-17 09:15:44,436 - INFO - Epoch: 5/5, Iter: 113/119 -- train_loss: 0.8663\n",
      "Epoch [5/5]: [112/119]  94%|█████████▍, loss=1 [04:28<00:22]\n",
      "Epoch [5/5]: [112/119]  94%|█████████▍, loss=0.866 [04:28<00:22]\n",
      "Epoch [5/5]: [113/119]  95%|█████████▍, loss=0.866 [04:28<00:13]\n",
      "2025-07-17 09:15:44,437 - INFO - Current learning rate: 3.5954660536605104e-07\n",
      "2025-07-17 09:15:44,541 - INFO - Epoch: 5/5, Iter: 114/119 -- train_loss: 1.0006\n",
      "Epoch [5/5]: [113/119]  95%|█████████▍, loss=0.866 [04:28<00:13]\n",
      "Epoch [5/5]: [113/119]  95%|█████████▍, loss=1 [04:28<00:13]\n",
      "Epoch [5/5]: [114/119]  96%|█████████▌, loss=1 [04:28<00:08]\n",
      "2025-07-17 09:15:44,542 - INFO - Current learning rate: 2.3155953756812456e-07\n",
      "2025-07-17 09:15:49,764 - INFO - Epoch: 5/5, Iter: 115/119 -- train_loss: 0.8215\n",
      "Epoch [5/5]: [114/119]  96%|█████████▌, loss=1 [04:33<00:08]\n",
      "Epoch [5/5]: [114/119]  96%|█████████▌, loss=0.821 [04:33<00:08]\n",
      "Epoch [5/5]: [115/119]  97%|█████████▋, loss=0.821 [04:33<00:10]\n",
      "2025-07-17 09:15:49,766 - INFO - Current learning rate: 1.3200648819452887e-07\n",
      "2025-07-17 09:15:49,894 - INFO - Epoch: 5/5, Iter: 116/119 -- train_loss: 1.0009\n",
      "Epoch [5/5]: [115/119]  97%|█████████▋, loss=0.821 [04:33<00:10]\n",
      "Epoch [5/5]: [115/119]  97%|█████████▋, loss=1 [04:33<00:10]\n",
      "Epoch [5/5]: [116/119]  97%|█████████▋, loss=1 [04:33<00:05]\n",
      "2025-07-17 09:15:49,895 - INFO - Current learning rate: 6.08931212322026e-08\n",
      "2025-07-17 09:15:49,997 - INFO - Epoch: 5/5, Iter: 117/119 -- train_loss: 0.9750\n",
      "Epoch [5/5]: [116/119]  97%|█████████▋, loss=1 [04:33<00:05]\n",
      "Epoch [5/5]: [116/119]  97%|█████████▋, loss=0.975 [04:33<00:05]\n",
      "Epoch [5/5]: [117/119]  98%|█████████▊, loss=0.975 [04:33<00:02]\n",
      "2025-07-17 09:15:49,999 - INFO - Current learning rate: 1.8223482616313685e-08\n",
      "2025-07-17 09:15:56,020 - INFO - Epoch: 5/5, Iter: 118/119 -- train_loss: 1.0049\n",
      "Epoch [5/5]: [117/119]  98%|█████████▊, loss=0.975 [04:39<00:02]\n",
      "Epoch [5/5]: [117/119]  98%|█████████▊, loss=1 [04:39<00:02]\n",
      "Epoch [5/5]: [118/119]  99%|█████████▉, loss=1 [04:39<00:02]\n",
      "2025-07-17 09:15:56,021 - INFO - Current learning rate: 4e-09\n",
      "2025-07-17 09:15:59,237 - INFO - Epoch: 5/5, Iter: 119/119 -- train_loss: 1.0014\n",
      "Epoch [5/5]: [118/119]  99%|█████████▉, loss=1 [04:42<00:02]\n",
      "Epoch [5/5]: [118/119]  99%|█████████▉, loss=1 [04:42<00:02]\n",
      "Epoch [5/5]: [119/119] 100%|██████████, loss=1 [04:42<00:00]\n",
      "2025-07-17 09:15:59,238 - INFO - Current learning rate: 1.8223482616313685e-08\n",
      "2025-07-17 09:15:59,238 - INFO - Engine run resuming from iteration 0, epoch 4 until 5 epochs\n",
      "[1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [5/5]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]#033[A\n",
      "Epoch [5/5]: [1/20]   5%|#033[32m▌         #033[0m [00:00<?]\n",
      "#033[A\n",
      "Epoch [5/5]: [2/20]  10%|#033[32m█         #033[0m [00:00<00:09]\n",
      "#033[A\n",
      "Epoch [5/5]: [2/20]  10%|#033[32m█         #033[0m [00:01<00:09]\n",
      "#033[A\n",
      "Epoch [5/5]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:10]\n",
      "#033[A\n",
      "Epoch [5/5]: [3/20]  15%|#033[32m█▌        #033[0m [00:01<00:10]#033[A\n",
      "#015Epoch [5/5]: [4/20]  20%|#033[32m██        #033[0m [00:01<00:10]#033[A\n",
      "Epoch [5/5]: [4/20]  20%|#033[32m██        #033[0m [00:03<00:10]#033[A\n",
      "Epoch [5/5]: [5/20]  25%|#033[32m██▌       #033[0m [00:03<00:14]#033[A\n",
      "Epoch [5/5]: [5/20]  25%|#033[32m██▌       #033[0m [00:04<00:14]\n",
      "#033[A\n",
      "Epoch [5/5]: [6/20]  30%|#033[32m███       #033[0m [00:04<00:12]\n",
      "#033[A\n",
      "Epoch [5/5]: [6/20]  30%|#033[32m███       #033[0m [00:05<00:12]#033[A\n",
      "Epoch [5/5]: [7/20]  35%|#033[32m███▌      #033[0m [00:05<00:14]#033[A\n",
      "Epoch [5/5]: [7/20]  35%|#033[32m███▌      #033[0m [00:06<00:14]\n",
      "#033[A\n",
      "Epoch [5/5]: [8/20]  40%|#033[32m████      #033[0m [00:06<00:11]\n",
      "#033[A\n",
      "Epoch [5/5]: [8/20]  40%|#033[32m████      #033[0m [00:07<00:11]\n",
      "#033[A\n",
      "Epoch [5/5]: [9/20]  45%|#033[32m████▌     #033[0m [00:07<00:09]\n",
      "#033[A\n",
      "Epoch [5/5]: [9/20]  45%|#033[32m████▌     #033[0m [00:09<00:09]\n",
      "#033[A\n",
      "Epoch [5/5]: [10/20]  50%|#033[32m█████     #033[0m [00:09<00:13]\n",
      "#033[A\n",
      "Epoch [5/5]: [10/20]  50%|#033[32m█████     #033[0m [00:10<00:13]#033[A\n",
      "#015Epoch [5/5]: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:11]#033[A\n",
      "Epoch [5/5]: [11/20]  55%|#033[32m█████▌    #033[0m [00:10<00:11]#033[A\n",
      "Epoch [5/5]: [12/20]  60%|#033[32m██████    #033[0m [00:10<00:08]#033[A\n",
      "Epoch [5/5]: [12/20]  60%|#033[32m██████    #033[0m [00:12<00:08]#033[A\n",
      "Epoch [5/5]: [13/20]  65%|#033[32m██████▌   #033[0m [00:12<00:07]#033[A\n",
      "Epoch [5/5]: [13/20]  65%|#033[32m██████▌   #033[0m [00:13<00:07]#033[A\n",
      "#015Epoch [5/5]: [14/20]  70%|#033[32m███████   #033[0m [00:13<00:06]#033[A\n",
      "Epoch [5/5]: [14/20]  70%|#033[32m███████   #033[0m [00:13<00:06]\n",
      "#033[A\n",
      "Epoch [5/5]: [15/20]  75%|#033[32m███████▌  #033[0m [00:13<00:04]\n",
      "#033[A\n",
      "Epoch [5/5]: [15/20]  75%|#033[32m███████▌  #033[0m [00:14<00:04]#033[A\n",
      "Epoch [5/5]: [16/20]  80%|#033[32m████████  #033[0m [00:14<00:03]#033[A\n",
      "Epoch [5/5]: [16/20]  80%|#033[32m████████  #033[0m [00:15<00:03]#033[A\n",
      "Epoch [5/5]: [17/20]  85%|#033[32m████████▌ #033[0m [00:15<00:02]#033[A\n",
      "Epoch [5/5]: [17/20]  85%|#033[32m████████▌ #033[0m [00:16<00:02]#033[A\n",
      "Epoch [5/5]: [18/20]  90%|#033[32m█████████ #033[0m [00:16<00:01]#033[A\n",
      "Epoch [5/5]: [18/20]  90%|#033[32m█████████ #033[0m [00:16<00:01]#033[A\n",
      "Epoch [5/5]: [19/20]  95%|#033[32m█████████▌#033[0m [00:16<00:00]#033[A\n",
      "Epoch [5/5]: [19/20]  95%|#033[32m█████████▌#033[0m [00:17<00:00]#033[A\n",
      "Epoch [5/5]: [20/20] 100%|#033[32m██████████#033[0m [00:17<00:00]#033[A\n",
      "#033[A\n",
      "2025-07-17 09:16:22,886 - INFO - Epoch[5] Complete. Time taken: 00:00:23.566\n",
      "2025-07-17 09:16:22,886 - INFO - Engine run finished. Time taken: 00:00:23.648\n",
      "2025-07-17 09:16:22,959 - INFO - Epoch[5] Complete. Time taken: 00:05:24.319\n",
      "2025-07-17 09:16:22,960 - INFO - Engine run finished. Time taken: 00:26:59.781\n",
      "2025-07-17 09:16:23,086 - INFO - Training completed in 27.00 minutes\n",
      "2025-07-17 09:16:24,464 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-07-17 09:16:24,465 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-07-17 09:16:24,465 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2025-07-17 09:17:20 Uploading - Uploading generated training model\n",
      "2025-07-17 09:17:20 Completed - Training job completed\n",
      "Training seconds: 2081\n",
      "Billable seconds: 2081\n",
      "CPU times: user 4.11 s, sys: 289 ms, total: 4.4 s\n",
      "Wall time: 35min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "import os\n",
    "\n",
    "image = \"763104351884.dkr.ecr.ap-southeast-1.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    image_uri=image,\n",
    "    entry_point=\"train_gpt.py\",\n",
    "    source_dir=\".\",\n",
    "    role=role,\n",
    "    region=\"ap-southeast-1\",\n",
    "    instance_type=\"ml.g4dn.2xlarge\", # ml.g5.xlarge ran out of memory halfway through 2nd epoch (free tier instance)\n",
    "    # instance_type='local',\n",
    "    instance_count=1,\n",
    "    framework_version=\"2.7.1\",\n",
    "    py_version=\"py312\",\n",
    "    enable_spot_training=True,  # for cost savings\n",
    "    input_mode=\"File\",\n",
    "    sagemaker_session=sess,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}, #use torchrun backend\n",
    "    # distribution={\"pytorchddp\": {\"enabled\": True}}, #use mpirun backend\n",
    ")\n",
    "inputs={\n",
    "    \"training\": train_channel,\n",
    "    \"models\": model_channel,\n",
    "    \"training_config\": os.environ.get(\"SM_INPUT_TRAINING_CONFIG\", tumor_config),\n",
    "}\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34073fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "network_tumor_1_key_metric=0.0066.pt\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp s3://sagemaker-ap-southeast-1-345594598345/pytorch-training-2025-07-15-08-01-51-392/output/model.tar.gz .\n",
    "#!tar -xvzf model.tar.gz\n",
    "\n",
    "import nibabel as nib\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "job_name = estimator.latest_training_job.name\n",
    "model_artifact_s3_uri = estimator.model_data\n",
    "\n",
    "print(\"Model artifact is at:\", model_artifact_s3_uri)\n",
    "\n",
    "destination_key = \"models/model.tar.gz\"\n",
    "\n",
    "# Perform the copy\n",
    "s3.copy_object(\n",
    "    Bucket=bucket_name,  # Destination bucket\n",
    "    CopySource={\n",
    "        'Bucket': bucket_name,  # Source bucket\n",
    "        'Key': model_artifact_s3_uri\n",
    "    },\n",
    "    Key=destination_key  # Destination key (what you want it to be)\n",
    ")\n",
    "\n",
    "print(f\"Copied model to s3://{bucket_name}/{destination_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4976e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
